{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the High Performance Computing (HPC) documentation of the University of Bern","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Official documentation site of the high performance computing and the HPC cluster UBELIX.</p> <p>Currently, the UBELIX cluster runs around 280 compute nodes featuring ~15k CPU cores and ~180 GPUs with almost one million GPU cores. The infrastructure is available to all University personnel for their scientific work. The cluster can also be used by students within a scope of a thesis or a course.</p> <p>If you are wondering\u2026 UBELIX is an acronym and stands for University of Bern Linux Cluster (Naming similarities to known Gauls are purely coincidental and not intended in any way).</p> <p>Job Monitoring</p> <p>See what is currently running on UBELIX on the Job Monitoring pages.</p>"},{"location":"#acknowledging-ubelix","title":"Acknowledging UBELIX","text":"<p>When you present results generated using our cluster UBELIX, we kindly ask you to acknowledge the usage of the cluster. We would also highly appreciate if you could send us a copy of your papers, posters and presentations mentioning the UBELIX cluster. Public visibility of our cluster and documenting results are important for us to ensure long-term funding of UBELIX.</p> <p>Whenever the UBELIX infrastructure has been used to produce results used in a publication or poster, we kindly request citing the service in the acknowledgements:</p> <pre><code>\"Calculations were performed on UBELIX (https://www.id.unibe.ch/hpc), the HPC cluster at the University of Bern.\"\n</code></pre>"},{"location":"code-of-conduct/","title":"Code of Conduct","text":"<p>On this page we list some expectations from our side and recommended practice that is crucial for maintaining a good and professional working relationship between the  user and the system administrators. Most of those contents are quite self-explanatory  while others help to reduce the amount of support time needed to allocate.</p>"},{"location":"code-of-conduct/#general","title":"General","text":"<ul> <li>We assume that you are familiar with some basic knowledge about Linux command    line (shell) navigation and shell scripting. If you never worked on the command    line, consider some Linux tutorials on the subject first.</li> <li>We expect you to exploit this valuable documentation before asking for help.   All that is needed to get some simple jobs done on UBELIX is documented here.</li> </ul>"},{"location":"code-of-conduct/#general-communication-with-the-cluster-administrators","title":"General Communication with the Cluster Administrators","text":"<ul> <li>Use the Service Portal    for questions, issues or comments regarding UBELIX. </li> <li>Do not use the personal email address of a cluster administrator. This   is important because it keeps all administrators informed about the ongoing    problem-solving process, and if one administrator is on vacation, another   administrator can help you with your question</li> <li>For each new problem start a new conversation with a new subject. Avoid to write   to us by replying to an old answer mail from the last problem that you received    from us or even worse by replying to mailing list email you received from us. The    point here is that though it looks like an ordinary email, you actually are opening   a new ticket in our ticket system (or reopening an old ticket if replying to an old email).</li> </ul>"},{"location":"code-of-conduct/#problem-solving-process","title":"Problem-Solving Process","text":"<ul> <li>Exploit resources provided by your institute/research group before asking the UBELIX    staff about domain-specific problems. We make an effort to help you, but we are no    experts in your field, hence a colleague from your group who uses the cluster to solve    a similar problem like you do might be a better first contact</li> <li>Ask Google for help before contacting us. We often also just \u201cgoogle\u201d for an answer,   and then forward the outcome to you.</li> <li>Do not ask for/expect step-by-step solutions to a problem. Sometimes we give    step-by-step instructions, but generally you should use our answers to do some    refined research on the problem. If you still stuck, we are happy to provide further    support</li> <li>Always give an exact as possible description of the problem. Provide your username,    error messages, the path to the job script, the id of the job, and other hints that make   the problem-solving process as economic as possible.</li> </ul>"},{"location":"code-of-conduct/#housekeeping","title":"Housekeeping","text":"<ul> <li>Clean up your home directory frequently, in particular before asking for an increase of your quota limit</li> <li>Do not save thousands of files in a single directory. Distribute the files to subdirectories</li> </ul>"},{"location":"code-of-conduct/#job-submission","title":"Job Submission","text":"<ul> <li>Before submitting the same job a hundred times, please verify that the job finishes   successfully. We often experience that hundreds of jobs getting killed due to an   invalid path referenced in the job script, which generates hundreds of notification    mails in our system.</li> </ul>"},{"location":"code-of-conduct/#cluster-performance","title":"Cluster Performance","text":"<ul> <li>DO NOT run resource-intensive computations directly on the login nodes AKA submit nodes. This   will have a negative impact on the performance of the whole cluster. Instead, generate a job script   that carries out the computations and submit this job script to the cluster using sbatch.</li> <li>DO NOT run server applications (PostgreSQL server, web server, \u2026) on the login nodes. Such a program usually run as a background process (daemon) rather   than being under the direct control of an interactive user. We will immediately kill such processes.</li> </ul>"},{"location":"teaching/","title":"Teaching with UBELIX","text":"<p>If you plan to use UBELIX for teaching in scheduled courses, we highly recommend that reach out to us for the following reasons:</p> <ul> <li>When possible, we can plan our maintenance sessions outside of your planned teaching / training dates.</li> <li>We can help with the reservation of computing resources so that they are reserved for your course and available the day of your teaching or training session.</li> <li>For hands-on sessions involving UBELIX, every student or trainee needs an active UBELIX account. If we know the list of participants ahead of time, we can already activate the accounts for all participants without the need for every person to register on the first day of the course.</li> </ul> <p>Please contact the UBELIX team to help you in the preparation of your teaching / training session.</p>"},{"location":"costs/faq/","title":"Billing FAQ","text":"<p>You can also find more information on the internal page of UBELIX</p> <p>Q: Will there be a possibility to cap my bill? A: Yes. We will provide a possibility to set-up a price-limit per project. This limit will be evaluated before starting a job. Also, cost-intensive jobs will print a warning-message on submit. Please note that the cost ceiling cannot be guaranteed in all cases, as simultaneous job submissions may cause one job to start before the other is recorded, potentially exceeding the limit.</p> <p>Q: How often will we receive a bill? A: We will send out bills monthly.</p> <p>Q: Is it possible to pre-pay my computing-costs? A: No, because of financial restrictions, we cannot accept money for computing in advance. You can although invest into resources.</p> <p>Q: Can group-leaders monitor usage? A: Yes, costs can be queried by everybody.</p> <p>Q: How does UBELIX on-demand work with billing? A: Exactly the same as with cli-jobs: The chosen resources and walltimes are billed on the selected WCKey</p> <p>Q: What can I do if I plan a course? Will this be billed? A: No, courses will remain for free. Please get in touch at least 14 days ahead of the course.</p> <p>Q: Is the pricing-model SNSF-compatible? A: Yes. Prices for both storage-products as well as computing on UBELIX have been reviewed and accepted by SNSF.</p> <p>Q: Does the pricing scheme also comply with other funders like innosuisse or SBFI A: Most likely, yes. We know from experience that projects already have been funded successfully. Please refer to the grants-office for further discussion.</p> <p>Q: How volatile will the published prices be? A: Prices for Hardware - especially GPUs - tend to be quite volatile. In order to keep our offer stable, we will ask for new prices twice a year. We reserve the right to adjust more frequently.</p> <p>Q: Is the F2-Freetier dependent on group-size? A: No, The calculation is done per project. Please note that if this gets abused, we might have to stop providing that free-tier.</p> <p>Q: How does UBELIX compare to cloud providers? A: As of 2025, UBELIX is 5\u201310x cheaper and includes management, energy, software, storage, staff etc.</p>"},{"location":"costs/freetier/","title":"Free tiers","text":"<p>To allow users without project-funding or with small tasks to run jobs on UBELIX for free, we developed three free tiers on UBELIX:</p>"},{"location":"costs/freetier/#f1-user","title":"F1 - User","text":"<p>The following resources are always available free of charge: - CPU: 1152 TRES run minutes - 2 GPUs (RTX 4090) \u2013 16 cards in pool - 1 GPU (H100) \u2013 8 cards in pool</p>"},{"location":"costs/freetier/#f2-research-group-project","title":"F2 - Research Group / Project","text":"<p>Each research group receives a credit of CHF 1000 per year. The deduction is applied at the end of the fiscal year. We will refund up to CHF 1000.- via internal transfer (\u201cUmbuchung\u201d).</p>"},{"location":"costs/freetier/#f3-preemptable","title":"F3 - Preemptable","text":"<p>Invested resources can be used free of charge if the investor is not using them. Checkpointing is highly recommended!</p>"},{"location":"costs/investments/","title":"Investment Scheme","text":""},{"location":"costs/investments/#overview","title":"Overview","text":"<p>Reserve dedicated resources for 12\u201360 months. Ideal for projects needing consistent compute power and minimal wait times. Offers up to 30% savings vs. PAYG.</p>"},{"location":"costs/investments/#pricing","title":"Pricing","text":"<p>Investors have to pay the CAPEX-cost of hardware minus a bonus for investing. This bonus is 6% per committed year up to 30% (5 years). All other costs to run and maintain the invested hardware, including power, cooling, staff, licenses, e.g. are covered by the IT office of the University of Bern.</p>"},{"location":"costs/investments/#features","title":"Features:","text":"<ul> <li>Pseudo-exclusive access (minimal queuing).</li> <li>Unused resources become free for others via preemptable QOS</li> <li>Funds reinvested in hardware upgrades.</li> </ul>"},{"location":"costs/investments/#limits","title":"Limits","text":"<ul> <li>Investments are limited to resource-types which are already available. If we do not provide a certain type of hardware, please get in touch</li> <li>Investing does not come with a SLA or an extended support-contract.</li> <li>There is an upper and lower limit on investments. Use the calculator and/or contact us if unsure.</li> </ul>"},{"location":"costs/investments/#tools","title":"Tools","text":"<ul> <li>Calculator: Estimate costs at UBELIX Calculator (internal only).</li> <li>Ordering: Submit a PDF offer to the UBELIX team.</li> </ul>"},{"location":"costs/overview/","title":"Costs and Investment","text":"<p>The HPC cluster is run and maintained by the IT office of the University of Bern. It is financed by our users and IT-budget of the directors office.</p> <p>Please note that as of November 2025 there will be a change to the usage-model of our cluster. It is possible to use our resources for free; you can make investments or use the pay-as-you-go-model.</p>"},{"location":"costs/overview/#workspace-storage-costs","title":"Workspace Storage Costs","text":"<p>Price and service details can be found at the official service page.</p> <p>Statement of Cost for SNSF</p> <p>On the official service page of the research storage service you can also find a statement of cost for the service that you can add to your SNF proposal in order to budget money for storage costs in your project.</p>"},{"location":"costs/payg/","title":"Pay-as-you-go (PAYG) Scheme","text":""},{"location":"costs/payg/#overview","title":"Overview","text":"<p>The PAYG model offers flexible, usage-based billing for UBELIX HPC resources. Ideal for sporadic or variable workloads, costs are calculated per minute, with no long-term commitments.</p>"},{"location":"costs/payg/#pricing-structure","title":"Pricing Structure","text":"<p>The Pricing can be found here. Link is only accessible internally or when in UniBE-VPN.</p> <p>Notes: - Billing is per-minute; debug/preemptable jobs are free. - CPU/memory costs use <code>max(cpu, mem)</code>.</p>"},{"location":"firststeps/","title":"Get Started","text":"<p>As with any environment where people collaborate, a shared understanding is essential to foster effective teamwork and ensure a positive experience on UBELIX HPC. Please remember that you are working within a shared system, and your actions can affect the workflows of others. We kindly ask you to review the key guidelines outlined in our Code of Conduct to help maintain a respectful and productive space for everyone.</p> <p>To start using UBELIX, please follow these steps:</p> <ol> <li> <p>Get a user account</p> </li> <li> <p>Set up an SSH key pair (Needed, if you are using UBELIX from a terminal)</p> </li> <li> <p>Log in to UBELIX with SSH client or with the web interface</p> </li> <li> <p>If applicable, move your data to UBELIX</p> </li> <li> <p>Explore your options for using UBELIX</p> </li> </ol> <p>Need help?</p> <p>If you need help getting started on UBELIX, please contact your local IT manager or supervisor if applicable. In case they are not able to provide the support you need, you can always contact the UBELIX support .</p>"},{"location":"firststeps/SSH-keys/","title":"Setting up SSH keypair","text":"<p>Tip</p> <p>A SSH keypair for password-less access within the UBELIX HPC system is automatically generated for all accounts since 12.11.2024. Note that this keypair is automatically rotated and should therefore not be used for any other purpose.</p> <p>If you want to use UBELIX from a terminal, we recommend to register a SSH keypair. SSH keypairs serve as a means of identifying a user to a SSH server. When using SSH keys your password will never be send over the network.</p> <p>UBELIX only accepts SSH keys based on the RSA (4096 bit) or ed25519 algorithms. If possible, we recommend using ed25519.</p>"},{"location":"firststeps/SSH-keys/#generate-your-ssh-keys","title":"Generate your SSH keys","text":"<p>If you already have an appropriate SSH key pair that you want to use with UBELIX, you may skip to registering your public key. If not, start by generating an SSH key pair as detailed below.</p> From a terminal (all OS)With MobaXTerm (Windows) <p>An SSH key pair can be generated using a Linux, macOS, Windows PowerShell terminal. For example, you can use the following command to generate an ed25519 key:</p> <pre><code>ssh-keygen -t ed25519 -f $HOME/.ssh/id_ed25519_ubelix\n</code></pre> <p>You will be asked for a passphrase. Please choose a secure passphrase. It should be at least 8 (preferably 12) characters long and should contain numbers, letters and special characters. Do not leave the passphrase empty.</p> <p>After that an SSH key pair is created, i.e. a pair of files containing the public and private keys, e.g. files named <code>id_ed25519_ubelix</code> (the private key) and <code>id_ed25519_ubelix.pub</code> (the public key) in your <code>/home/&lt;username&gt;/.ssh/</code> directory.</p> <p>An SSH key pair can be generated with MobaXterm (Tools :octicons-arrow-right-16: MobaKeyGen).</p> <p>In order to generate your key pairs for UBELIX, choose the option Ed25519.  Then, press the Generate button.</p> <p> </p> <p>You will be requested to move the mouse in the Key area to generate some  entropy; do so until the green bar is completely filled.</p> <p> </p> <p>After that, enter a comment in the Key comment field and a strong passphrase. Please choose a secure passphrase. It should be at least 8 (preferably 12) characters long and should contain numbers, letters and special characters. Do not leave the passphrase empty.</p> <p> </p> <p>The next step is to save your public and private key. Click on the Save  public key button and save it to the desired location (for example, with  <code>id_ed25519_ubelix.pub</code> as a name). Do the same with your private key by clicking on the Save private key button and save it to the desired location (for  example, with <code>id_ed25519_ubelix</code> as a name).</p> <p>Key format</p> <p>To use your key, you may need two different key formats. The instructions above generate a private key in PuTTY format (PPK), which can be used with PuTTY and in a MobaXTerm SSH session created via the MobaXTerm GUI.</p> <p>However, if you are using the OpenSSH client (the ssh command in a MobaXTerm terminal), you will need a key in the OpenSSH format. To convert your key, go to the Conversions :octicons-arrow-right-16: Export OpenSSH key menu in the key generator tool and save it in the OpenSSH format.</p> <p>You can convert between these two key formats at any time using the key generator tool. Load a key by using the Conversions :octicons-arrow-right-16: Import key menu, then save it in the desired format: </p> <ul> <li>OpenSSH to PPK: load a OpenSSH key, then save it with the Save     private key button</li> <li>PPK to OpenSSH: load a PPK key, then save it via the Conversions     :octicons-arrow-right-16: Export OpenSSH key menu</li> </ul> <p>Warning</p> <p>The private key should never be shared with anyone, not even with UBELIX staff. It should also be stored only on your local computer (public keys can be safely stored in cloud services). Protect it with a good password! Otherwise, anyone with access to the file system can steal your SSH key.</p>"},{"location":"firststeps/SSH-keys/#register-your-public-key","title":"Register your public key","text":"<p>To register your ssh key with your UBELIX account the generated public key need to be added to the <code>~/.ssh/authorized_keys</code> file in your UBELIX account. This step can be done by simply issuing from your terminal:</p> <pre><code>ssh-copy-id -i ~/.ssh/id_ed25519_ubelix.pub `&lt;alias&gt;`\n</code></pre> <p>If the previous steps were successful you can now login to UBELIX using your SSH keypair using </p> <p><pre><code>ssh -i ~/.ssh/id_ed25519_ubelix &lt;user&gt;@submit01.unibe.ch\n</code></pre> Note that you will be prompted for the passphrase of your SSH-key instead of your Campus Account password.</p>"},{"location":"firststeps/SSH-keys/#tweaking-your-ssh-config","title":"Tweaking your SSH config","text":"<p>To simplify the login procedure you can tweak the local SSH configuration by adding a host declaration to <code>~/.ssh/config</code> on your local desktop/laptop. This way we can omit typing the username and ssh-key from our <code>ssh</code> invocation. Note that you will need to substitute your own Campus Account username:</p> <pre><code>~/.ssh/config\n\nHost ubelix\n    Hostname submit02.unibe.ch\n    User &lt;user&gt;\n    IdentityFile ~/.ssh/id_ed25519_ubelix\n    ServerAliveInterval 60\n</code></pre> <p>From now on you can log in to the cluster by using the specified alias:</p> <pre><code>ssh ubelix\n</code></pre> <p>SSH session timeout</p> <p>In the definition above we silently introduced the <code>ServerAliveInterval 60</code> option. If a SSH connection goes idle for a specific amount of time (default 10 minutes), you may be confronted with a <code>\"Write failed: Broken pipe\"</code> error message or the connection is simply frozen, and you are forced to log in again. To prevent this from happening, we configure the client to periodically send a message to trigger a response from the remote server through the <code>ServerAliveInterval</code> option</p>"},{"location":"firststeps/SSH-keys/#ssh-agent","title":"SSH Agent","text":"<p>By this point you have replaced typing your campus account password by typing your ssk-key passphrase every time you login to UBELIX. But you can use the helper tool <code>ssh-agent</code> to securely save your passphrase, so you do not have to re-enter it all the time.</p> <p>The behavior of <code>ssh-agent</code> depends on the flavor and version of your operating system. On MacOS your keys can be saved in the system\u2019s keychain. Most Linux installations will automatically start <code>ssh-agent</code> when you log in.</p> <p>Add the key to ssh-agent:</p> <pre><code>ssh-add ~/.ssh/id_ed25519_ubelix\n</code></pre> <p>You will now have to provide your ssh key\u2019s passphrase once per reboot of your device.</p>"},{"location":"firststeps/accessUBELIX/","title":"Access to UBELIX","text":"<p>To access UBELIX, you need to be a researcher or student at the University of Bern with a staff, student or faculty Campus Account. Note that when using UBELIX, you accept and adhere to the Unibe IT Directives (Terms of Use), the HPC Operational concept as well as our Code of Conduct.</p> <p>To request the activation of your Campus Account, please send a request via https://serviceportal.unibe.ch/hpc including:</p> <ul> <li>the title HPC Account Activation</li> <li>a brief description of what you want to use the cluster for</li> </ul> <p>Multiple Accounts</p> <p>If you possess multiple Campus Accounts (e.g. staff and student) please activate your staff account.</p> <p>Once activated, you will receive a confirmation email containing initial instructions.</p>"},{"location":"firststeps/loggingin-webui/","title":"Logging in (with web interface)","text":"<p>Warning</p> <p>The UBELIX OnDemand portal is still under development. There will be bugs.  Please report them at https://serviceportal.unibe.ch/hpc.</p> <p>We provide interactive HPC access including various interactive apps through the browser-based UBELIX OnDemand service available at https://ondemand.hpc.unibe.ch.</p> <p>Tip</p> <p>UBELIX OnDemand requires an active UBELIX HPC account. Please request an account following the instructions in the documentation before you try to use the OnDemand portal!</p> <ol> <li>Visit https://ondemand.hpc.unibe.ch in your    web browser.</li> <li>You will be greeted by the Unibe Microsoft login screen. Use your @unibe.ch or @students.unibe.ch email address and the    correspoding password to log in.    </li> <li>Once the login is complete you will see the UBELIX OnDemand dashboard:    </li> </ol>"},{"location":"firststeps/loggingin/","title":"Logging in (with an SSH client)","text":"<p>Requirement</p> <p>Login to UBELIX is only possible from within the UniBE network. If you want to connect from outside, you must first establish a VPN connection. For VPN profiles and instructions see the official tutorial.</p> From a terminal (all OS)With MobaXTerm (Windows) <p>Connecting to UBELIX via the command line is possible from all major OS. Once you have activated your account for UBELIX you can connect using an ssh client:</p> <pre><code>$ ssh &lt;user&gt;@submit02.unibe.ch\n</code></pre> <p>where you need to replace <code>&lt;username&gt;</code> with your own Campus Account username.</p> <p>When you connect for the first time, you will also be asked to check the host key fingerprint of the system and need to type <code>yes</code> in order to accept it. The fingerprints of the UBELIX login nodes are listed in the table below. Please make sure that the host key fingerprint matches one of these.</p> <p>At the password prompt enter your Campus Account password:</p> <pre><code>$ ssh &lt;user&gt;@submit03.unibe.ch\nPassword:\n</code></pre> <p>Usually there is no indication of typing when entering your password (not even asterisks or bullets). That\u2019s intended. Just enter your password and press \u2018enter\u2019.</p> <p>After log in successfully you will see the welcome message and the command prompt:</p> <pre><code>Rocky 9.3 Blue Onyx\n\nFQDN:      submit03.ubelix.unibe.ch\nProcessor: 128x AMD EPYC 7742 64-Core Processor\nKernel:    5.14.0-362.13.1.el9_3.x86_64\nMemory:    128.223 GB\n\n[user@submit03 ~]$\n</code></pre> <p>MobaXterm combines Terminal sessions with file transfer (scp/ftp) and X Window Server. There are many more features which are not described here. MobaXterm can be downloaded on the MobaXterm Website. There are two versions, portable and installation. You can choose either one.</p> <p>After installing and starting MobaXterm, a SSH session need to be configured:</p> <ul> <li>Click \u2018Session\u2019 in the top left corner:  </li> <li>In \u201cSSH\u201d tab:<ul> <li>Set the remote host to a login node, e.g. submit01.unibe.ch</li> <li>Enable the \u201cSpecify username\u201d option and put your Campus Account short name in the corresponding box (here user ms20e149 will be used)</li> </ul> </li> <li> <p>In the \u201cAdvanced SSH settings\u201d</p> <ul> <li>Set SSH-browser type to \u2018SCP (enhanced speed)\u2019</li> <li>Optionally, tick the \u2018Follow SSH path\u2019 button </li> </ul> </li> <li> <p>From now one the settings are stored and you can access the session on the left at the star icon </p> </li> <li> <p>MobaXterm will ask you to store the Password and manage a MasterPassword. </p> </li> </ul> <p>After starting the session, you should see the UBELIX login message and prompt.   On the left hand side a File browser is located. There the UBELIX file system can be browsed and files up or downloaded, e.g. using drag and drop or the context menue.  </p> <p>Login nodes</p> <p>There are four login nodes in UBELIX:</p> <ul> <li>submit01.unibe.ch</li> <li>submit02.unibe.ch</li> <li>submit03.unibe.ch</li> <li>submit04.unibe.ch</li> </ul> <p>To access UBELIX, you can choose any one. If the load on a login node is high, you can log out and pick another one.</p>"},{"location":"firststeps/loggingin/#host-key-fingerprints","title":"Host key fingerprints","text":"Host Fingerprint submit01.unibe.ch SHA256:ScUNCxSnHzfxeLIVB8WY98EvKoVhBujbz6RXaSOw1VA (RSA) SHA256:qmMfIbwyosfLUsY8BMCTgj6HjQ3Im6bAdhCWK9nSiDs (ED25519) submit02.unibe.ch SHA256:eRTZGWp2bvlEbl8O1pigcONsFZAVKT+hp+5lSQ8lq/A (ED25519) SHA256:Q4M+Tv3XsAuFLPdv2t/x3X3QY3IDsKMbNBwFcLoQeS8 (RSA) submit03.unibe.ch SHA256:IYp6P61TybgGdbBd4w9+bQpIk8Yc3XfYLHG0CslCs6Y (RSA) SHA256:PUkldwWf86h26PSFHCkEGKsrYlXv668LeSnbHBrMoCQ (ED25519) submit04.unibe.ch SHA256:+maM19oNjmc6wyHyQSq1eqNtFKPPrEs+fdQm5maZyN8 (RSA) SHA256:D3cmfXkb40P7W935J2Un8sBUd4Sv2MNLkvz9isJOnu0 (ED25519)"},{"location":"firststeps/loggingin/#troubleshooting","title":"Troubleshooting","text":"<p>If you have trouble connecting to UBELIX, you can run the SSH client with verbose output enabled to get more information about what happens when you try to connect:</p> <pre><code>ssh -vvv &lt;username&gt;@submit02.unibe.ch\n</code></pre> <p>If you are unable to connect, and you contact the UBELIX support , we recommend that you provide the output of this command as part of your support request. Please include the output as text (copy from terminal, paste into support request), not as pictures.</p>"},{"location":"firststeps/movingdata/","title":"Moving data to/from UBELIX","text":"<p>For moving data to/from UBELIX, we recommend the use of the <code>scp</code> and <code>rsync</code> tools. See the data storage options page for an overview of where to store your data on UBELIX.</p> <p>In situations where you need to simply download data from the web, you can use tools like <code>wget</code> and <code>curl</code>.</p>"},{"location":"firststeps/movingdata/#copying-files-with-scp","title":"Copying files with <code>scp</code>","text":"<p>Copying files between different UNIX-like systems can be done with the <code>scp</code> command. This command, which stands for Secure Copy Protocol, allows you to transfer files between a local host and a remote host or between two remote hosts. The basic syntax of the <code>scp</code> command is the following:</p> <pre><code>scp &lt;origin-path&gt; [user@]host:&lt;destination-path&gt;\nscp [user@]host:&lt;origin-path&gt; &lt;destination-path&gt;\n</code></pre> <p>where <code>&lt;origin-path&gt;</code> is the path to the file you want to copy to the  destination defined by <code>&lt;destination-path&gt;</code>.</p> <p>Some common <code>scp</code> options:</p> <pre><code>-r copy directories recursively (Note that SCP follows symbolic links encountered in the tree traversal)\n-p preserve modification time, access time, and modes from the original file\n-v verbose mode\n</code></pre>"},{"location":"firststeps/movingdata/#copying-files-to-ubelix","title":"Copying Files to UBELIX","text":"<p>Copy the file <code>~/dir/file01</code> to your remote home directory:</p> <pre><code>$ scp ~/dir/file01 &lt;user&gt;@submit03.unibe.ch:\n</code></pre> <p>Copy multiple files to the remote directory <code>~/bar</code>:</p> <p>Note</p> <p>The destination directory must already exist. You can create a directory from remote with: <code>ssh &lt;user&gt;@submit03.unibe.ch 'mkdir -p ~/bar'</code></p> <pre><code>$ scp ~/dir/file01 ~/dir/file02 ~/dir/file03 &lt;user&gt;@submit03.unibe.ch:bar\n</code></pre> <p>Copy the directory <code>~/dir</code> to the remote directory <code>~/bar</code> using the <code>-r</code> (recursive) option:</p> <pre><code>$ scp -r ~/dir &lt;user&gt;@submit03.unibe.ch:~/bar/\n</code></pre>"},{"location":"firststeps/movingdata/#copying-files-from-ubelix","title":"Copying Files from UBELIX","text":"<p>Copy the remote file <code>~/bar/file01</code> to the current working directory on your local workstation:</p> <pre><code>$ scp &lt;user&gt;@submit03.unibe.ch:bar/file01 .\n</code></pre> <p>Copy multiple remote files to the local directory <code>~/dir</code>:</p> <pre><code>$ scp &lt;user&gt;@submit03.unibe.ch:bar/\\{file02,file03,file04\\} ~/dir\n</code></pre> <p>The local directory <code>~/dir</code> will be automatically created if it does not already exist</p> <p>Copy the remote directory <code>~/bar</code> to the current working directory on your local workstation:</p> <pre><code>$ scp -r &lt;user&gt;@submit03.unibe.ch:bar .\n</code></pre>"},{"location":"firststeps/movingdata/#copying-files-with-rsync","title":"Copying files with <code>rsync</code>","text":"<p>The <code>rsync</code> tool, which stands for Remote Sync, is a remote and local file synchronization tool. It has the advantage of minimizing the amount of data copied by only copying files that have changed. The advantages over <code>scp</code> are</p> <ul> <li>It allows for synchronization. <code>scp</code> always copies and transfers everything,   while <code>rsync</code> will only copy and transfer files that have changed.</li> <li>Better for the transfer of large files as <code>rsync</code> can save progress.   If the transfer is interrupted it can be resumed from the point of interruption.</li> </ul> <p>The basic syntax of the <code>rsync</code> command is the following:</p> <pre><code>rsync &lt;options&gt; &lt;origin-path&gt; &lt;destination-path&gt;\nrsync &lt;options&gt; &lt;origin-path&gt; [user@]host:&lt;destination-path&gt;\nrsync &lt;options&gt; [user@]host:&lt;origin-path&gt; &lt;destination-path&gt;\n</code></pre> <p>Common options:</p> <pre><code>-r copy directories recursively (does not preserve timestamps and permissions)\n-a archive mode (like -r, but also preserves timestamps, permissions, ownership, and copies symlinks as symlinks)\n-z compress data\n-v verbose mode (additional v's will increase verbosity level)\n-n dry-run\n-h output numbers in a human readable format\n</code></pre> <p>To copy the contents of remote directory <code>~/foo/</code> to the local directory <code>~/dir</code>:</p> <pre><code>$ rsync -az &lt;user&gt;@submit03.unibe.ch:foo/ ~/dir\n</code></pre> <p>Note</p> <p>With a trailing slash (<code>/</code>) after the source directory only the content of the source directory is copied to the destination directory. Without a trailing slash both the source directory and the content of the directory are copied to the destination directory.</p>"},{"location":"firststeps/movingdata/#copying-files-with-ondemand","title":"Copying files with OnDemand","text":"<p>You can also use the interactive UBELIX OnDemand portal to upload or download (small) files from/to UBELIX. Access the Files App from the top menu bar under <code>Files &gt; Home Directory</code>. </p>"},{"location":"firststeps/nextsteps/","title":"Next steps","text":"<p>Depending on your needs of UBELIX, you may want to take a look at:</p> <ul> <li> <p>The Quickstart section to get a brief overview on the basic UBELIX workflow.</p> </li> <li> <p>The Hardware section to get to know the UBELIX system architecture and the different hardware partitions available on UBELIX.</p> </li> <li> <p>The Run jobs section to identify ways to run computations on UBELIX.</p> </li> <li> <p>The Software section to figure out how to get your favorite piece of scientific software available on UBELIX.</p> </li> <li> <p>The Support section to get in contact with the UBELIX Support Team, in case you need further help using UBELIX.</p> </li> </ul>"},{"location":"firststeps/quickstart/","title":"Quick Start","text":"<p>This section is intended as a brief introduction into the UBELIX workflow. This page is an summary, a hands-on introduction, which targets primarily users without prior knowledge in high-performance computing. However, basic Linux knowledge is a prerequisite. If you are not familiar with basic Linux commands, there are many beginner tutorials available online. After reading this page you will have composed and submitted your first job successfully to the cluster. Links are provided throughout the text to point you to more in-depth information on the topic.</p>"},{"location":"firststeps/quickstart/#prerequisites","title":"Prerequisites","text":"<p>In order to follow these steps you have succefully managed to:</p> <ul> <li>login to the UBELIX cluster shell via <code>ssh</code>or the web interface</li> <li>copy data from/to the UBELIX cluster via <code>scp/rsync</code> or the web interface</li> </ul>"},{"location":"firststeps/quickstart/#use-software","title":"Use Software","text":"<p>On UBELIX you can make use of already pre-installed software or you can compile and install your own software. We use a module system to manage software packages, even different versions of the same software. This allows you to focus on getting your work done instead of compiling software. E.g. to get a list of all provided packages:</p> <pre><code>module avail\n</code></pre> <p>Workspace software stacks</p> <p><code>module spider</code> or <code>module avail</code> will only find packages in a Workspace software stack if the <code>Workspace</code> module for that workspace is loaded</p> <p>Furthermore, we suggest to work with so called toolchains. These are collections of modules build on top of each other. </p> <p>To set the environment for a scientific application with Python, load:</p> <pre><code>$ module load Anaconda3\n$ eval \"$(conda shell.bash hook)\"\n</code></pre> <p>To set the environment for compiling a scientific application with math libraries, OpenMPI and GCC, load:</p> <pre><code>$ module load foss\n$ module list\n\nCurrently Loaded Modules:\n  1) GCCcore/12.3.0                          9) OpenSSL/1.1                      17) OpenBLAS/0.3.23-GCC-12.3.0\n  2) binutils/.2.40-GCCcore-12.3.0     (H)  10) UCX/1.14.1-GCCcore-12.3.0        18) FlexiBLAS/3.3.1-GCC-12.3.0\n  3) GCC/12.3.0                             11) libfabric/1.18.0-GCCcore-12.3.0  19) FFTW/3.3.10-GCC-12.3.0\n  4) numactl/2.0.16-GCCcore-12.3.0          12) zlib/1.2.13-GCCcore-12.3.0       20) gompi/2023a\n  5) XZ/.5.4.2-GCCcore-12.3.0          (H)  13) libevent/2.1.12-GCCcore-12.3.0   21) FFTW.MPI/3.3.10-gompi-2023a\n  6) libxml2/.2.11.4-GCCcore-12.3.0    (H)  14) PMIx/4.2.4-GCCcore-12.3.0        22) ScaLAPACK/2.2.0-gompi-2023a-fb\n  7) libpciaccess/.0.17-GCCcore-12.3.0 (H)  15) UCC/1.2.0-GCCcore-12.3.0         23) foss/2023a\n  8) hwloc/2.9.1-GCCcore-12.3.0             16) OpenMPI/4.1.5-GCC-12.3.0\n\n  Where:\n   H:  Hidden Module\n</code></pre> <p>Scope</p> <p>The loaded version of a software is only active in your current session. If you open a new shell you are again using the default version of the software. Therefore, it is crucial to load the required modules from within your job script.</p> <p>But also keep in mind that the current environment will get forwarded into a job submitted from it. This may lead to conflicting versions of loaded modules and modules loaded in the script. </p> <p>The Software section is dedicated to this topic. More information can be found there.</p>"},{"location":"firststeps/quickstart/#hello-world","title":"Hello World","text":"<p>Currently you are on a submit server also known as login node. This server is for preparing the computations, i.e. downloading data, writing a job script, prepare some data etc. But you are not allowed to run computations on login nodes! So, you have to bring the computations to the compute nodes - by generating a job script and sending it to the cluster.</p> <p>Working interactively on a compute node</p> <p>When developing stuff it\u2019s often useful to have short iterations of try-error. Therefore it\u2019s also possible to work interactively on a compute node without having to send jobs to the cluster and wait until they finish just to see it didn\u2019t work. See Interactive Jobs for more information about this topic.</p> <p>To do some work on the cluster, you require certain resources (e.g. CPUs and memory) and a description of the computations to be done. A job consists of instructions to the scheduler in the form of option flags, and statements that describe the actual tasks. Let\u2019s start with the instructions to the scheduler:</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=1GB\n\n# Put your code below this line\n...\n</code></pre> <p>The first line makes sure that the file is executed using the bash shell. The remaining lines are option flags used by the <code>sbatch</code> command. The page Jobs Submission outlines the most important options of <code>sbatch</code>.</p> <p>Now, let\u2019s write a simple \u201chello, world\u201d-task:</p> <pre><code>...\n# Put your code below this line\nmodule load Workspace_Home\necho \"Hello, UBELIX from node $(hostname)\" &gt; hello.txt\n</code></pre> <p>After loading the Workspace module, we print the line <code>Hello, UBELIX from node &lt;hostname_of_the_executing_node&gt;</code> and redirect the output to a file named <code>hello.txt</code>. The expression <code>$(hostname)</code> means, run the command <code>hostname</code> and put its output here. Save the content to a file named <code>first.sh</code>.</p> <p>The complete job script looks like this:</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=1GB\n\n# Put your code below this line\nmodule load Workspace_Home\necho \"Hello, UBELIX from node $(hostname)\" &gt; hello.txt\n</code></pre>"},{"location":"firststeps/quickstart/#schedule-your-job","title":"Schedule Your Job","text":"<p>We can now submit our first job to the scheduler. The scheduler will then provide the requested resources to the job. If all requested resources are already available, then your job can start immediately. Otherwise your job will wait until enough resources are available. We submit our job to the scheduler using the <code>sbatch</code> command:</p> <p><pre><code>sbatch first.sh\n</code></pre> <pre><code>Submitted batch job 32490640\n</code></pre> If the job is submitted successfully, the command outputs a job-ID with which you can refer to your job later on. There are various options for different types of jobs provided in the scheduler. See sections Array Jobs, GPUs, and Interactive Jobs for more information</p>"},{"location":"firststeps/quickstart/#monitor-your-job","title":"Monitor Your Job","text":"<p>You can inspect the state of our active jobs (running or pending) with the squeue command:</p> <p><pre><code>squeue --job=32490640\n</code></pre> <pre><code>      JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n   32490640     epyc2    job01 testuser  R       0:22      1 bnode23\n</code></pre></p> <p>Here you can see that the job \u2018job01\u2019 with job-ID 32490640 is in state RUNNING (R). The job is running in the \u2018epyc2\u2019 partition (default partition) on bnode23 for 22 seconds. It is also possible that the job can not start immediately after submitting it to SLURM because the requested resources are not yet available. In this case, the output could look like this:</p> <p><pre><code>squeue --job=32490640\n</code></pre> <pre><code>       JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n    32490640     epyc2    job01 testuser PD       0:00      1 (Priority)\n</code></pre></p> <p>Here you can see that the job is in state PENDING (PD) and a reason why the job is pending. In this example, the job has to wait for at least one other job with higher priority. </p> <p>You can always list all your active (pending or running) jobs with squeue:</p> <p><pre><code>squeue --me\n</code></pre> <pre><code>      JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n   34651451     epyc2 slurm.sh  testuser PD       0:00      2 (Priority)\n   34651453     epyc2 slurm.sh  testuser PD       0:00      2 (Priority)\n   29143227     epyc2     Rjob  testuser PD       0:00      4 (JobHeldUser)\n   37856328       bdw   mpi.sh  testuser  R       4:38      2 anode[012-014]\n   32634559       bdw  fast.sh  testuser  R    2:52:37      1 anode12\n   32634558       bdw  fast.sh  testuser  R    3:00:54      1 anode14\n   32634554       bdw  fast.sh  testuser  R    4:11:26      1 anode08\n   32633556       bdw  fast.sh  testuser  R    4:36:10      1 anode08\n</code></pre></p> <p>Further information on on job monitoring you find on page Monitoring Jobs. Furthermore, in the Job handling section you find additional information about Investigating a Job Failure and Check-pointing. </p>"},{"location":"hardware/","title":"Overview","text":"<p>UBELIX (University of Bern Linux Cluster) is a HPC cluster that currently features ~12k CPU cores and 160 GPUs. It is a heterogeneous cluster, meaning UBELIX consists of different generations of compute nodes. Compute nodes, front-end servers and the storage are interconnected through a high speed Infiniband network. UBELIX is used by various institutes and research groups within all faculties at the University of Bern.</p>"},{"location":"hardware/#high-level-system-overview","title":"High-level system overview","text":"<p>UBELIX can only be reached within the university network. User landing point are the login nodes, where jobs can be prepared and submitted. Computational tasks are scheduled and managed on the compute nodes using SLURM, the workload manager. All compute nodes as well as the login nodes have access to the parallel file system.</p> <p>UBELIX is a heterogeneous machine, consisting of different architectures. The majority of the CPU compute power in UBELIX is found in the AMD (epyc) hardware partition that features AMD EPYC CPUs and a smaller Intel Broadwell (bdw) partition with Intel Xeon processors. In addition to this, there is a GPU partition which features GPU accelerated nodes with a variety of GPUs.</p>"},{"location":"hardware/cpu/","title":"CPU nodes","text":"<p>The UBELIX CPU-based hardware partition currently consists of about 250 compute nodes. Due to their different architectures, their specifications differ as shown in the table below.</p> Nodes CPUs CPU cores Memory Disk Network 12 2x AMD EPYC 9654(2.4 GHz base, 3.55 GHz boost) 192(2x96) 1.5 TiB 1.92TiB 100Gb/s 68 2x AMD EPYC 7742(2.25 GHz base, 3.4 GHz boost) 128(2x64) 1 TiB 240GiB 100Gb/s 170 2x Intel Xeon E5-2630 (2.2 GHz base, 3.1 GHz boost) 20(2x10) 128 GiB 1TiB (HDD) 40 Gb/s <p>See the Slurm partitions page for an overview of options for allocating these nodes.</p>"},{"location":"hardware/cpu/#overview","title":"Overview","text":"Overview of a UBELIX CPU compute node"},{"location":"hardware/cpu/#cpu","title":"CPU","text":"<p>Each UBELIX CPU compute node is equipped with two CPU sockets with up to 96 cores each, depending on the node type.</p>"},{"location":"hardware/cpu/#memory","title":"Memory","text":"<p>The CPU compute nodes are equipped with roughly 8GB of DDR memory per CPU core on the AMD EPYC nodes and about 6GB per CPU core on the Intel Xeon nodes.</p>"},{"location":"hardware/cpu/#network","title":"Network","text":"<p>Depending on their generation, the UBELIX CPU compute nodes each have a single 40Gb/s or 100 Gb/s interface to the Infiniband interconnect.</p>"},{"location":"hardware/cpu/#disk-storage","title":"Disk storage","text":"<p>Depending on the node type the CPU compute nodes are equipped with up to 1.5TiB of local NVMe SSD storage avaible as local scratch for high-performance I/O operations. See the UBELIX data storage options for details.</p>"},{"location":"hardware/gpu/","title":"GPU nodes","text":"<p>The UBELIX GPU-based hardware partition currently consists of about 20 compute nodes. Due to their different architectures, their specifications differ as shown in the table below.</p> Nodes GPUs CPU cores GPU Memory Network 6 8x NVIDIA RTX3090 32 (2x16) 24 GB 40Gb/s or 100Gb/s 8 8x NVIDIA RTX4090 128 (2x64) 24 GB 100Gb/s 1 6x NVIDIA A100 128 (2x64) 80 GB 100Gb/s 5 8x NVIDIA H100 128 (2x64) 96 GB 100Gb/s 2 8x NVIDIA H200 128 (2x64) 141 GB 100Gb/s <p>See the Slurm partitions page for an overview of options for allocating these nodes.</p>"},{"location":"hardware/gpu/#overview","title":"Overview","text":"Overview of a UBELIX H100 GPU compute node"},{"location":"hardware/gpu/#gpu","title":"GPU","text":"<p>Each GPU compute node is equipped with eight GPUs of a given type with up to 96GB GPU memory (H100) depending on the node type.</p>"},{"location":"hardware/gpu/#cpu-memory","title":"CPU &amp; Memory","text":"<p>Due to the different GPU compute node generations the amount of CPU core and memory per node differs. In terms of available memory per GPU this can be summarized as follows:</p> GPU Type CPUs per GPU Memory per GPU Nvidia RTX 3090 4 60GB Nvidia RTX 4090 16 90GB Nvidia A100 20 80GB Nvidia H100 16 90GB Nvidia H200 16 90GB"},{"location":"hardware/gpu/#network","title":"Network","text":"<p>Depending on their generation, the UBELIX CPU compute nodes each have a single 40Gb/s or 100 Gb/s interface to the Infiniband interconnect.</p>"},{"location":"hardware/gpu/#disk-storage","title":"Disk storage","text":"<p>Depending on the node type the CPU compute nodes are equipped with up to 1.5TiB of local NVMe SSD storage avaible as local scratch for high-performance I/O operations. See the UBELIX data storage options for details.</p>"},{"location":"hardware/network/","title":"Network and interconnect","text":"<p>All UBELIX compute nodes use the Infiniband interconnect 40 Gbps or 100 Gbps network interconnect (NIC).</p>"},{"location":"hardware/network/#topology","title":"Topology","text":"<p>UBELIX mostly uses the fat-tree network topology to enable fast and efficient communication between the different compute nodes in the cluster. Due to the moderate size of the UBELIX cluster we can operate a two-level topology that consists of two layers of switches:</p> <ol> <li>Leaf switches (connected to the nodes)</li> <li>Top switches (connecting the leaf switches to each other)</li> </ol> <p>Each node connects to a leaf switch, and each leaf switch is connected to every top switch. This creates multiple paths for data to travel between nodes. The multiple paths between individual components of the cluster prevent bottlenecks, allowing smooth data flow. Additionally this topology includes redundancy at the switch level, allowing to reroute traffic should a path fail.</p> <p>The network is designed to handle many simultaneous data transfers with little to no delay, providing high performance under load.This is represented by a low blocking factor. Blocking refers to how much the network might limit data transfers when many nodes are communicating at once. For most nodes, UBELIX operates a fully non-blocking fat-tree, so that no bottlenecks due to network congestion exist.</p> Example fat tree topology."},{"location":"runjobs/","title":"Overview","text":"<p>Here you find general descriptions of how to run jobs on UBELIX, i.e. how to run your scientific software using the job scheduler, general information about the UBELIX environment, as well as the new UBELIX web interface that you can use instead of the traditional approaxh using a terminal. If you are looking for ways to install your software on UBELIX or advice for a specific application, consult the software section instead.</p> <p>When you log in to UBELIX, you access one of the login nodes. These login nodes are shared by all users and are only intended for simple management tasks, e.g.</p> <ul> <li>compiling software (but consider allocating a compute node for large build   jobs)</li> <li>submitting and managing scheduled jobs</li> <li>moving data</li> <li>light pre- and postprocessing (a few cores / a few GB of memory)</li> </ul> <p>All compute heavy tasks must be submitted through the job scheduler such that they are run on the compute nodes.</p> <p>Warning</p> <p>All tasks not adhering to the above fair use rules for the login nodes will be terminated without warning.</p> <p>Before you can run jobs on UBELIX you will need to </p> <ul> <li>install your software stack</li> <li>understand the type of job your trying to run and find suitable partition</li> <li>decide where to put your data and get your data there</li> <li>create a job script to launch your job</li> <li>submit the job</li> </ul> <p>If you\u2019re a new user please familiarize yourself with the UBELIX environment:</p> <ul> <li>Read the Slurm quickstart guide to get started submitting  jobs through the UBELIX job scheduler.</li> <li>Read the module environment page to learn how to use the module system on UBELIX to find already installed software and to manage your own software installations.</li> <li>Read the data storage options page to learn more   about where to store your data.</li> </ul>"},{"location":"runjobs/partitions/","title":"SLURM partition and QOS","text":"<p>UBELIX provides different CPU and GPU architectures. These are generally structured in partitions and further tunable by \u201cQuality of Service\u201d (QoS).</p>"},{"location":"runjobs/partitions/#partitions","title":"Partitions","text":"<p>We are currently operating the following partitions:</p> Partition job type CPU / GPU node / GPU memory local Scratch epyc2 (default) single and multi-core AMD Epyc2 2x64 cores  AMD Epyc4 2x96 cores 1TB  1.5TB 1TB bdw full nodes only (x*20cores) Intel Broadwell 2x10 cores 156GB 1TB gpu GPU  (8 GPUs per node,  varying CPUs) Nvidia RTX 3090  Nvidia RTX 4090  Nvidia A100  Nvidia H100  Nvidia H200 24GB  24GB  80GB  96GB  141GB 1.92TB  1.92TB  1.92TB  1.92TB  1.92TB gpu-invest GPU see gpu partition icpu-investor single and multi-core see epyc2 partition <p>The current usage can be listed on the UBELIX status page</p>"},{"location":"runjobs/partitions/#qos","title":"QoS","text":"<p>Within these partitions, QoS are used to distinguish different job limits. Each QoS has a specific purpose, e.g. to allow quick debug jobs to schedule faster than regular jobs.</p> <p>The following QoS are defined on UBELIX:</p> QoS Time limit Max Jobs Partitions Description job_cpu 96 hours 20000 epyc2,bdw This is the default CPU qos. It\u2019s used for all general computing. job_cpu_long 16 days 50 epyc2,bdw This CPU qos is used for very long jobs. Note: Checkpointing is recommended! job_cpu_debug 20 min 1 epyc2,bdw This CPU qos is used for quick debug jobs (max 10 cores) job_gpu 24 hours gpu This is the default GPU qos. It\u2019s used for general GPU computing. job_gpu_debug 20 min 1 gpu This GPU qos is used for quick debug jobs on GPUs (max 1 GPU). job_gpu_preemptable 24 hours gpu-invest This GPU qos is used to request idle investor GPU resources. See the note below for details! job_gpu_investor gpu-invest These GPU qos are used by investors to request their GPU resources. job_interactive 8 hours 1 all This qos is used for interactive CPU/GPU jobs (i.e, OnDemand). Jobs are assigned higher priority to start quickly. job_icpu-investor icpu-investor These CPU qos are used by investors to request their CPU resources. <p>Some QoS have more specific resource limits associated to them, e.g. the number of GPUs that can be requested per user. These limits can be viewed using the <code>sqos</code> command:</p> <pre><code>sqos -h\nUsage: ./sqos [partition_name | qos_name]\n\nIf a partition name is given, it retrieves all QoS associated with that partition as per slurm.conf.\nIf a QoS name is given, it displays the details for that specific QoS.\nWithout arguments, the script shows all QoS for the current user.\n\nExamples:\n  sqos                   # Show all QoS for the current user\n  sqos partition_name    # Show QoS for the specified partition\n  sqos qos_name          # Show details for the specified QoS\n</code></pre>"},{"location":"runjobs/partitions/#investor-qos","title":"Investor QoS","text":"<p>Investors get pseudo-exclusive access to their invested resources. The membership to these investor qos is managed by the investor or their deputy. Membership changes need to be communicated to the HPC team. For details on investing in UBELIX see Costs and Investments.</p>"},{"location":"runjobs/partitions/#preemptable-qos","title":"Preemptable QoS","text":"<p>The resources dedicated to investors can be used by non-investing users too. Idle investor resources can be used by jobs with the QOS <code>job_gpu_preemptable</code>. However, these preemptable jobs may be terminated by investor jobs at any time! If the job has been terminated to free resources for the investor, the preemptable job is rescheduled in the queue. This makes the qos <code>job_gpu_preemptable</code> especially suitable to jobs that support automatic checkpointing or restarts.</p>"},{"location":"runjobs/ondemand/","title":"Welcome to UBELIX OnDemand","text":"<p>Warning</p> <p>This is a pre-release version of the UBELIX OnDemand portal. There will be bugs. Please report them at https://serviceportal.unibe.ch/hpc.</p> <p>UBELIX OnDemand is an exciting new service providing easy access to the HPC system at Unibe, designed with user-friendliness in mind.</p> <p>With the new OnDemand portal available at https://ondemand.hpc.unibe.ch, you can:</p> <ul> <li>Browse Files: Easily navigate your files, with the option to upload and download smaller files for quick access and management.</li> <li>View Jobs: Keep track of your currently running jobs, helping you stay informed about your computational tasks.</li> <li>Web-Based Shell Sessions: Start a web-based shell session on a UBELIX submit node, allowing you to interact with the system without needing a local terminal.</li> <li>Jupyter Lab/Notebook: Launch a Jupyter Lab/Notebook server on a compute node directly from your browser, ideal for data analysis, scientific computing, and interactive coding.</li> <li>(VS)Code Server: Start a (VS)Code server on a compute node, enabling you to code and develop directly in your browser with the powerful features of Visual Studio Code.</li> </ul> <p>Please note that while UBELIX OnDemand offers a user-friendly, interactive interface, it does not replace the command line login to UBELIX. The command line remains the more powerful option for performing extensive and complex tasks, offering greater flexibility and control over your computing environment. UBELIX OnDemand is a new low barrier entry to the HPC system at Unibe.</p>"},{"location":"runjobs/ondemand/#ondemand-overview","title":"OnDemand Overview","text":"<p>We provide interactive HPC access including various interactive apps through the browser-based UBELIX OnDemand service available at https://ondemand.hpc.unibe.ch.</p>"},{"location":"runjobs/ondemand/#loggin-in","title":"Loggin In","text":"<p>Tip</p> <p>UBELIX OnDemand requires an active UBELIX HPC account. Please request an account following the instructions in the documentation before you try to use the OnDemand portal!</p> <ol> <li>Visit https://ondemand.hpc.unibe.ch in your    web browser.</li> <li>You will be greeted by the Unibe Microsoft login screen. Use your @unibe.ch or @students.unibe.ch email address and the    correspoding password to log in.    </li> <li>You will be asked to grant the OnDemand    access to some of your profile information including your email. This is    required to map your adress to a local user account.</li> <li>Once the login is complete you will see the UBELIX OnDemand dashboard:    </li> </ol>"},{"location":"runjobs/ondemand/#files-app","title":"Files App","text":"<p>Access the Files App from the top menu bar under <code>Files &gt; Home Directory</code>. Using the Files App, you can use your web browser to:</p> <ul> <li>View files in the UBELIX filesystem.</li> <li>Create and delete files and directories.</li> <li>Upload and download (small) files from the UBELIX filesystem to your computer.</li> </ul> <p></p>"},{"location":"runjobs/ondemand/#view-active-jobs","title":"View Active Jobs","text":"<p>View and cancel active Slurm jobs from <code>Jobs &gt; Active Jobs</code>. This includes jobs started via <code>sbatch</code> and <code>srun</code>as well as jobs started (implicitly) via UBELIX OnDemand.</p>"},{"location":"runjobs/ondemand/#shell-access","title":"Shell Access","text":"<p>UBELIX OnDemand allows easy browser based UBELIX shell access from the top menu bar under <code>Clusters &gt; UBELIX Shell Access</code>.</p>"},{"location":"runjobs/ondemand/#interactive-apps","title":"Interactive Apps","text":"<p>UBELIX OnDemand provides interactive apps. You can launch interactive apps from the <code>Interactive Apps</code> menu on the top menu bar. The available interactive apps include:</p> <ul> <li> Jupyter Server (for working with Jupyter notebooks)</li> <li> RStudio Server (for working in RStudio sessions)</li> <li> Code Server (VS Code) (for code editing using Visual Studio    Code)</li> </ul>"},{"location":"runjobs/ondemand/code-server/","title":"(VS)Code server","text":"<p>The (VS)Code server allows you to use the (VS)Code editor on UBELIX from your browser.</p> <ol> <li>Select the (VS)Code Server app under Interactive Apps    </li> <li>Provide the job specification you want for the (VS)Code Server    </li> <li>Once your server is ready, click <code>Connect to VS Code</code> to access it.    </li> </ol> <p>Warning</p> <p>The job may still be running if you close the window or log out. When you are done, shut down (VS)Code Server by clicking <code>Delete</code> on the session under My Interactive Sessions. You can confirm that the interactive session has stopped by checking My Interactive Sessions.</p>"},{"location":"runjobs/ondemand/code-server/#known-limitations","title":"Known limitations","text":"<p>coming soon</p>"},{"location":"runjobs/ondemand/jupyter/","title":"Jupyter","text":"<p>The Jupyter server allows you to use the start interactive Python editors on UBELIX from your browser.</p> <ol> <li>Select the Jupyter Notebook app under Interactive Apps        </li> <li>Provide the job specification you want for the Jupyter Server    </li> <li>Once your server is ready, click <code>Connect to Jupyter</code> to access it.    </li> </ol> <p>Warning</p> <p>The job may still be running if you close the window or log out. When you are done, shut down Jupyter by clicking <code>Delete</code> on the session under My Interactive Sessions. You can confirm that the interactive session has stopped by checking My Interactive Sessions.</p> <p>At this point you should already have a fully working Jupyter environment. To start working with Jupyter Notebooks, please also see the Jupyter Documentation.</p>"},{"location":"runjobs/ondemand/jupyter/#loading-software-modules","title":"Loading software modules","text":"<p>You can load additional software modules directly from within Jupyter using the modules extensions:</p> <ol> <li>Choose the Module View from the left side panel</li> <li>Search for the desired module in the search top search bar</li> <li>Load/Unload modules as required</li> </ol> <p></p>"},{"location":"runjobs/ondemand/jupyter/#installing-python-packages","title":"Installing Python packages","text":"<p>A variety of standard Python packages (such as numpy, scipy, matplotlib and pandas) are available automatically. To see what packages are available, open a Terminal notebook or open a Terminal on UBELIX. Then load the Ananaconda3 module and list the installed packages:</p> <pre><code>module load Anaconda3/2024.02-1\neval \"$(conda shell.bash hook)\"\nconda list\n</code></pre> <p>There should be no issues using conda or pip to install or upgrade packages and then use them in a Jupyter notebook, but you will need to make sure to install the new versions or additional packages in your home or scratch directories because you do not have write permissions to the module directories. If you\u2019d like to install packages with <code>conda install</code> you\u2019ll need to create a conda environment in which to install packages and then create a kernel associated with your Conda environment as discussed in the next section. If you need just a few extra packages you can use <code>pip install --user &lt;package&gt;</code> to install the missing packages.</p> <p>Tip</p> <p>We recommend using conda environments over pip installed packages as these are less prone to package conflicts during system upgrades. If you\u2019re experienced with Python virtualenvs you may choose these as well.</p>"},{"location":"runjobs/ondemand/jupyter/#adding-new-kernels","title":"Adding new kernels","text":"<p>Jupyter supports notebooks in dozens of languages, including IPython, R, Julia, etc. If you\u2019d like to use a language not already present in the default Jupyter configuration, you\u2019ll need to create your own kernel. You may also need to create your own kernel for a language already supported if you want to customize your environment. For example if you\u2019d like to work in your conda environments when using your notebook.</p>"},{"location":"runjobs/ondemand/jupyter/#adding-a-conda-environment-to-jupyter","title":"Adding a conda environment to Jupyter","text":"<p>You can add a new (Python) kernel to your Jupyter based on a conda environment. When in Jupyter, you will then be able to select the name from the kernel list, and it will be using the packages you installed. Follow these steps to do this (replacing <code>&lt;env&gt;</code> with the name you want to give your conda environment):</p> <pre><code>conda activate &lt;env&gt;\npython -m ipykernel install --user --name &lt;env&gt;\n</code></pre>"},{"location":"runjobs/ondemand/jupyter/#removing-a-conda-environment-from-jupyter","title":"Removing a conda environment from Jupyter","text":"<p>If you remove conda environments using <code>conda remove --name &lt;env&gt; --all</code> the Jupyter kernel is left behind leading to a broken kernel. You can remove the broken kernel using:</p> <pre><code>jupyter kernelspec uninstall &lt;unwanted-kernel&gt;\n</code></pre>"},{"location":"runjobs/ondemand/jupyter/#working-with-jupyter-extensions","title":"Working with jupyter extensions","text":"<p>Warning</p> <p>Using additional Jupyter extensions is considered an expert feature from our side. Please note that we can\u2019t provide support for issues with extensions at this time. (Beta testers: If you have suggestions on this workflow we\u2019re happy to hear them!)</p> <p>Since jupyter is installed in a location that is not writable by regular users, the Jupyter Extension Manager is read-only. This means you can\u2019t install, remove, enable or disable Jupyter extensions directly from the Extension Manager within Jupyter.</p> <p>If you want to use custom jupyter extensions please follow the following procedure:</p> <ol> <li>Load the Anaconda3 module that is used for OnDemand Jupyter:    <pre><code>module load Anaconda3/2024.02-1\n</code></pre></li> <li>Ensure the <code>pip</code> version matches the following:    <pre><code>$ which pip\n/software.9/software/Anaconda3/2024.02-1/bin/pip\n</code></pre></li> <li>Install a jupyterlab extension <code>&lt;jupyterlab_extension&gt;</code> ensuring it will be    compatible with the OnDemand jupyter version (4.0.11):    <pre><code>pip install jupyterlab==4.0.11 &lt;jupyter_extension&gt; --user\n</code></pre></li> </ol> <p>Warning</p> <p>It is important that the jupyterlab version specified matches the version on    the server (<code>jupyterlab==4.0.11</code>) exactly. Otherwise you might end up with a    incomtatible versions that could prevent your notebook from starting.</p> <p>If you no longer need an extension please remove it from your Jupyter using:</p> <pre><code>pip uninstall &lt;jupyter_extension&gt;\n</code></pre> <p>After stopping and restarting your server, when you return to the Jupyter page, you should see your extensions added or removed from the Extension Manager.</p>"},{"location":"runjobs/ondemand/jupyter/#known-limitations","title":"Known limitations","text":"<ul> <li>Extensions: It is not possible to enable/disable Jupyter extensions through   the Extension manager. Please see the section   Working  with jupyter extensions</li> </ul>"},{"location":"runjobs/ondemand/rstudio/","title":"RStudio","text":"<p>The RStudio server allows you to use RStudio on UBELIX.</p> <ol> <li>Select the RStudio Server app under Interactive Apps    </li> <li>Provide the job specification you want for the RStudio server    </li> <li>Once RStudio is ready, click <code>Connect to RStudio Server</code> to access RStudio.    </li> </ol> <p>Warning</p> <p>The job may still be running if you close the window or log out. When you are done, shut down RStudio by clicking <code>Delete</code> on the session under My Interactive Sessions. You can confirm that the interactive session has stopped by checking My Interactive Sessions.</p>"},{"location":"runjobs/ondemand/rstudio/#loading-software-modules","title":"Loading software modules","text":"<p>If you need to load additional software modules within RStudio you can open a Terminal window within RStudio to use the familiar <code>module</code> commands. Note that you need to restart R from within RStudio to pickup the newly loaded modules.</p> <p>Alternatively you can add additional module commands in the Environment Setup section of the RStudio submission form in the OnDemand portal, i.e,</p> <pre><code>module load HDF/4.2.15-GCCcore-11.3.0\n</code></pre>"},{"location":"runjobs/ondemand/rstudio/#installing-r-packages","title":"Installing R packages","text":"<p>A vast amount of core R packages are pre-installed centrally - you can see what is available by using the <code>library()</code> function within R. However, given the number of packages and multiple versions available, other packages should be installed by the user.</p> <p>Installing R packages from CRAN is pretty straightforward thanks to install.packages() function. For instance:</p> <pre><code>&gt; install.packages(c(\"ggplot2\", \"dplyr\"))\n</code></pre> <p>By default, when you install R packages, R will try to install them in the central installation. Since this central installation is shared among all users on the cluster, it\u2019s obviously impossible to install directly your packages there. This is why this location is not writable and you will get this kind of message:</p> <pre><code>&gt; install.packages(\"ggplo2\")\nWarning in install.packages(\"ggplo2\") :\n  'lib = \"/software.9/software/R/4.2.1-foss-2022a/lib/R/library/\"' is not writable\nWould you like to use a personal library instead? (yes/No/cancel)\n</code></pre> <p>This is why you have to answer yes to this \u201cWould you like to use a personal library instead?\u201d question.</p>"},{"location":"runjobs/ondemand/rstudio/#known-limitations","title":"Known limitations","text":"<ul> <li>custom R versions, e.g. from Anaconda are not supported.   If you have an idea on how to make this happen, get in touch!</li> <li>you need to properly stop your R session from within RStudio Server before you exit or you will receive Warnings after the next login</li> <li>RStudio Server doesn\u2019t know about any SLURM environment variables. Use <code>system('nproc')</code> is a reliable way to determine how many cores are actually available to you. Please avoid using <code>parallel::detectCores()</code> in your code as it does not return the number of \u201callowed\u201d cores correctly.</li> </ul>"},{"location":"runjobs/ondemand/rstudio/#common-problems","title":"Common problems","text":""},{"location":"runjobs/ondemand/rstudio/#r-is-taking-longer-to-start-than-usual","title":"R is taking longer to start than usual","text":"<p>Two of the most common session startup problems are (1) oversized global R environments, which take a long time to load into memory when the session is resumed, and (2) problematic code in <code>.Rprofile</code> which takes too long to run or throws errors during startup.</p> <p>In order to help eliminate these variables as the cause of session startup issues, RStudio Server can start sessions in Safe Mode. This mode starts the session without running <code>.Rprofile</code> or restoring the global environment. If a session takes too long to start, the user will be given the option to restart a session in Safe Mode.</p> <p>In some cases using safe mode shows the same issue. In this case the issue may be linked to a previous R session and may be fixed by removing the previously active session: <code>rm -r ~/.local/share/rstudio/sessions/active</code>.</p>"},{"location":"runjobs/ondemand/troubleshooting/","title":"Troubleshooting OnDemand","text":""},{"location":"runjobs/ondemand/troubleshooting/#common-problems","title":"Common problems","text":"<p>coming soon</p>"},{"location":"runjobs/ondemand/troubleshooting/#general-information-for-troubleshooting-ondemand","title":"General information for troubleshooting OnDemand","text":"<p>Logs and scripts for each interactive session with Open OnDemand are stored in:</p> <pre><code>~/ondemand/data/sys/dashboard/batch_connect/sys\n</code></pre> <p>There are directories for each interactive app type within this directory. For example, to see the scripts and logs for an RStudio session, you might look at the files under:</p> <pre><code>~/ondemand/data/sys/dashboard/batch_connect/sys/rstudio/output/b5733507-a750-4bb9-8d4b-710618ce0de1\n</code></pre> <p>where <code>b5733507-a750-4bb9-8d4b-710618ce0de1</code> corresponds to a specific session of an OOD app (the RStudio app in this case).</p>"},{"location":"runjobs/ondemand/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you need help getting started on UBELIX OnDemand, please contact your local IT manager or supervisor if applicable. In case they are not able to provide the support you need, you can always contact the UBELIX support .</p>"},{"location":"runjobs/scheduled-jobs/checkpointing/","title":"Checkpointing","text":"<p>Checkpointing a job means that you frequently save the job state so that you can resume computation from the last checkpoint in case of a crash. On this page we provide some useful information for making your own code checkpoint-able.</p>"},{"location":"runjobs/scheduled-jobs/checkpointing/#why-checkpointing-is-important","title":"Why checkpointing is important","text":"<p>Imagine a job is already running for several hours when an event occurs which leads to the abortion of the job. Such events can be:</p> <ul> <li>Exceeding the time limit</li> <li>Exceeding allocated memory</li> <li>Job gets preempted by another job (gpu partition only!)</li> <li>Node failure</li> </ul> <p>Tip</p> <p>Some applications provide built-in checkpoint/restart mechanisms: Gaussian, Quantum Espresso, CP2K and more. Check the documentation of your application to see if checkpoiting/restarting is already supported!</p>"},{"location":"runjobs/scheduled-jobs/checkpointing/#general-recipe-for-checkpointing-your-own-code","title":"General recipe for checkpointing your own code","text":"<p>Introducing checkpointing logic in your code consists of 3 steps</p> <ol> <li>Look for a state file containing a previously saved state.</li> <li>If a state file exists, then restore the state. Else, start from scratch.</li> <li>Periodically save the state.</li> </ol>"},{"location":"runjobs/scheduled-jobs/checkpointing/#using-unix-signals","title":"Using UNIX signals","text":"<p>You can save the state of your job at specific points in time, after certain iterations, or at whatever event you choose to trigger a state saving. You can also trap specific UNIX signals and act as soon as the signal occurs. The following table lists common signals that you might want to trap in your program:</p> Signal Name Signal Number Description Default Disposition SIGTERM 15 SIGTERM initiates the termination of a process Term - Terminate the process SIGCONT 18 SIGCONT continues a stopped process Cont - Continue the process if stopped SIGUSR1 10 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process SIGUSR2 12 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process <p>SLURM sends SIGCONT followed by SIGTERM just before a job is canceled. Trapping the signal (e.g. SIGTERM) gives you 60 seconds for housekeeping tasks, e.g. save current state. At the latest after that your job is canceled with SIGKILL. This is true for jobs canceled by the owner using <code>scancel</code> and jobs canceled by SLURM, e.g. because of exceeding time limit.</p> <p>Note</p> <p>See <code>kill -l</code> for a list of all supported signals. Note that some signals cannot be trapped, e.g SIGKILL</p>"},{"location":"runjobs/scheduled-jobs/checkpointing/#register-a-signal-handler-for-a-unix-signal","title":"Register a signal handler for a UNIX signal","text":"<p>The following examples show how to register a signal handler in different languages, but omit the logic for creating a checkpoint and restart a job from an existing checkpoint. We will provide a working example further down below on this page.</p> <p>Bash</p> <pre><code>#!/bin/bash\n\nfunction signal_handler {\n  # Save program state and exit\n  (...)\n  exit\n}\n\ntrap signal_handler TERM\n(...)\n</code></pre> <p>C/C++</p> <pre><code>#include &lt;signal.h&gt;  // C\n#include &lt;csignal&gt;   // C++\n\nvoid signal_handler(int signal) {\n  // Save program state and exit\n  (...)\n exit(0);\n}\n\n// Register signal handler for SIGTERM\nsignal(SIGTERM, signal_handler);  // signal_handler: function to handle signal\n(...)\n</code></pre> <p>Python</p> <pre><code>#! /usr/bin/env python\nimport signal\nimport sys\n\n\ndef signal_handler(sig, frame):\n  # Save program state and exit\n  (...)\n  sys.exit(0)\n\n\nsignal.signal(signal.SIGTERM, signal_handler)\n(...)\n</code></pre>"},{"location":"runjobs/scheduled-jobs/checkpointing/#signaling-checkpoint-creation-without-canceling-the-job","title":"Signaling checkpoint creation without canceling the job","text":"<p>SLURM distinguishes between the job script, its child processes and job steps. Job steps are launched using <code>srun</code>. </p> <p>All applications which should reveive more signals than the default SIGERM at the end of the job, does need to be started using <code>srun</code>. Then signals (here <code>SIGUSR1</code>) can be send using:</p> <pre><code>scancel --signal=USR1 &lt;jobID&gt;\n</code></pre> <p>If you need/want to handle signals with the batch script, add <code>--batch</code> (signals only to the batch script) or <code>--full</code> (signal to all steps and the batch script), e.g.</p> <pre><code>scancel --full --signal=USR1 &lt;jobid&gt;\n</code></pre> <p>Therewith, you can use a UNIX signal to trigger the creation of a checkpoint of a running job. For example, consider a job that traps SIGUSR1 and saves intermediate results as soon as the signal occurs. You can then create a checkpoint by signaling SIGUSR1 to the job.</p> <p>Note</p> <p>Using <code>scancel</code> with the <code>--signal</code> option won\u2019t terminate the job or job step.</p>"},{"location":"runjobs/scheduled-jobs/container-jobs/","title":"Container jobs","text":"<p>UBELIX provides access to a <code>apptainer</code> runtime for running applications in software containers. Currently, there are two major providers of the <code>apptainer/singularity</code> runtime, namely Singularity CE and Apptainer, with the latter being a fork of the former. For most cases, these should be fully compatible. No modules need to be loaded to use <code>apptainer</code> on UBELIX. You can always check the version of apptainer using the command <code>apptainer --version</code>.</p> <p>See the Apptainer containers install page for details about creating UBELIX compatible software containers.</p>"},{"location":"runjobs/scheduled-jobs/container-jobs/#the-basics-of-running-a-container-on-ubelix","title":"The basics of running a container on UBELIX","text":"<p>Applications in a container may be run by combining Slurm commands with Singularity commands, e.g., to get the version of Ubuntu running in a container stored as \u201cubuntu_22.04.sif\u201d, we may use <code>srun</code> to execute the <code>apptainer</code> container</p> <pre><code>$ srun --partition=&lt;partition&gt; --account=&lt;account&gt; apptainer exec ubuntu_22.04.sif cat /etc/os-release\n</code></pre> <p>which prints something along the lines of</p> <pre><code>PRETTY_NAME=\"Ubuntu 22.04.1 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.1 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n</code></pre>"},{"location":"runjobs/scheduled-jobs/container-jobs/#binding-network-file-systems-in-the-container","title":"Binding network file systems in the container","text":"<p>By default, the network file system partitions, such as <code>/scratch</code> or <code>/storage</code> are not accessible from the within the container. To make them available, they need to be explicitly bound by passing the <code>-B/--bind</code> command line option to <code>apptainer exec/run</code>. For instance</p> <pre><code>$ srun --partition=&lt;partition&gt; --account=&lt;project_name&gt; apptainer exec -B /scratch/&lt;path_to_project&gt; ubuntu_21.04.sif ls /scratch/&lt;path_to_project&gt;\n</code></pre> <p>Warning</p> <p>Since project folder paths like <code>/scratch/</code> are symlinks on UBELIX, you must bind these full paths to make them available in the container. Simply binding <code>/scratch</code> will not work.</p>"},{"location":"runjobs/scheduled-jobs/container-jobs/#running-containerized-mpi-applications","title":"Running containerized MPI applications","text":"<p>Running MPI applications in a container requires that you either bind the host MPI (the MPI stack provided as part of UBELIX or install a UBELIX compatible MPI stack in the container.</p>"},{"location":"runjobs/scheduled-jobs/container-jobs/#using-the-host-mpi","title":"Using the host MPI","text":"<p>To properly make use of UBELIX high-speed network, it is necessary to mount a few host system directories inside the container and set <code>LD_LIBRARY_PATH</code> so that the necessary dynamic libraries are available at run time. This way, the UBELIX MPI stack replaces the MPI installed in the container image.</p> <p>Details of this approach will be made available in the future.</p>"},{"location":"runjobs/scheduled-jobs/container-jobs/#using-the-container-mpi","title":"Using the container MPI","text":"<p>MPI applications can also be run using an MPI stack installed in the container. To do so, Slurm needs to be instructed to use the PMI-2 process management interface by passing <code>--mpi=pmi2</code> to <code>srun</code>, e.g.</p> <pre><code>$ srun --partition=&lt;partition&gt; --mpi=pmi2 --nodes=2 apptainer run mpi_osu.sif\n</code></pre> <p>which produces an output along the lines of</p> <pre><code># OSU MPI Bandwidth Test v5.3.2\n# Size      Bandwidth (MB/s)\n1                       0.50\n2                       1.61\n4                       3.57\n8                       6.54\n16                      9.65\n32                     18.04\n64                     35.27\n128                    67.76\n256                    91.12\n512                   221.09\n1024                  278.88\n2048                  471.54\n4096                  917.02\n8192                 1160.74\n16384                1223.41\n32768                1397.97\n65536                1452.23\n131072               2373.07\n262144               2104.56\n524288               2316.71\n1048576              2478.30\n2097152              2481.68\n4194304              2380.51\n</code></pre> <p>Note that this approach gives lower bandwidths, especially for the larger message sizes, than is the case when using the UBELIX MPI. In general, the performance obtained from using the container MPI might be low compared to the results obtained when using the host\u2019s MPI. For a more in-depth discussion about MPI in containers, we suggest that you read this introduction to MPI in containers.</p>"},{"location":"runjobs/scheduled-jobs/deleting-jobs/","title":"Deleting Jobs","text":"<p>Sometimes you\u2019ll need to delete pending or even running jobs. Use the <code>scancel</code> command to delete active jobs.</p>"},{"location":"runjobs/scheduled-jobs/deleting-jobs/#scancel","title":"scancel","text":"<p>Syntax</p> <pre><code>scancel [options] &lt;jobid&gt; ...\n</code></pre> <p><code>scancel</code> can be restricted to a subset of jobs, using the following options with the related value, e.g:</p> <pre><code>--me                jobs of current user\n-A, --account       jobs under this charge account\n-n, --jobname       jobs with this job name\n-p, --partition     jobs in this partition\n-t, --state         jobs in this state\n</code></pre>"},{"location":"runjobs/scheduled-jobs/deleting-jobs/#examples","title":"Examples","text":"<p>Delete specific job:</p> <pre><code>scancel 12345678\n</code></pre> <p>Delete all running jobs:</p> <pre><code>scancel --state=R\n</code></pre> <p>Delete all of your jobs:</p> <pre><code>scancel --me\n</code></pre>"},{"location":"runjobs/scheduled-jobs/dependencies/","title":"Job Dependencies","text":"<p>This pages describes the SLURM depencency feature. This feature is used when you need to chain jobs, due to dependencies. For example: </p> <ul> <li>a preprocessing job with 1 core should be followed by a simulation with 40 cores. Results should be post processed with a single core job. </li> <li>a post processing job should be submitted after all tasks of a job array are finished</li> </ul>"},{"location":"runjobs/scheduled-jobs/dependencies/#usage","title":"Usage","text":"<p>The follow-up job need to specify the dependency using the <code>sbatch</code> option <code>--dependency=&lt;type&gt;:&lt;listOfJobIDs&gt;</code>.  The type can be <code>after</code>, <code>afterok</code>, <code>afterany</code>, <code>afternotok</code>, <code>aftercorr</code>, <code>expand</code>, <code>singleton</code>. (see <code>man sbatch</code> for more info). </p> <p>The underlying job (which this job depends on) need to be submitted first. The related job ID can be caught, by collecting the sbatch output with the <code>--parsable</code> option, e.g.</p> <pre><code>jid_w01=$(sbatch --parsable job01.sh)\n</code></pre>"},{"location":"runjobs/scheduled-jobs/dependencies/#example","title":"Example","text":"<p>A pipeline should be build with: - preparation: <code>job_prep.sh</code> - 2 worker jobs (<code>job01.sh</code> and <code>job02.sh</code>)  - if successfull: a collector job (<code>job_coll.sh</code>) - otherwise: a handling the error job (<code>job_handle_err.sh</code>) - The following script would submit all 3 job with respect to their dependencies. </p> <pre><code>jid_pre=$(sbatch --parsable job_prep.sh)\njid_w01=$(sbatch --parsable --dependency=afterok:${jid_pre} job01.sh)\njid_w02=$(sbatch --parsable --dependency=afterok:${jid_pre} job02.sh)\nsbatch --dependency=afterok:${jid_w01}:${jid_w02} job_coll.sh\nsbatch --dependency=afternotok:${jid_w01}:${jid_w02} job_handle_err.sh\n</code></pre>"},{"location":"runjobs/scheduled-jobs/dependencies/#dependency-on-array-job","title":"Dependency on array job","text":"<p>When specifying a dependency to an array job only one job ID need to be specified, no matter how many array tasks are included. </p> <p>Thus, a 100 task array job and a postprocessing job can be launched using:</p> <p><pre><code>jid=$(sbatch --parsable --array=1-100 job_arr.sh)\nsbatch --dependency=afterok:${jid} job_post.sh\n</code></pre> Where the postprocessing job only runs if all 100 array task ended without error. </p>"},{"location":"runjobs/scheduled-jobs/fair-share/","title":"Fair Share","text":""},{"location":"runjobs/scheduled-jobs/fair-share/#description","title":"Description","text":"<p>The provided resources of UBELIX are meant to be provided in a fair fashion between all cluster users. Every participating user is entitled for the same amount of resources. Fair-share is the largest factor in determining priority, but not the only one.</p> <p>The fair-share system is designed to encourage users to balance their use of resources over time and de-prioritize users/accounts with above average usage.</p>"},{"location":"runjobs/scheduled-jobs/fair-share/#fair-share-score","title":"Fair Share Score","text":"<p>Compared to a baseline, users and projects can use their fair share of resources slowly or fast for a given period. This rate is represented through the Fair Share score. The Fair Share score is a number between 0 and 1.</p> <p>Generally, users and projects with a larger Fair Share score will receive a higher priority in the queue.</p> <ul> <li>Using your share faster than your expected rate of usage will usually cause your Fair Share score to decrease. The more extreme the overuse, the more severe the likely drop.</li> <li>Using your share slower than your expected rate of usage will usually cause your Fair Share score to increase. The more extreme the underuse, the greater the Fair Share bonus.</li> <li>Using the cluster unevenly will cause your Fair Share score to decrease.</li> </ul>"},{"location":"runjobs/scheduled-jobs/gpus/","title":"GPUs","text":"<p>This page contains all information you need to successfully submit GPU-jobs on UBELIX. When submitting to the GPU partition the GPU type specification is required.</p> <p>Applications do only run on GPUs if they are built specifically to run on GPUs that means with GPU support, e.g. CUDA. Please ensure that your application supports GPUs before submitting to the GPU partitions.</p>"},{"location":"runjobs/scheduled-jobs/gpus/#gpu-types","title":"GPU Types","text":"<p>UBELIX currently features various types of GPUs. You have to choose an architecture and use one of the following <code>--gres</code> option to select it.</p> Type SLURM gres option Nvidia Geforce RTX 3090 <code>--gres=gpu:rtx3090:&lt;number_of_gpus&gt;</code> Nvidia Geforce RTX 4090 <code>--gres=gpu:rtx4090:&lt;number_of_gpus&gt;</code> Nvidia A100 <code>--gres=gpu:a100:&lt;number_of_gpus&gt;</code> Nvidia H100 <code>--gres=gpu:h100:&lt;number_of_gpus&gt;</code> Nvidia H200 <code>--gres=gpu:h200:&lt;number_of_gpus&gt;</code> <p>Alternatively, you may use the <code>--gpus</code>, <code>--gpus-per-node</code> and <code>--gpus-per-tasks</code> otions. Note that the GPU type still needs to be specified as shown above.</p> <p>For details on the memory available on the different types of GPU, please see our GPU Hardware page.</p>"},{"location":"runjobs/scheduled-jobs/gpus/#job-submission","title":"Job Submission","text":"<p>GPU jobs must be submitted to the <code>gpu</code> or <code>gpu-invest</code> partitions.</p> <pre><code>#SBATCH --partition=gpu #or gpu-invest\n#SBATCH --gres=gpu:&lt;type&gt;:&lt;number_of_gpus&gt;\n</code></pre>"},{"location":"runjobs/scheduled-jobs/gpus/#requesting-cpu-and-memory-resources-with-gpus","title":"Requesting CPU and memory resources with GPUs","text":"<p>To ensure fair GPU allocations a restriction on the CPU and memory resources that can be requested per GPU is implemented.</p> <p>In the past, we observed that GPU resources were often left unused because some jobs requested disproportionately large amounts of CPU or memory per GPU. To address this issue, we have implemented a restriction on the CPU and memory resources that can be requested per GPU:</p> Type CPUs per GPU Memory per GPU Nvidia RTX 3090 4 60GB Nvidia RTX 4090 16 90GB Nvidia A100 20 80GB Nvidia H100 16 90GB Nvidia H200 16 90GB <p>If you submit a GPU job that requests more resources than are available per GPU, your job will be rejected. If your job requires more CPU and memory resources, you may choose to allocate additional GPUs even if these additional GPUs remain unused by your application.</p>"},{"location":"runjobs/scheduled-jobs/gpus/#qos-job_gpu_preemptable","title":"QoS <code>job_gpu_preemptable</code>","text":"<p>For investors we provide the <code>gpu-invest</code> investor partitions with a specific QoS per investor that guarantees instant access to the purchased resources. Nevertheless, to efficiently use all resources, the QoS <code>job_gpu_preemptable</code> exists in the <code>gpu</code> partition. Jobs, submitted with this QoS have access to all GPU resources, but  may be interrupted if resources are required for investor jobs. Short jobs, and jobs that make use of checkpointing will benefit from these additional resources.</p> <p>Example: Requesting any four RTX3090 from the resource pool in the <code>gpu-invest</code> partition: <pre><code>#SBATCH --partition=gpu-invest\n#SBATCH --qos=job_gpu_preemptable\n#SBATCH --gres=gpu:rtx3090:4\n\n## By default, jobs that are preempted are resubmitted automatically.\n## If this is undesirable for you, use the following option to enable that the job, if preempted,\n## won't be re-queued but canceled instead:\n#SBATCH --no-requeue\n</code></pre></p>"},{"location":"runjobs/scheduled-jobs/gpus/#cuda","title":"CUDA","text":"<p>We provide compiler and library to build CUDA-based application. These are accessible using environment modules. Use <code>module spider CUDA</code> to see which versions are available:</p> <pre><code>module spider CUDA\n------------------------------------------------------------------------------------------------------------------------------------\n  CUDA:\n------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA\n      and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual\n      instruction set and memory of the parallel computational elements in CUDA GPUs.\n\n     Versions:\n        CUDA/11.8.0\n        CUDA/12.1.1\n        CUDA/12.2.0\n</code></pre> <p>Run <code>module load &lt;module&gt;</code> to load a specific version of CUDA:</p> <pre><code>module load CUDA/12.2.0\n</code></pre> <p>cuDNN</p> <p>If you need cuDNN you must only load the cuDNN module. The appropriate CUDA version is then loaded automatically as a dependency.</p>"},{"location":"runjobs/scheduled-jobs/gpus/#gpu-usage-monitoring","title":"GPU Usage Monitoring","text":"<p>To verify the usage of one or multiple GPUs the <code>nvidia-smi</code> tool can be utilized. The tool needs to be launched on the related node. After the job started running, a new job step can be created using <code>srun</code> and call <code>nvidia-smi</code> to display the resource utilization. Here we attach the process to an job with the jobID <code>123456</code>. You need to replace the jobId with your gathered jobID previously presented in the sbatch output.</p> <pre><code>$ sbatch job.sh\nSubmitted batch job 123456\n$ squeue --me\n# verify that job gets started\n$ srun --overlap --jobid 123456 nvidia-smi\nFri Nov 11 11:11:11 2021\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  On   | 00000000:04:00.0 Off |                  N/A |\n| 23%   25C    P8     8W / 250W |      1MiB / 11178MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA GeForce ...  On   | 00000000:08:00.0 Off |                  N/A |\n| 23%   24C    P8     8W / 250W |      1MiB / 11178MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n</code></pre> <p>Therewith the GPU core utilization and memory usage can be displayed for all GPU cards belonging to that job.</p> <p>Note that this is a one-off presentation of the usage and the called <code>nvidia-smi</code> command runs within your allocation. The required resources for this job step should be minimal and should not noticeably influence your job performance.</p>"},{"location":"runjobs/scheduled-jobs/gpus/#further-information","title":"Further Information","text":"<p>CUDA: https://developer.nvidia.com/cuda-zone CUDA C/C++ Basics: http://www.nvidia.com/docs/IO/116711/sc11-cuda-c-basics.pdf </p>"},{"location":"runjobs/scheduled-jobs/interactive/","title":"Interactive Slurm jobs","text":"<p>Interactive jobs allow a user to interact with applications on the compute nodes. With an interactive job, you request time and resources to work on a compute node directly, which is different to a batch job where you submit your job to a queue for later execution.</p> <p>You can use two commands to create an interactive session: <code>srun</code> and <code>salloc</code>. Both of these commands take options similar to <code>sbatch</code>.</p>"},{"location":"runjobs/scheduled-jobs/interactive/#using-salloc","title":"Using <code>salloc</code>","text":"<p>Using <code>salloc</code>, you allocate resources and spawn a shell used to execute parallel tasks launched with <code>srun</code>. For example, you can allocate 1 nodes with 20 CPU cores for 30 minutes with the command</p> <pre><code>$ salloc --nodes=1 --cpus-per-task=20 --partition=&lt;partition&gt; --time=00:30:00\nsalloc: Granted job allocation 123456\nsalloc: Waiting for resource configuration\n</code></pre> <p>Once the allocation is made, this command will start a shell on the login node. You can start parallel execution on the allocated nodes with <code>srun</code>.</p> <pre><code>$ srun ./my_application\n</code></pre> <p>After the execution of your application ended, the allocation can be terminated by exiting the shell (<code>exit</code>).</p> <p>When using <code>salloc</code>, a shell is spawned on the login node. If you want to obtain a shell on the first allocated compute node you can use <code>srun --pty</code>.</p> <pre><code>$ srun --pty bash -i\n</code></pre> <p>If you want to use an application with a GUI, you can use the <code>--x11</code> flag with <code>srun</code> to enable X11 forwarding.</p>"},{"location":"runjobs/scheduled-jobs/interactive/#using-srun","title":"Using <code>srun</code>","text":"<p>For simple interactive session, you can use <code>srun</code> with no prior allocation. In this scenario, <code>srun</code> will first create a resource allocation in which to run the job. For example, to allocate 1 node and 20 CPU cores for 30 minutes and spawn a shell</p> <pre><code>$ srun --partition=&lt;partition&gt; --time=00:30:00 --nodes=1 --cpus-per-task=20 --pty bash\n</code></pre>"},{"location":"runjobs/scheduled-jobs/interactive/#using-srun-to-check-running-jobs","title":"Using <code>srun</code> to check running jobs","text":"<p>Currently, <code>ssh</code>\u2018ing to compute nodes is not allowed, but the <code>srun</code> command can be used to check in on a running job in the cluster. In this case, you need to give the job ID and possibly also the specific name of a compute node to <code>srun</code>.</p> <p>This starts a shell where you can run any command on the first allocated node in a specific job:</p> <pre><code>$ srun --overlap --pty --jobid=&lt;jobid&gt; $SHELL\n</code></pre> <p>By default, you will be connected to the master node of your job which is the  first node in your allocation and the one on which your batch script is executed. </p> <p>If your job spans multiple nodes and you need to connect to a specific compute node, you can achieve this by adding the <code>-w &lt;node_name&gt;</code> option.</p> <pre><code>$ srun --overlap --pty --jobid=&lt;jobid&gt; -w bnodeXXX $SHELL\n</code></pre> <p>where <code>bnodeXXX</code> is the hostname of the node you wish to access. To obtain the list of allocated nodes for a specific job with ID <code>&lt;jobid&gt;</code>, you can utilize  the following command:</p> <pre><code>$ sacct --noheader -X -P -oNodeList --jobs=&lt;jobid&gt;\n</code></pre>"},{"location":"runjobs/scheduled-jobs/investigating-job-failure/","title":"Investigating a Job Failure","text":"<p>Not all jobs execute successfully. There are a list of reasons jobs or applications stop or crash.  The most common causes are:</p> <ul> <li>exceeding resource limits and </li> <li>software-specific errors </li> </ul> <p>Here, discussed are ways to gather information, aspects of avoiding misleading information and aspects of common issues. </p> <p>Tip</p> <p>It is important to collect error/output messages either by writing such information to the default location or by specifying specific locations using the <code>--error</code>/<code>--output</code> option. Do not redirect the error/output stream to /dev/null unless you know what you are doing. Error and output messages are the starting point for investigating a job failure.</p>"},{"location":"runjobs/scheduled-jobs/investigating-job-failure/#exceeding-resource-limits","title":"Exceeding Resource Limits","text":"<p>Each partition defines maximum and default limits for runtime and memory usage.  Within the job specification the current limits can be defined within the ranges.  For better scheduling, the job requirements should be estimated and the limits should be adapted to the needs. The lower the limits the better SLURM can find a spot.  Furthermore, the less resource overhead is specified the less resources are wasted, e.g. for memory. </p> <p>If a job exceeds the runtime or memory limit, it will get killed by SLURM.  In both cases, the error file provides appropriate information:</p>"},{"location":"runjobs/scheduled-jobs/investigating-job-failure/#time-limit","title":"Time limit","text":"<pre><code>(...)\nslurmstepd: error: *** JOB 41239 ON fnode01 CANCELLED AT 2016-11-30T11:22:57 DUE TO TIME LIMIT ***\n(...)\n</code></pre>"},{"location":"runjobs/scheduled-jobs/investigating-job-failure/#memory-limit","title":"Memory limit","text":"<pre><code>(...)\nslurmstepd: error: Job 41176 exceeded memory limit (3940736 &gt; 2068480), being killed\nslurmstepd: error: Exceeded job memory limit\nslurmstepd: error: *** JOB 41176 ON fnode01 CANCELLED AT 2016-11-30T10:21:37 ***\n(...)\n</code></pre>"},{"location":"runjobs/scheduled-jobs/investigating-job-failure/#software-errors","title":"Software Errors","text":"<p>The exit code of a job is captured by SLURM and saved as part of the job record. For <code>sbatch</code> jobs the exit code of the batch script is captured. For <code>srun</code>, the exit code will be the return value of the executed command. Any non-zero exit code is considered a job failure, and results in job state of <code>FAILED</code>. When a signal was responsible for a job/step termination, the signal number will also be captured, and displayed after the exit code (separated by a colon).</p> <p>Tip</p> <p>Always check application-specifc output files for error messages.</p>"},{"location":"runjobs/scheduled-jobs/investigating-job-failure/#job-state-completed-but-should-be-failed","title":"Job state COMPLETED but should be FAILED","text":"<p>Depending on the execution order of the commands in the batch script, it is possible that a specific command fails but the batch script will return zero indicating success. Consider the following simplified R example:</p> <p>fail.R <pre><code>var&lt;-sq(1,1000000000)\n</code></pre></p> <p>job.sbatch <pre><code>#!/bin/bash\n# Slurm options\n#SBATCH --mail-user=mustermann@unibe.ch\n#SBATCH --mail-type=begin,end,fail\n#SBATCH --job-name=\"Simple Example\"\n#SBATCH --time=00:05:00\n#SBATCH --mem-per-cpu=2G\n\n# Put your code below this line\nR CMD BATCH --vanilla fail.R\necho \"Script finished\"\n</code></pre></p> <p>The exit code and state wrongly indicates that the job finished successfully:</p> <pre><code>$ sbatch job_slurm.sh\nSubmitted batch job 41585\n\n$ sacct -j 41585\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n41585        Simple E +        all         id          1  COMPLETED      0:0\n41585.batch       batch                    id          1  COMPLETED      0:0\n</code></pre> <p>Only the R-specific output file shows the error:</p> <p>fail.Rout <pre><code>(...)\n&gt; var&lt;-sq(1,1000000000)\nError: could not find function \"sq\"\nExecution halted\n</code></pre></p> <p>You can bypass this problem by exiting with a proper exit code as soon as the command failed:</p> <p>jobsbatch</p> <pre><code>#!/bin/bash\n# Slurm options\n#SBATCH --mail-user=nico.faerber@id.unibe.ch\n#SBATCH --mail-type=begin,end,fail\n#SBATCH --job-name=\"Simple Example\"\n#SBATCH --time=00:05:00\n#SBATCH --mem-per-cpu=2G\n\n# Put your code below this line\nR CMD BATCH --vanilla fail.R || exit 91\necho \"Script finished\"\n</code></pre> <p>Now, the exit code and state matches the true outcome:</p> <pre><code>$ sbatch job_slurm.sh\nSubmitted batch job 41925\n\n$ sacct -j 41925\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n41925        Simple E +        all         id          1     FAILED     91:0\n41925.batch       batch                    id          1     FAILED     91:0\n</code></pre>"},{"location":"runjobs/scheduled-jobs/monitoring-jobs/","title":"Monitoring Jobs","text":"<p>This page provides information about monitoring user jobs. Slurm provides different commands to gather information about current, future and past jobs. </p> <ul> <li><code>squeue</code> - provides information on current and future jobs</li> <li><code>scontrol</code> - provides detailed information on currently running jobs</li> <li><code>sacct</code> - provides detailed information on past jobs</li> </ul> <p>Note that the output format of most Slurm commands is highly configurable to your needs. Look for the \u2013format or \u2013Format options in the <code>man</code> pages of the command.</p>"},{"location":"runjobs/scheduled-jobs/monitoring-jobs/#squeue","title":"squeue","text":"<p>Monitoring your jobs</p> <p>Use the <code>squeue --me</code> command to get a overview of all your active (running and pending) jobs in the cluster.</p> <p>Syntax</p> <pre><code> squeue [options]\n</code></pre> <p>Common options</p> <pre><code>--me                                Display all your currently pending or running jobs\n--jobs=&lt;job_id[,job_id[,...]]&gt;      Request specific jobs to be displayed\n--partition=&lt;part[,part[,...]]&gt;     Request jobs to be displayed from a comma separated list of partitions\n--states=&lt;state[,state[,...]]&gt;      Display jobs in specific states. Comma separated list or \"all\". Default: \"PD,R,CG\"\n</code></pre> <p>The default output format is as follows:</p> <pre><code>JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n</code></pre> <p>where</p> <pre><code>JOBID              Job or step ID. For array jobs, the job ID format will be of the form &lt;job_id&gt;_&lt;index&gt;\nPARTITION          Partition of the job/step\nNAME               Name of the job/step\nUSER               Owner of the job/step\nST                 State of the job/step. See below for a description of the most common states\nTIME               Time used by the job/step. Format is days-hours:minutes:seconds\n                   (days,hours only printed as needed)\nNODES              Number of nodes allocated to the job or the minimum amount of nodes required\n                   by a pending job\nNODELIST(REASON)   For pending jobs: Reason why pending. For failed jobs: Reason why failed.\n                   For all other job states: List of allocated nodes. See below for a list of the most\n                   common reason codes\n</code></pre> <p>See the man page for more information: <code>man squeue</code></p>"},{"location":"runjobs/scheduled-jobs/monitoring-jobs/#job-states","title":"Job States","text":"<p>During its lifetime, a job passes through several states:</p> <pre><code>PD                 Pending. Job is waiting for resource allocation\nR                  Running. Job has an allocation and is running\nS                  Suspended. Execution has been suspended and resources have been released for other jobs\nCA                 Cancelled. Job was explicitly cancelled by the user or the system administrator\nCG                 Completing. Job is in the process of completing. Some processes on some nodes may still be active\nCD                 Completed. Job has terminated all processes on all nodes with an exit code of zero\nF                  Failed. Job has terminated with non-zero exit code or other failure condition\n</code></pre>"},{"location":"runjobs/scheduled-jobs/monitoring-jobs/#why-is-my-job-still-pending","title":"Why is my job still pending?","text":"<p>Finding a REASON</p> <p>The REASON column of the <code>squeue --me</code> output gives you a hint why your job is not running.</p> <p>(Resources)</p> <p>The job is waiting for resources to become available so that the jobs resource request can be fulfilled.</p> <p>(Priority)</p> <p>The job is not allowed to run because at least one higher prioritized job is waiting for resources.</p> <p>(Dependency)</p> <p>The job is waiting for another job to finish first (\u2013dependency=\u2026 option).</p> <p>(DependencyNeverSatisfied)</p> <p>The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs.</p> <p>(QOSMaxCpuPerUserLimit)</p> <p>The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish.</p> <p>(AssocGrpCpuLimit)</p> <p>dito.</p> <p>(AssocGrpJobsLimit)</p> <p>The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish.</p> <p>(ReqNodeNotAvail, UnavailableNodes:\u2026)</p> <p>Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using <code>scontrol show reservation</code>.</p>"},{"location":"runjobs/scheduled-jobs/monitoring-jobs/#why-cant-i-submit-further-jobs","title":"Why can\u2019t I submit further jobs?","text":"<pre><code>_sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n</code></pre> <p>\u2026 indicates that you have reached the maximum of allowed jobs to be submitted to a specific partition.</p>"},{"location":"runjobs/scheduled-jobs/monitoring-jobs/#examples","title":"Examples","text":"<p>List all your currently running jobs:</p> <pre><code>squeue --me --states=R\n</code></pre> <p>List all your currently running jobs in the <code>gpu</code> partition:</p> <pre><code>squeue --me --partition=gpu --states=R\n</code></pre>"},{"location":"runjobs/scheduled-jobs/monitoring-jobs/#scontrol","title":"scontrol","text":"<p>Use the <code>scontrol</code> command to show more detailed information about a job</p> <p>Syntax</p> <pre><code> scontrol [options] [command]\n</code></pre>"},{"location":"runjobs/scheduled-jobs/monitoring-jobs/#examples_1","title":"Examples","text":"<p>Show detailed information about job with ID 500:</p> <pre><code>scontrol show jobid 500\n</code></pre> <p>Show even more detailed information about job with ID 500 (including the jobscript):</p> <pre><code>scontrol -dd show jobid 500\n</code></pre>"},{"location":"runjobs/scheduled-jobs/monitoring-jobs/#sacct","title":"sacct","text":"<p>Use the <code>sacct</code> command to query information about past jobs</p> <p>Syntax</p> <pre><code> sacct [options]\n</code></pre> <p>Common options</p> <pre><code>--endtime=end_time            Select jobs in any state before the specified time.\n--starttime=start_time        Select jobs in any state after the specified time.\n--state=state[,state[,...]]   Select jobs based on their state during the time period given.\n                              By default, the start and end time will be the current time\n                              when the --state option is specified, and hence only currently\n                              running jobs will be displayed.\n</code></pre>"},{"location":"runjobs/scheduled-jobs/preemption/","title":"Preemption","text":"<p>Slurm supports job preemption, the act of \u201cstopping\u201d jobs to let a high-priority job run. On UBELIX we use the Slurm preemption feature to allow the usage of idle resources on the investor partitions. This allows us to achieve a higher cluster utilization and increase the throughput. In our standard configuration the preempted job(s) are requeued and started using other resources.</p>"},{"location":"runjobs/scheduled-jobs/preemption/#preemptable-qos","title":"Preemptable QoS","text":"<p>Idle investor resources can be used by jobs with the QOS <code>job_gpu_preemptable</code>. However, these preemptable jobs may be terminated by investor jobs at any time! The qos <code>job_gpu_preemptable</code> is especially suitable to jobs that support automatic checkpointing or restarts.</p> <p>To submit a preemptable job request the <code>gpu-invest</code> partition and the QoS <code>job_gpu_preemptable</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"Preemptable Job example\"\n#SBATCH --time=02:00:00\n\n#SBATCH --partition=gpu-invest\n#SBATCH --qos=job_gpu_preemptable\n\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=1G\n#SBATCH --gres=gpu/rtx4090:1\n\n# Your code below this line\n</code></pre>"},{"location":"runjobs/scheduled-jobs/slurm-quickstart/","title":"Slurm quickstart","text":"<p>A HPC cluster is made up of a number of compute nodes, which consist of one or more processors, memory and in the case of the GPU nodes, GPUs. The resource manager allocates these computing resources to the user. This is achieved through the submission of jobs by the user. A job describes the computing resources required to run application(s) and how to run it. UBELIX uses Slurm as job scheduler and resource manager.</p>"},{"location":"runjobs/scheduled-jobs/slurm-quickstart/#slurm-commands-overview","title":"Slurm commands overview","text":"<p>In the following, you will learn how to submit your job using the Slurm Workload Manager. The following quickstart will introduce you to the basics of interacting with Slurm. If you would like to play around with Slurm in a sandboxed environment before submitting real jobs on UBELIX, we highly recommend that you try the interactive SlurmLearning tutorial.</p> <p>The main commands for using Slurm are summarized in the table below.</p> Command Description <code>sbatch</code> Submit a batch script <code>srun</code> Run a parallel job(step) <code>squeue</code> View information about jobs in the scheduling queue <code>scancel</code> Signal or cancel jobs, job arrays or job steps <code>sinfo</code> View information about nodes and partitions"},{"location":"runjobs/scheduled-jobs/slurm-quickstart/#creating-a-batch-script","title":"Creating a batch script","text":"<p>The most common type of job is a batch job. They are submitted to the scheduler using a batch job script and the <code>sbatch</code> command.</p> <p>A batch job script is a text file containing information about the job to be run: Explicitly, the amount of computing resource and the tasks that must be executed.</p> <p>A batch script is summarized by the following steps:</p> <ul> <li>the interpreter to use for the execution of the script: bash</li> <li>directives that define the job options: resources, run time, \u2026</li> <li>setting up the environment: prepare input, environment variables, \u2026</li> <li>run the application</li> </ul> <p>As an example, let\u2019s look at this batch job script:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"Simple Python example\"\n#SBATCH --time=02:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=1G\n#SBATCH --partition=epyc2\n\n# Your code below this line\nmodule load Anaconda3\neval \"$(conda shell.bash hook)\"\n\npython3 script.py\n</code></pre> <p>In the previous example, the first line <code>#!/bin/bash</code> specifies that the script should be interpreted as a bash script.</p> <p>The lines starting with <code>#SBATCH</code> are directives for the workload manager. These have the general syntax</p> <pre><code>#SBATCH option_name=argument\n</code></pre> <p>Now that we have introduced this syntax, we can go through the directives one by one. The first directive is</p> <pre><code>#SBATCH --job-name=exampleJob\n</code></pre> <p>which sets the name of the job. It can be used to identify a job in the queue and other listings.</p> <p>The remaining lines specify the resources needed for the job. The first one is the maximum time your job can run. If your job exceeds the time limit, it is terminated regardless of whether it has finished or not.</p> <pre><code>#SBATCH --time=02:00:00\n</code></pre> <p>The time format is <code>hh:mm:ss</code> (or <code>d-hh:mm:ss</code> where <code>d</code> is the number of days). Therefore, in our example, the time limit is 2 hours.</p> <p>The next four lines of the script describe the computing resources that the job will need to run</p> <pre><code>#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=1G\n</code></pre> <p>In this instance, we request one task (process) to be run on one node. A task corresponds to a process. 8 CPU cores is requested for the one task as well as 1 GiB of memory should be allocated to each cpu of the job.</p> <p>The next line defines the Slurm partition to which the job will be submitted. Slurm partitions are (possibly overlapping) groups of nodes with similar resources or associated limits. In our example, the job doesn\u2019t use a any special resources and will fit perfectly onto the <code>epyc2</code> partition.</p> <pre><code>#SBATCH --partition=epyc2\n</code></pre> <p>Now that the needed resources for the job have been defined, the next step is to set up the environment. For example, copy input data from your home directory to the scratch file system or export environment variables.</p> <pre><code>module load Anaconda3\neval \"$(conda shell.bash hook)\"\n</code></pre> <p>In our example, we load a Anaconda module so that the <code>python</code> application is available to the batch job. Finally, with everything set up, we can launch our program using the <code>python3</code> command.</p> <pre><code>python3 script.py\n</code></pre> <p>More details may be found on the dedicated batch jobs page.</p>"},{"location":"runjobs/scheduled-jobs/slurm-quickstart/#submit-a-batch-job","title":"Submit a batch job","text":"<p>To submit the job script we just created, we use the <code>sbatch</code> command. The general syntax can be condensed as</p> <pre><code>$ sbatch [options] job_script [job_script_arguments ...]\n</code></pre> <p>The available options are the same as the one you use in the batch script: <code>sbatch --time=01:00:00</code> in the command line and <code>#SBATCH --time=01:00:00</code> in a batch script are equivalent. The command line value takes precedence if the same option is present both on the command line and as a directive in a script.</p> <p>For the moment, let\u2019s limit ourselves to the most common way to use the <code>sbatch</code>: passing the name of the batch script which contains the submission options.</p> <pre><code>$ sbatch myjob.sh\nSubmitted batch job 123456\n</code></pre> <p>The <code>sbatch</code> command returns immediately. If the job is successfully submitted, the command prints out the ID number of the job.</p> <p>More details may be found on the dedicated batch jobs page.</p>"},{"location":"runjobs/scheduled-jobs/slurm-quickstart/#examine-the-queue","title":"Examine the queue","text":"<p>Once you have submitted your batch script, it won\u2019t necessarily run immediately. It may wait in the queue of pending jobs for some time before its required resources become available. To view your jobs in the queue, use the <code>squeue</code> command.</p> <pre><code>$ squeue\n  JOBID PARTITION     NAME     USER    ST       TIME  NODES NODELIST(REASON)\n 123456     epyc2 Simple P ubelix_usr  PD       0:00      1 (Priority)\n</code></pre> <p>The output shows the state of your job in the <code>ST</code> column. In our case, the job is pending (<code>PD</code>). The last column indicates the reason why the job isn\u2019t running: <code>Priority</code>. This indicates that your job is queued behind a higher priority job. One other possible reason can be that your job is waiting for resources to become available. In such a case, the value in the <code>REASON</code> column will be <code>Resources</code>.</p> <p>Let\u2019s look at the information that will be shown if your job is running:</p> <pre><code>$ squeue --me\n  JOBID PARTITION     NAME     USER    ST       TIME  NODES NODELIST(REASON)\n 123456     epyc2 Simple P ubelix_usr  R        0:00      1 bnode001\n</code></pre> <p>The <code>ST</code> column will now display a <code>R</code> value (for <code>RUNNING</code>). The <code>TIME</code> column will represent the time your job has been running. The list of nodes on which your job is executing is given in the last column of the output.</p>"},{"location":"runjobs/scheduled-jobs/slurm-quickstart/#cancelling-a-job","title":"Cancelling a job","text":"<p>Sometimes things just don\u2019t go as planned. If your job doesn\u2019t run as expected, you may need to cancel your job. This can be achieved using the <code>scancel</code> command which takes the job ID of the job to cancel.</p> <pre><code>$ scancel &lt;jobid&gt;\n</code></pre> <p>The job ID can be obtained from the output of the <code>sbatch</code> command when submitting your job or by using <code>squeue</code>. The <code>scancel</code> command applies to either a pending job waiting in the queue or to an already running job. In the first case, the job will simply be removed from the queue while in the latter, the execution will be stopped.</p>"},{"location":"runjobs/scheduled-jobs/submission/","title":"Submitting Jobs","text":"<p>This page covers submitting Slurm batch jobs on UBELIX. If you are not already familiar with Slurm, you should read the Slurm quickstart guide which covers the basics. You can also refer to the Slurm documentation or manual pages, in particular the page about sbatch.</p>"},{"location":"runjobs/scheduled-jobs/submission/#resource-allocation","title":"Resource Allocation","text":"<p>Every job submission starts with a resources allocation (nodes, cores, memory). An allocation is requested for a specific amount of time, and can be created using the <code>salloc</code>, <code>sbatch</code> commands. Whereas <code>salloc</code> and <code>sbatch</code> only create resource allocations, <code>srun</code> launches parallel tasks within such a resource allocation.</p>"},{"location":"runjobs/scheduled-jobs/submission/#performance-considerations","title":"Performance considerations","text":"<p>It is crucial to specify a more or less accurate runtime for your job. Requesting too little will result in job timeout, while requesting too much will have a negative impact on job start time and job throughput: Jobs with a shorter runtime have a greater chance to benefit from being backfilled and may therefore start earlier.</p> <p>It is crucial to request the correct amount of memory for your job. Requesting too little memory will result in job abortion. Requesting too much memory is a waste of resources that could otherwise be allocated to other jobs.</p> <p>It is crucial to request the correct amount of cores and tasks for your job. Requesting the correct amount of cores and tasks for your job is necessary for optimal job performance. To efficiently to this you need to understand the characteristics of your application:</p> <ul> <li>Is it a serial application that can only use one core?    Use <code>--ntasks=1 / --cpus-per-task=1</code>.</li> <li>Is is able to run on a single machine using multiple cores?    Use <code>--ntasks=1 / --cpus-per-task=X</code>.</li> <li>Or does it support execution on multiple machines with MPI?    Use <code>--ntasks=X / --cpus-per-task=1</code>.</li> </ul> <p>Job arrays</p> <p>Submit series of jobs (collection of similar jobs) as array jobs instead of one by one. This is crucial for backfilling performance and hence job throughput. Instead of submitting the same job repeatedly. See Array jobs.</p>"},{"location":"runjobs/scheduled-jobs/submission/#common-slurm-options","title":"Common Slurm options","text":"<p>Here is an overview of some of the most commonly used Slurm options.</p>"},{"location":"runjobs/scheduled-jobs/submission/#basic-job-specification","title":"Basic job specification","text":"Option Description <code>--time</code> Set a limit on the total run time of the job allocation. Format: <code>dd-hh:mm:ss</code> <code>--account</code> Charge resources used by this job to specified project <code>--partition</code> Request a specific partition for the resource allocation <code>--qos</code> Specify \u201cQuality of Service\u201d. This can be used to change job limits, e.g. for long jobs or short jobs with large resources. See Partition/QoS page <code>--job-name</code> Specify a job name. Example: <code>--job-name=\"Simple Matlab\"</code> <code>--output</code> Redirect standard output. All directories specified in the path must exist before the job starts! By default stderr and stdout are merged into a file <code>slurm-%j.out</code>, where <code>%j</code> is the job allocation number. Example: <code>--output=myCal_%j.out</code> <code>--error</code> Redirect standard error. All directories specified in the path must exist before the job starts! By default stderr and stdout are merged into a file <code>slurm-%j.out</code>, where <code>%j</code> is the job allocation number. Example: <code>--output=myCal_%j.err</code> <code>--mail-user</code> Mail address to contact job owner. Must be a valid unibe email address, if used! Example:<code>--mail-user=foo.bar@unibe.ch</code> <code>--mail-type</code> When to notify a job owner: <code>none</code>, <code>all</code>, <code>begin</code>, <code>end</code>, <code>fail</code>, <code>requeue</code>, <code>array_tasks</code>. Eaxample: <code>--mail-type=end,fail</code>"},{"location":"runjobs/scheduled-jobs/submission/#request-cpu-cores","title":"Request CPU cores","text":"Option Description <code>--cpus-per-task</code> Set the number of cores per task"},{"location":"runjobs/scheduled-jobs/submission/#request-memory","title":"Request memory","text":"Option Description <code>--mem</code> Set the memory per node. Note: Try to use <code>--mem-per-cpu</code> or <code>--mem-per-gpu</code> instead. <code>--mem-per-cpu</code> Set the memory per allocated CPU cores. Example: <code>--mem-per-cpu=2G</code> <code>--mem-per-gpu</code> Set the memory per allocated GPU. Example: <code>--mem-per-gpu=2G</code>"},{"location":"runjobs/scheduled-jobs/submission/#request-gpus","title":"Request GPUs","text":"Option Description <code>--gpus</code> Set the total number of GPUs to be allocated for the job <code>--gpus-per-node</code> Set the number of GPUs per node <code>--gpus-per-task</code> Set the number of GPUs per task <p>For details on how to request GPU resources on UBELIX, please see the GPUs page.</p>"},{"location":"runjobs/scheduled-jobs/submission/#specify-tasks-distribution-mpi","title":"Specify tasks distribution (MPI)","text":"Option Description <code>--nodes</code> Number of nodes to be allocated to the job <code>--ntasks</code> Set the maximum number of tasks (MPI ranks) <code>--ntasks-per-node</code> Set the number of tasks per node <code>--ntasks-per-socket</code> Set the number of tasks on each node <code>--ntasks-per-core</code> Set the maximum number of task on each core"},{"location":"runjobs/scheduled-jobs/submission/#sbatch","title":"sbatch","text":"<p>The <code>sbatch</code> command is used to submit a job script for later execution. It is the most common way to submit a job to the cluster due to its reusability. Slurm options are usually embedded in a job script prefixed by <code>#SBATCH</code> directives. Slurm options specified as command line options overwrite corresponding options embedded in the job script</p> <p>Syntax</p> <pre><code>sbatch [options] script [args...]\n</code></pre>"},{"location":"runjobs/scheduled-jobs/submission/#job-script","title":"Job Script","text":"<p>Sneak Peek: A simple Python example</p> <p>Create sbmission script, <code>python_job.sh</code> allocating 8CPUs, 8GB memory for 1hour: <pre><code>#!/bin/bash\n#SBATCH --job-name=\"Simple Python example\"\n#SBATCH --time=01:00:00\n#SBATCH --mem-per-cpu=1G\n#SBATCH --cpus-per-task=8\n\n# Your code below this line\nmodule load Anaconda3\neval \"$(conda shell.bash hook)\"\npython3 script.py\n</code></pre></p> <p>Submit the job script:</p> <pre><code>sbatch python_job.sh\nSubmitted batch job 30215045\n</code></pre> <p>See below for more details and background information.</p> <p>A batch script is summarized by the following steps:</p> <ul> <li>the interpreter to use for the execution of the script: bash</li> <li>directives that define the job options: resources, run time, \u2026</li> <li>setting up the environment: prepare input, environment variables, \u2026</li> <li>run the application</li> </ul> <p>The job script acts as a wrapper for your actual job. The first line is generally <code>#!/bin/bash</code> and specifies that the script should be interpreted as a bash script.</p> <p>The lines starting with <code>#SBATCH</code> are directives for the workload manager. These have the general syntax</p> <pre><code>#SBATCH option_name=argument\n</code></pre> <p>The available options are shown above and are the same as the one you use on the command line: <code>sbatch --time=01:00:00</code> in the command line and <code>#SBATCH --time=01:00:00</code> in a batch script are equivalent. The command line value takes precedence if the same option is present both on the command line and as a directive in a script.</p>"},{"location":"runjobs/scheduled-jobs/submission/#salloc","title":"salloc","text":"<p>The <code>salloc</code> command is used to allocate resources (e.g. nodes), possibly with a set of constraints (e.g. number of processor per node) for later utilization. It is typically used to allocate resources and spawn a shell, in which the <code>srun</code> command is used to launch parallel tasks.</p> <p>Syntax <pre><code>salloc [options] [&lt;command&gt; [args...]]\n</code></pre></p> <p>Example <pre><code>bash$ salloc -N 2 -t 10\nsalloc: Granted job allocation 247\nbash$ module load foss\nbash$ srun ./mpi_hello_world\nHello, World.  I am 1 of 2 running on knlnode03.ubelix.unibe.ch\nHello, World.  I am 0 of 2 running on knlnode02.ubelix.unibe.ch\nbash$ exit\nsalloc: Relinquishing job allocation 247\n</code></pre></p>"},{"location":"runjobs/scheduled-jobs/submission/#srun","title":"srun","text":"<p>The <code>srun</code> command creates job steps. One or multiple <code>srun</code> invocations are usually used from within an existing resource allocation. Thereby, a job step can utilize all resources allocated to the job, or utilize only a subset of the resource allocation. Multiple job steps can run sequentially in the order defined in the batch script or run in parallel, but can together never utilize more resources than provided by the allocation.</p> <p>Warning</p> <p>Do not submit a job script using <code>srun</code> directly. Always create an allocation with <code>salloc</code> or embed it in a script submitted with <code>sbatch</code>.</p> <p>Syntax <pre><code>srun [options] executable [args...]\n</code></pre></p> <p>Use <code>srun</code> in your job script for executables if these are:</p> <ul> <li>MPI applications</li> <li>multiple job tasks (serial or parallel jobs) simultaneously within an allocation</li> </ul> <p>Example Run MPI task:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"Open MPI example\"\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=20\n#SBATCH --mem-per-cpu=2G\n#SBATCH --time=06:00:00\n\n# Your code below this line\nmodule load foss\nsrun ./mpi_app.exe\n</code></pre> <p>Run two jobs simultaneously:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"Simultaneous example\"\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=4\n\n# Your code below this line\n# run 2 threaded applications side-by-side\nsrun --tasks=1 --cpus-per-task=4 ./app1 inp1.dat &amp;\nsrun --tasks=1 --cpus-per-task=4 ./app2 inp2.dat &amp;\nwait\n# wait: Wait for both background commands to finish. This is important when running bash commands in the background (using &amp;)! Otherwise, the job ends immediately.\n</code></pre> <p>Please run series of similar tasks as job array. See Array Jobs.</p>"},{"location":"runjobs/scheduled-jobs/submission/#requesting-a-partition-qos","title":"Requesting a Partition / QoS","text":"<p>Per default jobs are submitted to the <code>epyc2</code> partition and the default QoS <code>job_cpu</code>. The partition option can be used to request different hardware, e.g. <code>gpu</code> partition. And the QoS can be used to run in a specific queue, e.g. <code>job_gpu_debug</code>:</p> <pre><code>#SBATCH --partition=gpu\n#SBATCH --qos=job_gpu_debug\n</code></pre> <p>See Partitions / QoS for a list of available partitions and QoS and its specifications.</p>"},{"location":"runjobs/scheduled-jobs/submission/#job-examples","title":"Job Examples","text":""},{"location":"runjobs/scheduled-jobs/submission/#sequential-job","title":"Sequential Job","text":"<p>Running a serial job with email notification in case of error (1 task is default value):</p> <pre><code>#!/bin/bash\n#SBATCH --mail-user=foo.bar@unibe.ch\n#SBATCH --mail-type=end,fail\n#SBATCH --job-name=\"Serial Job\"\n#SBATCH --time=00:10:00\n\n# Your code below this line\necho \"I'm on host: $HOSTNAME\"\n</code></pre>"},{"location":"runjobs/scheduled-jobs/submission/#parallel-jobs","title":"Parallel Jobs","text":"<p>Shared Memory Jobs (e.g. OpenMP)</p> <p>SMP parallelization is based upon dynamically created threads (fork and join) that share memory on a single node. The key request is <code>--cpus-per-task</code>. To run N threads in parallel, we request N CPUs on the node (<code>--cpus-per-task=N</code>). </p> <pre><code>#!/bin/bash\n#SBATCH --mail-user=foo.bar@unibe.ch\n#SBATCH --mail-type=end,fail\n#SBATCH --job-name=\"SMP Job\"\n#SBATCH --mem-per-cpu=2G\n#SBATCH --cpus-per-task=16\n#SBATCH --time=01:00:00\n\n# Your code below this line\nsrun ./my_binary\n</code></pre> <p>MPI Jobs (e.g. Open MPI)</p> <p>MPI parallelization is based upon processes (local or distributed) that communicate by passing messages. Since they don\u2019t rely on shared memory those processes can be distributed among several compute nodes. Use the option <code>--ntasks</code> to request a certain number of tasks (processes) that can be distributed over multiple nodes:</p> <pre><code>#!/bin/bash\n#SBATCH --mail-user=foo.bar@unibe.ch\n#SBATCH --mail-type=end\n#SBATCH --job-name=\"MPI Job\"\n#SBATCH --mem-per-cpu=2G\n#SBATCH --ntasks=8\n#SBATCH --time=04:00:00\n\n# Your code below this line\n# First set the environment for using Open MPI\nmodule load foss\nsrun ./my_binary\n</code></pre> <p>On the \u2018bdw\u2019 partition you must use all CPUs provided by a node (20 CPUs). For example to run an OMPI job on 80 CPUs, do:</p> <pre><code>#!/bin/bash\n#SBATCH --mail-user=foo.bar@baz.unibe.ch\n#SBATCH --mail-type=end,fail\n#SBATCH --job-name=\"MPI Job\"\n#SBATCH --mem-per-cpu=2G\n#SBATCH --nodes=4     ## or --ntasks=80\n#SBATCH --ntasks-per-node=20\n#SBATCH --time=12:00:00\n\n# Your code below this line\nmodule load foss\nsrun ./my_binary\n</code></pre>"},{"location":"runjobs/scheduled-jobs/submission/#gpu-jobs","title":"GPU Jobs","text":"<p>For information on how to run GPU jobs on UBELIX, please see the GPUs page.</p>"},{"location":"runjobs/scheduled-jobs/submission/#automatic-requeuing","title":"Automatic requeuing","text":"<p>The UBELIX Slurm configuration has automatic requeuing of jobs upon node failure enabled. It means that if a node fails, your job will be automatically resubmitted to the queue and will have the same job ID and possibly truncate the previous output. Here are some important parameters you can use to alter the default behavior.</p> <ul> <li>you can disable automatic requeuing using the <code>--no-requeue</code> option</li> <li>you can avoid your output file being truncated in case of requeuing by using   the <code>--open-mode=append</code> option</li> </ul> <p>If you want to perform specific operations in your batch script when a job has been requeued, you can check the value of the <code>SLURM_RESTART_COUNT</code> variable. The value of this variable will be <code>0</code> if it is the first time the job is run. If the job has been restarted, the value will be the number of times the job has been restarted.</p>"},{"location":"runjobs/scheduled-jobs/submission/#common-error-messages","title":"Common error messages","text":"<p>Below are some common error messages you may get when your job submission fails.</p>"},{"location":"runjobs/scheduled-jobs/submission/#job-violates-accountingqos-policy","title":"Job violates accounting/QOS policy","text":"<p>The complete error message is:</p> <pre><code>sbatch: error: AssocMaxSubmitJobLimit\nsbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n</code></pre> <p>The most common causes are:</p> <ul> <li>your project has already used all of its allocated compute resources.</li> <li>job script is missing the <code>--account</code> parameter.</li> <li>your project has exceeded the limit for the number of simultaneous jobs, either   running or queuing. Note that Slurm counts each job within an array job as a   separate job.</li> </ul>"},{"location":"runjobs/scheduled-jobs/submission/#environment-variables","title":"Environment Variables","text":"<p>Slurm sets various environment variables available in the context of the job script. Some are set based on the requested resources for the job.</p> Environment Variable Set By Option Description <code>SLURM_JOB_NAME</code> <code>--job-name</code> Name of the job <code>SLURM_ARRAY_JOB_ID</code> ID of your job <code>SLURM_ARRAY_TASK_ID</code> <code>--array</code> ID of the current array task <code>SLURM_ARRAY_TASK_MAX</code> <code>--array</code> Job array\u2019s maximum ID (index) number <code>SLURM_ARRAY_TASK_MIN</code> <code>--array</code> Job array\u2019s minimum ID (index) number <code>SLURM_ARRAY_TASK_STEP</code> <code>--array</code> Job array\u2019s index step size <code>SLURM_NTASKS</code> <code>--ntasks</code> Same as <code>-n</code>, <code>--ntasks</code> <code>SLURM_NTASKS_PER_NODE</code> <code>--ntasks-per-node</code> Number of tasks requested per node.  Only set if the <code>--ntasks-per-node</code> option is specified <code>SLURM_CPUS_PER_TASK</code> <code>--cpus-per-task</code> Number of cpus requested per task.  Only set if the <code>--cpus-per-task</code> option is specified <code>TMPDIR</code> References the disk space for the job on the local scratch <p>For the full list, see <code>man sbatch</code></p>"},{"location":"runjobs/scheduled-jobs/throughput/","title":"Array Jobs","text":"<p>Slurm job arrays provide a way to submit a large number of independent jobs. When a job array script is submitted, a specified number of array tasks are created from the batch script. A batch script for such an array job may look like this:</p> <pre><code>#!/bin/bash\n#SBATCH --array=1-16\n#SBATCH --output=array_%A_%a.out\n#SBATCH --error=array_%A_%a.err\n#SBATCH --time=01:00:00\n#SBATCH --ntasks=1\n#SBATCH --mem=4G\n\n# Print the task index.\necho \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID\n\nsrun ./myapp --input input_data_${SLURM_ARRAY_TASK_ID}.inp\n</code></pre> <p>In this example 16 array tasks will be launched (<code>--array=1-16</code>). These tasks will be copies of the batch script: in our example, each array task will be allocated one task and 4 GB of memory. The <code>SLURM_ARRAY_TASK_ID</code> environment variable identifies each array task uniquely. In the example, we use this variable to provide different input files for each of the array tasks since our application <code>myapp</code> supports an <code>--input</code> flag to provide input files.</p> <p>If you want to reuse the same batch script for different array ranges, you can omit the <code>--array</code> directive in the batch script and instead specify the range when you submit your job.</p> <pre><code>$ sbatch --array=1-16 job.script\n</code></pre>"},{"location":"runjobs/scheduled-jobs/throughput/#defining-the-array-range","title":"Defining the array range","text":"<p>There are several ways to define the range of the index values for a job array:</p> <pre><code># Job array with tasks index values from 0 to 15\n#SBATCH --array=0-15\n\n# Job array with tasks index values 1, 2, 9, 22 and 31\n#SBATCH --array=1,2,9,22,31\n\n# Job array with tasks index values 1, 3, 5 and 7\n#SBATCH --array=1-7:2\n\n# Job array with tasks index values 1, 3, 5, 7 and 20\n#SBATCH --array=1-7:2,20\n</code></pre> <p>You can also specify the maximum number of simultaneously running tasks using the <code>%</code> sign, e.g.</p> <pre><code>#SBATCH --array=0-15%4\n</code></pre> <p>In this example, the maximum number of array tasks running simultaneously will be limited to 4.</p>"},{"location":"runjobs/scheduled-jobs/throughput/#output-files","title":"Output files","text":"<p>Per default the output files are named as <code>slurm-&lt;jobid&gt;_&lt;taskid&gt;.out</code>. When renaming the output/error files variables for the job ID (<code>%A</code>) and for the task ID (<code>%a</code>) can be used. For example:</p> <p><pre><code>#SBATCH --output=array_example_%A_%a.out\n#SBATCH --error=array_example_%A_%a.err\n</code></pre> Thus a file <code>array_example_6543212_12.out</code> will be written for the 12th task of job 6543212.</p>"},{"location":"runjobs/scheduled-jobs/throughput/#managing-job-array-tasks","title":"Managing job array tasks","text":"<p>Use the <code>squeue</code> command to examine the state of your job array. The still pending array tasks are shown as one entry while the running ones are shown as individual entries with their job IDs taking the form <code>&lt;jobid&gt;_&lt;arrayindex&gt;</code>.</p> <pre><code>$ squeue --me\n  JOBID   PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n123456_[3-16] small  example user_tes  PD       0:00      1 (Resources)\n123456_1      small  example user_tes   R       0:17      1 anode124\n123456_2      small  example user_tes   R       0:23      1 anode125\n</code></pre> <p>If you wish to cancel some of the array tasks of a job array, you can use the <code>scancel</code> command as with any other job. For example, to cancel array tasks with indexes from 1 to 3 from job array 2021, use the following command</p> <pre><code>$ scancel 2021_[1-3]\n</code></pre> <p>which is equivalent to</p> <pre><code>$ scancel 2021_1 2021_2 2021_3\n</code></pre> <p>On the other hand, if you want to cancel the whole job array, only specifying the job ID suffice.</p> <pre><code>$ scancel 2021\n</code></pre>"},{"location":"runjobs/scheduled-jobs/throughput/#environment-variables","title":"Environment variables","text":"<p>In addition to the <code>SLURM_ARRAY_TASK_ID</code> variable discussed above, Slurm will set additional environment variables that describe the job array. These variables are summarized in the table below.</p> Variable Description <code>SLURM_ARRAY_TASK_ID</code> Job array index value <code>SLURM_ARRAY_TASK_COUNT</code> Number of array tasks in the job array <code>SLURM_ARRAY_TASK_MIN</code> Value of the highest job array index <code>SLURM_ARRAY_TASK_MAX</code> Value of the lowest job array index"},{"location":"runjobs/scheduled-jobs/throughput/#full-array-job-example","title":"Full array job example","text":"<p>In this section, we give a full example of running the same program with 1000 different command line arguments, submitted as an array job.</p> <p>The parameters to use may be stored in a file, 1000 lines long and named <code>args.txt</code>. Each line in this file contains two parameters to be passed to our program as command-line arguments. As an example, the first 4 lines of this file may be</p> <pre><code>$ head -n 4 args.txt\n  0.025 25.8\n  0.125 30.8\n  0.489 14.4\n  0.861 78.7\n</code></pre> <p>In the context of a job array, we can extract the parameters for each of the array tasks with the help of the <code>SLURM_ARRAY_TASK_ID</code> variable. For example, the first parameter can be obtained as</p> <pre><code>param_a=$(cat args.txt | \\\nawk -v var=$SLURM_ARRAY_TASK_ID 'NR==var {print $1}')\n</code></pre> <p>where we use <code>awk</code> to extract the line corresponding to the index of the task in the job array and extract the first field. The same can be done for the second parameter by replacing <code>print $1</code> by <code>print $2</code>.</p> <p>The complete batch script is presented below. We create a job array of 1000 tasks that uses 2 cores and 2 GB of memory per core (4 GB/task).</p> <pre><code>#!/bin/bash\n#SBATCH --time=00:30:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=2G\n#SBATCH --array=1-1000\n\n# Get first argument\nparam_a=$(cat args.txt | \\\nawk -v var=$SLURM_ARRAY_TASK_ID 'NR==var {print $1}')\n\n# Get second argument\nparam_b=$(cat args.txt | \\\nawk -v var=$SLURM_ARRAY_TASK_ID 'NR==var {print $2}')\n\n./myapp -a $param_a -b $param_b \\\n        -o output_dir/output_${SLURM_ARRAY_TASK_ID}.out\n</code></pre>"},{"location":"software/","title":"Overview","text":"<p>On this page, you will find information about pre-installed software on UBELIX as well as guidance on ways to install additional software yourself.</p> <p>UBELIX users are expected to be able to compile and install the software that they need themselves in their own project directories. If they cannot do that, they need to secure help from their local IT support responsible or supervisor if applicable before requesting support from the UBELIX team.</p> <p>Please note that UBELIX Team can only offer limited help with installing scientific software. The UBELIX team is very small, and we cannot maintain and install all software centrally (e.g. <code>module load xyz</code>) for all the projects running on UBELIX. The maximum amount of help we can offer to each user is on the order of a few hours, including answering the support tickets.</p>"},{"location":"software/#pre-installed-software","title":"Pre-installed software","text":"<p>On UBELIX, we provide some pre-installed software in a central software stack. Consult the software environment page for instructions on identifying the pre-installed software on UBELIX through the module system.</p>"},{"location":"software/#installing-additional-software","title":"Installing additional software","text":"<p>Warning</p> <p>You cannot use the package managers apt, dnf or yum for this, since these commands require root privileges to install software system wide. Instead you have to compile and install the software yourself. </p> <p>Tip</p> <p>If you know that some missing software could be of general interest for a wide community on our machines, you can ask us to install the software system wide.</p> <p>If you intend to install Python packages, please consult the Python packages installation guide for an overview of your options.</p> <p>If you intend to install R packages, please consult the R packages installation guide for an overview of your options.</p> <p>If you need to install common scientific software and libraries, please use EasyBuild. EasyBuild is the primary software installation tool on UBELIX. It is used to install most software in the central software stack on UBELIX, but it is also extremely easy to install additional software in your personal or project space and have it integrate fully with the software stacks.</p> <p>The preferred location for software installations is a workspace directory, so that a software installation can be shared with all users in your project. Software can also be installed in your home directory, but it is not recommended and you will not get additional quota for it. Creating permanent software installations in your <code>/scratch</code> directories is not recommended as these will be cleaned automatically.</p> <p>Please note that UBELIX operates under a \u201cbring your own license\u201d model for commercial software. Only a small amount of software (mainly development tools) are covered by the UBELIX or university budget. If your organisation has existing software licenses, these licenses can in many cases be used on UBELIX by checking out the licenses from the organisation\u2019s license server across the network when a job starts on a compute node. Please note that there are currently no facilities for running private license servers inside UBELIX.</p>"},{"location":"software/Lmod_modules/","title":"Software Environment","text":"<p>Software on UBELIX can be accessed through modules. With the help of the <code>module</code> command, you will be able to load and unload the desired compilers, tools and libraries.</p> <p>Software modules allow you to control which software and versions are available in your environment. Modules contain the necessary information to allow you to run a particular application or provide you access to a particular library so that</p> <ul> <li>different versions of a software package can be provided.</li> <li>you can easily switch to different versions without having to explicitly   specify different paths.</li> <li>you don\u2019t have to deal with dependent modules, they are loaded at the same   time as the software.</li> </ul>"},{"location":"software/Lmod_modules/#the-module-command","title":"The <code>module</code> command","text":"<p>Modules are managed by the <code>module</code> command:</p> <pre><code>$ module &lt;sub-command&gt; &lt;module-name&gt;\n</code></pre> <p>where the sub-command indicates the operation you want to perform. The sub-command is followed by the name of the module on which you want to perform the operation.</p> Sub-command Description <code>spider</code> Search for modules and display help <code>keyword</code>, <code>key</code> Search for modules based on keywords <code>avail</code>, <code>av</code> List available modules <code>whatis</code> Display short information about modules <code>help</code> Print the help message of a module <code>list</code> List the currently modules loaded <code>load</code>, <code>add</code> Load a module <code>unload</code> Remove a module from your environment <code>purge</code> Unload all modules from your environment <code>show</code> Show the commands in the module\u2019s definition file"},{"location":"software/Lmod_modules/#finding-modules","title":"Finding modules","text":"<p>Lmod is a hierarchical module system. It distinguishes between installed modules and available modules. Installed modules are all modules that are installed on the system. Available modules are all modules that can be loaded directly at that time without first loading other modules. The available modules are often only a subset of the installed modules. However, Lmod can tell you for each installed module what steps you must take to also make it available so that you can load it. Therefore, the commands for finding modules are so important.</p> <p>Some modules may also provide multiple software packages or extensions. Lmod can also search for these.</p>"},{"location":"software/Lmod_modules/#module-spider","title":"module spider","text":"<p>The basic command to search for software on UBELIX is <code>module spider</code>. It has three levels, producing different outputs:</p> <ol> <li> <p><code>module spider</code> without further arguments will produce a list of all     installed software and show some basic information about those packages.     Some packages may have an <code>(E)</code> behind their name and will appear in blue     (in the default color scheme) which means that they are part of a different     package. The following levels of <code>module spider</code> will then tell you how to     find which module(s) to load.</p> <p>Note that <code>module spider</code> will also search in packages that are hidden from being displayed. These packages can be loaded and used. However, we hide them either because they are not useful to regular users or because we think that they will rarely or never be directly loaded by a user and want to avoid overloading the module display.</p> </li> <li> <p><code>module spider &lt;name of package&gt;</code> will search for the specific package. This     can be the name of a module, but it will also search some other information     that can be included in the modules. The search is case-insensitive, e.g.</p> <pre><code>$ module spider netcdf\n</code></pre> <p>will show something along the lines of</p> <pre><code>------------------------------------------------------------------------------------------------------------------------------------\n     netCDF:\n------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the\n      creation, access, and sharing of array-oriented scientific data.\n\n     Versions:\n        netCDF/4.8.0-gompi-2021a\n        netCDF/4.9.0-gompi-2022a\n        netCDF/4.9.2-gompi-2023a\n        netCDF/4.9.2-iimpi-2023a\n\n------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"netCDF\" package (including how to load the modules) use the module's full name.\n</code></pre> <p>(abbreviated output) so even though the capitalization of the name was wrong, it can tell us that there are multiple versions of the netCDF library. The <code>gompi-xxxx</code> and <code>iimpi-xxxx</code> tell that the difference is the compiler that was used to install the library. This is important as it is risky to combine modules compiled with different compilers.</p> <p>In some cases, if there is no ambiguity, <code>module spider</code> will already produce help about the package.</p> </li> <li> <p><code>module spider &lt;module name&gt;/&lt;version&gt;</code> will show more help information     about the package, including information on which other modules need to be     loaded to be able to load the package, e.g.</p> <pre><code>$ module spider git/2.41.0-GCCcore-12.3.0-nodocs\n</code></pre> <p>will return</p> <pre><code>------------------------------------------------------------------------------------------------------------------------------------\n  git: git/2.41.0-GCCcore-12.3.0-nodocs\n------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      Git is a free and open source distributed version control system designed to handle everything from small to very large\n      projects with speed and efficiency.\n\n\n    This module can be loaded directly: module load git/2.41.0-GCCcore-12.3.0-nodocs\n\n    Help:\n      Description\n      ===========\n      Git is a free and open source distributed version control system designed\n      to handle everything from small to very large projects with speed and efficiency.\n\n\n      More information\n      ================\n       - Homepage: https://git-scm.com\n</code></pre> </li> </ol>"},{"location":"software/Lmod_modules/#module-avail","title":"module avail","text":"<p>The <code>module avail</code> command is used to show only available modules, i.e., modules that can be loaded directly without first loading other modules. It can be used in two ways:</p> <ol> <li> <p>Without a further argument, it will show an often lengthy list of all     available modules. Some modules will be marked with <code>(D)</code> which means that     they are the default module that would be loaded should you load the module     using only its name.</p> </li> <li> <p>With the name of a module (or a part of the name), it will show all modules     that match that (part of) a name:</p> <pre><code>$ module avail netcdf\n</code></pre> <p>will show something along the lines of</p> <pre><code>----------------------------------------------------- /software.9/modulefiles/data -----------------------------------------------------\n   netCDF-C++4/4.3.1-gompi-2023a           netCDF-Fortran/4.6.1-iimpi-2023a (D)    netCDF/4.9.2-gompi-2023a\n   netCDF-C++4/4.3.1-iimpi-2023a    (D)    netCDF/4.8.0-gompi-2021a                netCDF/4.9.2-iimpi-2023a (D)\n   netCDF-Fortran/4.6.1-gompi-2023a        netCDF/4.9.0-gompi-2022a\n\n  Where:\n   D:  Default Module\n</code></pre> </li> </ol>"},{"location":"software/Lmod_modules/#loading-and-unloading-modules","title":"Loading and unloading modules","text":"<p>Loading and unloading modules in Lmod is very similar to other module systems. Also, note that only available modules can be loaded with the commands below. Some installed modules may only become available after first loading other modules as discussed above.</p> <p>To load a module, use the <code>module load</code> command. For example, to load the  FFTW library, use:</p> <pre><code>$ module load FFTW\n</code></pre> <p>This command will load the default version of the module. If the software you loaded has dependencies, they will be loaded in your environment at the same time.</p> <p>To load a specific version of the module, you need to specify it after the name of the module:</p> <pre><code>$ module load FFTW/3.3.10-GCC-12.3.0\n</code></pre> <p>To unload a module from your environment, use the <code>unload</code> sub-command followed by the name of the module you want to remove.</p> <pre><code>$ module unload FFTTW\n</code></pre> <p>In most cases multiple <code>module load</code> or <code>module unload</code> commands can be combined in a single <code>module load</code> or <code>module unload</code> command.</p> <p>You can also remove all loaded modules from your environment by using the <code>purge</code> sub-command.</p> <pre><code>$ module purge\n</code></pre>"},{"location":"software/Lmod_modules/#listing-loaded-modules","title":"Listing loaded modules","text":"<p>You can list currently loaded modules using</p> <pre><code>$ module list\n</code></pre>"},{"location":"software/Lmod_modules/#workspace-modules","title":"Workspace modules","text":"<p>Workspace modules provide support for user-friendly file system access and custom software stacks in HPC Workspaces.</p> <pre><code>module load Workspace\n</code></pre> <p>The workspace module provides the following variables:</p> Variable Function <code>$WORKSPACE</code> full path to the Workspace. Thus, you can access the workspace using: <code>cd $WORKSPACE</code> <code>$SCRATCH</code> full path to the Workspace SCRATCH directory. Thus you can access it using: <code>cd $SCRATCH</code> <p>The Workspace module provides tools to install custom software within your Workspace. See our EasyBuild documentation for details on how to install software modules for all users of a Workspace. Workspace software modules can be accessed by loading the Workspace module and the software product module.</p> <p>The workspace module will also modify the following environment variables: </p> Variable Function <code>$APPTAINER_BINDPATH</code> using singularity, the Workspace directory will be bind into the container without manual specification. The <code>WORKSPACE</code> variable as well as the <code>SCRATCH</code> variable will also be ported into the container. Thus, you can specify locations with <code>$WORKSPACE</code> or <code>$SCRATCH</code> within the container. <code>$PYTHONPATH</code> if <code>Python</code> or <code>Anaconda</code> is loaded beforehand, it is set to: <code>$WORKSPACE/PyPackages/lib/pythonXXX/site-packages</code> where <code>XXX</code> is the Python major and minor version. And also add the <code>bin</code> directory to <code>$PATH</code>. <code>$PYTHONPACKAGEPATH</code> if <code>Python</code> or <code>Anaconda</code> is loaded beforehand, it is set to: <code>$WORKSPACE/PyPackages</code>. This can be used for e.g. <code>pip install --prefix $PYTHONPACKAGEPATH</code> <code>$CONDA_ENVS_PATH</code> used to create conda environments shared within the Workspace <code>$R_LIBS</code> used to install additional R packages in the shared Workspace. The directory need to be created first. See R page <p>umask</p> <p>The Workspace module sets the umask to 002. Thus files and directories get group-writeable, e.g.: <pre><code>-rw-rw-r-- 1 user group 0 Mar 15 15:15 /path/to/file\n</code></pre></p> <p>Special Workspaces</p> <p>The special module  <code>Workspace_Home</code> can be used to setup your <code>$HOME</code> directory as a personal workspace.</p> <p>The special module <code>Workspace_SW_only</code> can be used to provide the access to a software stack of an HPC Workspace <code>B</code> while working in an HPC Workspace <code>A</code>:</p> <pre><code>HPC_WORKSPACE=A module load Workspace\nHPC_WORKSPACE=B module load Workspace_SW_only\n</code></pre> <p>When you want to load packages from your <code>$HOME</code> workspace while working in <code>A</code>, use</p> <pre><code>HPC_WORKSPACE=$HOME module load Workspace_SW_only\nHPC_WORKSPACE=A module load Workspace\n</code></pre> <p>Note that the variable <code>HPC_WORKSPACE</code> is cleared after each loading of a <code>Workspace*</code>module. The currently loaded Workspace names are stored in <code>$HPC_WORKSPACE_LOADED</code> for the Workspace module and <code>$HPC_WORKSPACE_SW_ONLY</code> for the Workspace_SW_only module.</p>"},{"location":"software/Lmod_modules/#toolchains","title":"Toolchains","text":"<p>A toolchain is a set of modules all building on top of each other. Toolchains are provided in different versions and updated typically once a year. The UBELIX team will only support the three most recent toolchain versions available.</p> <p>The two main toolchains foss and intel are subdivided into sub-toolchains that belong to the same family.</p> Toolchain packages foss GCC, OpenMPI, OpenBLAS, FFTW, ScaLAPACK intel Intel compiler, (GCC required), MKL, Intel MPI Family Subtoolchains foss GCC, gompi intel iompi, iomkl <p>Matching toolchain and version</p> <p>When loading multiple modules, they should be based on the same toolchain (or at least the same toolchain family) and the same version.</p> <p>Use <code>module list</code> to show loaded modules and verify matching versions!</p>"},{"location":"software/Lmod_modules/#saving-your-environment","title":"Saving your environment","text":"<p>Sometimes, if you frequently use multiple modules together, it might be useful to save your environment as a module collection.</p> <p>A collection can be created using <code>save</code> sub-command.</p> <pre><code>$ module save &lt;collection-name&gt;\n</code></pre> <p>Your saved collections can be listed using the <code>savelist</code> sub-command.</p> <pre><code>$ module savelist\n</code></pre> <p>Of course, the main interest of a collection is that you can load all the modules it contains in one command. This is done using the <code>restore</code> sub-command.</p> <pre><code>$ module restore &lt;collection-name&gt;\n</code></pre> <p>More options to manage collections of modules can be found by running <code>module help</code> or in the Lmod User Manual.</p>"},{"location":"software/Lmod_modules/#pre-installed-software","title":"Pre-installed software","text":"<p>This a (incomplete) list of the pre-installed software available on UBELIX.</p>"},{"location":"software/Lmod_modules/#toolchains-compilers","title":"Toolchains / Compilers","text":"Name Version(s) foss 2021a, 2021b, 2022a, 2022b, 2023a intel 2021a, 2021b, 2022a, 2022b, 2023a PGI 19.4 NAGfor 6.2.14 NVHPC 23.7 (CUDA/12.1.1)"},{"location":"software/Lmod_modules/#gpu-compiler-tools","title":"GPU Compiler / Tools","text":"Name Version(s) CUDA 11.8.0, 12.1.1, 12.2.0 cuDNN 8.7.0 (CUDA 11.8.0), 8.9.2 (CUDA 12.1.1 / 12.2.0)"},{"location":"software/Lmod_modules/#ecosystems","title":"Ecosystems","text":"Name Version(s) Anaconda3 2023.09-0, 2024.02-1 R 4.2.1 MATLAB R2023b"},{"location":"software/Lmod_modules/#scientific-libraries","title":"Scientific libraries","text":"Name Version(s) GDAL 3.7.1 GEOS 3.12.0 GMP 6.2.1 GSL 2.7 HDF5 1.14.0 netCDF 4.9.2 netCDF-C++4 4.3.1 netCDF-Fortran 4.6.1"},{"location":"software/Lmod_modules/#scientific-software","title":"Scientific software","text":"Name Version(s) Bowtie2 2.4.4 BLAST 2.2.26 canu 2.2 CDO 2.1.1, 2.2.2 Clustal-Omega 1.2.4 CP2K 2023.1 cutadapt 3.4 GROMACS 2023.3 MEGAHIT 1.2.9 Nextflow 23.04.2 NCO 5.1.9 ORCA 5.0.3 RAxML 8.2.12 RSEM 1.3.3 RSeQC 4.0.0 Subread 2.0.3 SMRT-Link 13.1.0 prokka 1.14.5"},{"location":"software/Lmod_modules/#visualization","title":"Visualization","text":"Name Version(s) ncview 2.1.8 ParaView 5.11.2"},{"location":"software/compiling/gnu/","title":"GNU compilers","text":"<p>The GNU Compiler Collection (GCC) includes front ends for the C (<code>gcc</code>), C++ (<code>g++</code>), and Fortran (<code>gfortran</code>) programming languages.</p>"},{"location":"software/compiling/gnu/#choose-a-version","title":"Choose a version","text":"<p>The GNU Compiler Collection is available from the <code>foss</code> module. This module loads the default version of the compiler.</p> <pre><code>$ module load foss\n</code></pre> <p>If you wish to use an older or newer version, you can list the available version with</p> <pre><code>$ module spider foss\n</code></pre> <p>and then switch to the desired version using</p> <pre><code>$ module swap foss foss/&lt;version&gt;\n</code></pre>"},{"location":"software/compiling/gnu/#openmp-support","title":"OpenMP Support","text":"<p>OpenMP is turned off by default. You can turn it on using the <code>-fopenmp</code> flag.</p>"},{"location":"software/compiling/gnu/#optimization-options","title":"Optimization options","text":"<p>:material-help-circle-outline: <code>man gcc</code> - <code>man gfortran</code></p> <p>The default optimization level of the GNU compiler is <code>-O0</code>. It is therefore necessary to add additional optimization flags. A good starting point is</p> <pre><code>-O2 -ftree-vectorize -funroll-loops -ffast-math\n</code></pre> <ul> <li>the <code>-O2</code> option performs nearly all supported optimizations</li> <li>the <code>-ffast-math</code> relax the IEEE specifications for math functions. This option   can produce incorrect results, don\u2019t use this flag if your code is sensitive    to floating-point optimizations.</li> <li>the <code>-funroll-loops</code> option allows the compiler to unroll loops</li> </ul> <p>A more aggressive option might be</p> <pre><code>-O3 -funroll-loops\n</code></pre> <p>or for even more aggressive optimization</p> <pre><code>-Ofast -funroll-loops\n</code></pre> <p>The <code>-Ofast</code> enables all <code>-O3</code> optimizations and disregards strict standards compliance.</p> <ul> <li>GCC documentation about optimization options</li> </ul>"},{"location":"software/compiling/gnu/#legacy-fortran-codes","title":"Legacy Fortran codes","text":"<p>It is common to experience problems when compiling older Fortran codes with GCC 10 and newer versions. Typically, these codes are not fully compliant with the Fortran standard. The most common error message is <code>Error: Type mismatch ...</code> in connection with MPI calls. In those cases, a less strict compiler mode can be activated with the extra flags:</p> <pre><code>-fallow-argument-mismatch\n</code></pre> <p>or</p> <pre><code>-std=legacy\n</code></pre>"},{"location":"software/compiling/gnu/#compiler-feedback","title":"Compiler Feedback","text":"<p>Information about the optimizations and transformations performed by the compiler can be obtained using the <code>-fopt-info</code> option.</p> <ul> <li>GCC documentation about developer options</li> </ul>"},{"location":"software/compiling/gnu/#debugging","title":"Debugging","text":"<p>To ease a debugging process, it is useful to generate an executable containing debugging information. For this purpose, you can use the <code>-g</code> option.</p> <p>Most of the time, the debug information works best at low levels of code optimization, so consider using the <code>-O0</code> level. The <code>-g</code> options can be specified on a per-file basis so that only a small part of your application incurs the debugging penalty.</p> <ul> <li>GCC documentation about debug options</li> </ul>"},{"location":"software/compiling/intel/","title":"Intel compilers","text":"<p>The Intel compoler suite includes front ends for the C (<code>icc</code>), C++ (<code>icx</code>), and Fortran (<code>ifort</code>) programming languages.</p>"},{"location":"software/compiling/intel/#choose-a-version","title":"Choose a version","text":"<p>The Intel compilers are available from the <code>intel</code> module. This module loads the default version of the compiler.</p> <pre><code>$ module load intel\n</code></pre> <p>If you wish to use an older or newer version, you can list the available version with</p> <pre><code>$ module spider intel\n</code></pre> <p>and then switch to the desired version using</p> <pre><code>$ module swap intel intel/&lt;version&gt;\n</code></pre>"},{"location":"software/compiling/intel/#openmp-support","title":"OpenMP Support","text":"<p>OpenMP is turned off by default. You can turn it on using the <code>-qopenmp</code> flag.</p>"},{"location":"software/compiling/intel/#optimization-options","title":"Optimization options","text":"<p>:material-help-circle-outline: <code>man icc</code> - <code>man ifort</code></p> <p>The default optimization level of the Intel compiler is <code>-O2 -fp-model=fast</code>. Therefore the most essential optimization flags are set by default:</p> <pre><code>-O2 -fp-model=fast\n</code></pre> <ul> <li>the <code>-O2</code> option performs nearly all supported optimizations</li> <li> <p>the <code>-fp-model=fast</code> relax the IEEE specifications for math functions. This option   can produce incorrect results, don\u2019t use this flag if your code is sensitive   to floating-point optimizations.</p> </li> <li> <p>Intel documentation about optimization options</p> </li> </ul>"},{"location":"software/compiling/intel/#compiler-feedback","title":"Compiler Feedback","text":"<ul> <li>Intel documentation about developer options</li> </ul>"},{"location":"software/compiling/intel/#debugging","title":"Debugging","text":"<p>To ease a debugging process, it is useful to generate an executable containing debugging information. For this purpose, you can use the <code>-g</code> option.</p> <p>Most of the time, the debug information works best at low levels of code optimization, so consider using the <code>-O0</code> level. The <code>-g</code> options can be specified on a per-file basis so that only a small part of your application incurs the debugging penalty.</p> <ul> <li>Intel documentation about debug options</li> </ul>"},{"location":"software/containers/apptainer/","title":"Singularity/Apptainer","text":"<p>We support Apptainer/Singularity containers as an alternative way to bring your scientific application to UBELIX instead of installing it using EasyBuild.</p> <p>If you are familiar with Docker containers, Apptainer containers are essentially the same thing, but are better suited for multi-user HPC systems such as UBELIX. The main benefit of using a container is that it provides an isolated software environment for each application, which makes it easier to install and manage complex applications.</p> <p>This page provides guidance on preparing your Apptainer containers for use with UBELIX. Please consult the container jobs page for guidance on running your container on UBELIX.</p> <p>Note</p> <p>There are two major providers of the <code>apptainer</code> runtime, namely Singularity CE and Apptainer, with the latter being a fork of the former. For most cases, these should be fully compatible. UBELIX provides a Apptainer runtime.</p>"},{"location":"software/containers/apptainer/#pulling-container-images-from-a-registry","title":"Pulling container images from a registry","text":"<p>Apptainer allows pulling existing container images (Apptainer or Docker) from container registries such as DockerHub. Pulling container images from registries can be done on UBELIX. For instance, the Ubuntu image <code>ubuntu:22.04</code> can be pulled from DockerHub with the following command:</p> <pre><code>$ apptainer pull docker://ubuntu:22.04\n</code></pre> <p>This will create the Apptainer image file <code>ubuntu_22.04.sif</code> in the directory where the command was run. Once the image has been pulled, the container can be run. Instructions for running the container may be found on the container jobs page.</p> <p>Take care when pulling container images</p> <p>Please take care to only use images uploaded from reputable sources as these images can easily be a source of security vulnerabilities or even contain malicious code.</p> <p>Set cache directories when using Docker containers</p> <p>When pulling or building from Docker containers using <code>apptainer</code>, the conversion can be quite heavy. You may need to use the scratch filesystem if the build does not succeed in the in-memory filesystem <code>/tmp</code>, i.e. Singularity cache directory, i.e.</p> <pre><code>$ mkdir -p /scratch/network/users/$USER\n$ export APPTAINER_TMPDIR=/scratch/network/users/$USER\n$ export APPTAINER_CACHEDIR=/scratch/network/users/$USER\n</code></pre>"},{"location":"software/containers/apptainer/#building-apptainersingularity-sif-containers","title":"Building Apptainer/Singularity SIF containers","text":"<p>As an example, consider building a container that is compatible with the MPI stack on UBELIX.</p> <p>Warning</p> <p>For optimally performing MPI-enabled containers, the application inside the container must be dynamically linked to an MPI version that is compatible with the host MPI.</p> <p>The following Singularity definition file <code>mpi_osu.def</code>, installs OpenMPI-4.1.5, which is compatible with the Cray-MPICH found on UBELIX. That OpenMPI will be used to compile the OSU micro-benchmarks. Finally, the OSU point to point bandwidth test is set as the \u201crunscript\u201d of the image.</p> <pre><code>bootstrap: docker\nfrom: ubuntu:22.04\n\n%post\n    # Install software\n    apt-get update\n    apt-get install -y file g++ gcc gfortran make gdb strace wget ca-certificates --no-install-recommends\n\n    # Install OpenMPI\n    wget -q https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.5.tar.gz\n    tar xf openmpi-4.1.5.tar.gz\n    cd openmpi-4.1.5\n    ./configure --prefix=/usr/local\n    make -j\n    make install\n    ldconfig\n\n    # Build osu benchmarks\n    wget -q http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-5.3.2.tar.gz\n    tar xf osu-micro-benchmarks-5.3.2.tar.gz\n    cd osu-micro-benchmarks-5.3.2\n    ./configure --prefix=/usr/local CC=$(which mpicc) CFLAGS=-O3\n    make\n    make install\n    cd ..\n    rm -rf osu-micro-benchmarks-5.3.2\n    rm osu-micro-benchmarks-5.3.2.tar.gz\n\n%runscript\n    /usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw\n</code></pre> <p>The image can be built with</p> <pre><code>$ apptainer build mpi_osu.sif mpi_osu.def\n</code></pre> <p>See the container jobs MPI documentation page for instructions on running this <code>mpi_osu.sif</code> MPI container on UBELIX.</p>"},{"location":"software/installing/easybuild/","title":"EasyBuild","text":"<p>Most pre-installed software in the UBELIX software stack is installed through EasyBuild. The central software stack is kept as compact as possible to ease maintenance and to avoid user confusion. E.g., packages for which users request special customisations will never be installed in the central software stack.</p> <p>This, however, does not mean that you manually compile all the software  you need for your project on UBELIX. We have made it very easy to install additional software in your home or workspace directories (where the latter is a better choice as you can then share it with the other people in your project). After installing, using the software requires not much more than loading a module in exactly the same way as it would be in a central installation.</p>"},{"location":"software/installing/easybuild/#beginners-guide-to-installing-software-on-ubelix","title":"Beginner\u2019s guide to installing software on UBELIX","text":"<p>If you are new to EasyBuild and UBELIX, it might be a good idea to first read  through this chapter once, and then start software installations.</p>"},{"location":"software/installing/easybuild/#easybuild-recipes","title":"EasyBuild recipes","text":"<p>EasyBuild installs software through recipes that give instructions to create a single module that most of the time provides a single package. It will also tell EasyBuild which other modules a package depends on, so that these can also be installed automatically if needed (through their own EasyBuild recipes).</p> <p>An EasyBuild build recipe is a file with a name that consists of different components and ends with \u2018.eb\u2019. Consider, e.g., a build recipe for the software CDO:</p> <pre><code>CDO-2.2.2-gompi-2023a.eb\n</code></pre> <p>The first part of the name, <code>CDO</code>, is the name of the package. The second part of the name, <code>2.2.2</code> is the version of CDO, in this case the 2.2.2 release. </p> <p>The next part, <code>gompi-2023a</code>, denotes the so-called toolchain used for the build. The most common EasyBuild toolchains on UBELIX are:</p> Toolchain packages foss GCC, OpenMPI, OpenBLAS, FFTW, ScaLAPACK intel Intel compiler, (GCC required), MKL, Intel MPI gompi GCC, OpenMPI iimpi Intel compiler, Intel MPI <p>Some recipes also compute with a version suffix. Version suffixes are typically used to distinguish different builds of the same package version, i.e, a CUDA enabled version and a standard version.</p> <p>EasyBuild build recipes are stored in repositories with a fixed directory  structure. On UBELIX we already provide two such repositories, one containing all the software that is installed in the central software stack and one that contains EasyBuild recipes that users can install themselves or use as a basis to make a customised installation of software.</p>"},{"location":"software/installing/easybuild/#preparation-load-workspace-and-easybuild-modules","title":"Preparation: Load Workspace and EasyBuild modules","text":"<p>In most cases a subdirectory in your <code>[Workspace][workspace]</code> directory is the best location to install software as that directory is shared with all users in your project so that everybody can use the software. Therefore, EasyBuild will by default install software in <code>$WORKSPACE/EasyBuild</code>. However, since this environment variable is empty, you need to load the Workspace module to populate the variable:</p> <pre><code>module load Workspace\n</code></pre> <p>Alternatively you can install software in your user home.</p> <pre><code>module load Workspace_Home\n</code></pre> <p>Tip</p> <p>With the <code>Workspace</code>-module loaded you will also see the software that you have installed yourself when you do <code>module avail</code>. Also, <code>module spider</code> will also search those directories.</p>"},{"location":"software/installing/easybuild/#step-1-load-easybuild","title":"Step 1: Load EasyBuild","text":"<p>The next step to install software in the directory you have just indicated, is to load the <code>EasyBuild</code> module:</p> <pre><code>$ module load EasyBuild\n</code></pre> <p>If you want more information about the full configuration of EasyBuild including the paths were new software will be installed, you can execute</p> <pre><code>$ eb --show-config\n</code></pre> <p>EasyBuild is configured so that it searches in the user repository and the default repository on the system. The current directory is not part of the default search path but can be easily added with a command line option. By default, EasyBuild will not install dependencies of a package and fail instead, if one or more of the dependencies cannot be found, but that is also easily changed on the command line.</p>"},{"location":"software/installing/easybuild/#step-2-find-a-package-and-selecting-the-toolchain","title":"Step 2: Find a package and selecting the toolchain","text":"<p>EasyBuild has a large repository of available packages in different versions. Available packages can be searched using the following command, here for the CDO package</p> <pre><code>eb --search CDO\n</code></pre> <p>You will find that there are different versions of CDO and for different toolchains available (<code>gompi</code>, <code>iimpi</code>).</p> <p>You can list all dependencies using:</p> <pre><code>eb -Dr CDO-2.2.2-gompi-2023a.eb\n</code></pre> <p>Dependencies marked with <code>x</code> are already installed, the other dependencies will be installed if using the robot option.</p>"},{"location":"software/installing/easybuild/#step-3-install-the-package","title":"Step 3: Install the package","text":"<p>To show how to actually install a package, we continue with our <code>CDO-2.2.2-gompi-2023a.eb</code> example.</p> <p>After selecting the package installation recipe and the target software stack, the installation process can be submitted.  With the following commands, SLURM job files will be created, and submitted to the compute nodes. There the packages are build and module files created. The general syntax is: <pre><code>eb-install-all [options] [easybuild options] &lt;easyconfig&gt;.eb\n</code></pre> Additional SLURM arguments can be selected using the <code>--slurm-args</code> option, e.g. <code>--slurm-args='--account=xyz --time=00:10:00 --cpus-per-task'</code>. If specific architectures should be selected use e.g. <code>--arch='broadwell epyc2'</code>. After this options, EasyBuild arguments can be provided without prefix, e.g. <code>--robot</code>. </p> <p>For our example we can use the following command to generate the module on all architectures:</p> <pre><code>eb-install-all --robot CDO-2.2.2-gompi-2023a.eb\n</code></pre> <p>The job output is presented in the <code>eb_out.*</code> files, one for each architecture.</p> <p>If the build could not be finished in the default time of 1h, the walltime can be extended using:</p> <pre><code>eb-install-all --robot --slurm-args='--time=05:00:00' ...\n</code></pre> <p>Note</p> <p>Please check the end of the out file for the COMPLETED: Installation ended successfully statement.</p> <p>When all jobs have finished, you can type <code>module avail</code> and you should see the</p> <pre><code>CDO/2.2.2-gompi-2023a\n</code></pre> <p>module in the list. The <code>CDO/2.2.2-gompi-2023a</code> module can now be used just like any other module on the system. To use the CDO module, you don\u2019t need to load <code>EasyBuild</code>. That was only required for installing the package.  All you need to do to use the CDO module we just installed is </p> <pre><code>module load Workspace\nmodule load CDO/2.2.2-gompi-2023a\n</code></pre>"},{"location":"software/installing/easybuild/#some-common-problems","title":"Some common problems","text":"<ol> <li> <p><code>module avail</code> does not show the module.</p> <p>There are two possible causes for this.</p> <ol> <li> <p>Lmod builds a cache of all modules on the system. EasyBuild will clear the cache      so that it will be rebuilt after installing a software package and hence the      newly installed modules should be found. In rare cases, Lmod may be in a corrupt     state. In those cases the best solution is to clear the cache (unless it happens     right after running the <code>eb</code> command to install a module): </p> <pre><code>rm -rf ~/.lmod.d/.cache\n</code></pre> <p>and to log out and log in again to start with a clean shell.</p> </li> <li> <p>If the problem occurs later on, e.g., while running a job, then a common cause is that     you have  not compiled the package for the currently active software stack.     Please make sure to install software through the <code>eb-install-all</code>     command to generate modules for all available architectures.</p> </li> </ol> </li> <li> <p>EasyBuild complains that some modules are already loaded.</p> <p>EasyBuild prefers to work in a clean environment with no modules loaded that are installed via EasyBuild except for a very select list. It will complain if other modules are loaded (though only fail if a module for one of the packages that you try to install is already loaded). It is best to take this warning seriously and to install in a relatively clean shell, as otherwise the installation process may pick up software libraries that it should not have used.</p> </li> </ol>"},{"location":"software/installing/easybuild/#advanced-usage-adapting-easyconfigs","title":"Advanced Usage: Adapting EasyConfigs","text":"<p>In the following description and example we update an existing old EasyConfig for newer versions. In our case we want to update the version of Relion, the toolchain, and dependent libraries it is build with. </p> <ul> <li> <p>setup EasyBuild environment <pre><code>module load EasyBuild\nmodule load Workspace   ### OR Workspace_Home\n</code></pre></p> </li> <li> <p>find a suitable easyconfig <pre><code>$ eb --search Relion\n</code></pre> alternatively you may find easyconfigs online, e.g. https://github.com/easybuilders/easybuild-easyconfigs</p> </li> <li> <p>copy the easyconfig into a working directory (here <code>.</code>) <pre><code>$ cp $EBROOTEASYBUILD/easybuild/easyconfigs/r/RELION/RELION-3.0.4-foss-2017b.eb .\n</code></pre></p> </li> <li> <p>rename to the targeted versions (here newer relion, newer toolchain) <pre><code>$ mv RELION-3.0.4-foss-2017b.eb RELION-3.1.2-foss-2023a.eb\n</code></pre></p> </li> <li> <p>find the new versions of toolchain and libraries</p> <ul> <li>all installed version of a package can be listed using <code>module avail package</code>, e.g. <code>module avail foss</code></li> <li>available easyconfigs of non-installed packages can be listed using <code>eb --search package</code>. If there is a targeted version available, you can just define that dependency version in the above easyconfig and EasyBuild will find and use it. </li> </ul> </li> <li> <p>update the versions settings in the file</p> <ul> <li>package version, the toolchain version, and all related libraries</li> <li>Keep in mind that toolchain versions need to match (see toolchains above) <pre><code>easyblock = 'CMakeMake'\n\nname = 'RELION'\nversion = '3.1.2'                            #### The Relion version was '3.0.4' before\n\nhomepage = 'http://www2.mrc-lmb.cam.ac.uk/relion/index.php/Main_Page'\ndescription = \"\"\"RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a stand-alone computer\n program that employs an empirical Bayesian approach to refinement of (multiple) 3D reconstructions or 2D class\n averages in electron cryo-microscopy (cryo-EM).\"\"\"\n\ntoolchain = {'name': 'foss', 'version': '2023a'}   ### the foss toolchain version was 2020b before\ntoolchainopts = {'openmp': True}\n\nsource_urls = ['https://github.com/3dem/relion/archive']\nsources = ['%(version)s.tar.gz']\nchecksums = ['2580d66088923a644bc7d3b02efd154b775a3ec3d010426f382bb3be5db9c98b']\n\nbuilddependencies = [('CMake', '3.26.333]    ### was 3.9.5\n\ndependencies = [\n    ('X11', '20230603'),                     ### was 20171023\n    ('FLTK', '1.3.8'),                       ### was 1.3.4\n    ('LibTIFF', '4.5.0'),                    ### 4.0.9\n    ('tbb', '2021.11.0'),                       ### 2018_U5\n]\n\nconfigopts = \"-DCMAKE_SHARED_LINKER_FLAGS='-lpthread'  -DMPI_INCLUDE_PATH=$EBROOTOPENMPI/include \"\nconfigopts += \"-DMPI_C_COMPILER=$EBROOTOPENMPI/bin/mpicc -DMPI_CXX_COMPILER=$EBROOTOPENMPI/bin/mpicxx \"\nconfigopts += \"-DCUDA=OFF -DCudaTexture=OFF \"\nconfigopts += \"-DALTCPU=ON -DFORCE_OWN_TBB=OFF \"\n\nsanity_check_paths = {\n    'files': ['bin/relion'],\n    'dirs': []\n}\n\nmoduleclass = 'bio'\n</code></pre></li> </ul> </li> <li> <p>update the checksum (if package version is changed) The downloaded source packages are typically checked with SHA256 checksums. When we change to a different source code versio, the checksum changes too. And need to be updated. <pre><code>$ eb --force --inject-checksums sha256 RELION-3.1.2-foss-2023a.eb\n</code></pre></p> </li> <li> <p>build the new package as described in Installation above, e.g. <pre><code>$ eb-install-all --robot RELION-3.1.2-foss-2023a.eb\n</code></pre></p> </li> </ul>"},{"location":"software/installing/easybuild/#tips-and-tricks","title":"Tips and tricks","text":"<p>Even if EasyBuild tries to simplify the installation process, not always EasyConfigs are Build without issues. There can be several types of issues. Starting form issues in finding exiting packages up to compilation issues. </p>"},{"location":"software/installing/easybuild/#more-information","title":"More information","text":"<p>In the EasyBuild output <code>eb_out.*</code> files are issues summarized. Often more details are required. There are more detailed log files created in the temporary directory.  On the compute nodes they are deleted at the end of the job, but on the login node (epyc2) they are kept. The location is mentioned near the end of the output and typically is after <code>Results of the build can be found in the log file</code>.</p>"},{"location":"software/installing/easybuild/#lock-directories","title":"Lock directories","text":"<p>EasyBuild has a procedure to prevent building the same package (same version, same software stack) using lock files. If <code>eb-install-*</code> crashes due to time limit, the lock files are not removed properly. Therewith the next time you start <code>eb-install-*</code> for that package a message like will be presented at the end of the output:</p> <p><pre><code>ERROR: Build of /path/to/easyconfig.eb failed (err: 'build failed (first 300 chars): Lock /path/to/.locks/packageVersion.lock already exists, aborting!')\n</code></pre> In that moment the lock file should be already removed and the process can finally be started successfully again. </p>"},{"location":"software/installing/easybuild/#running-easybuild-interactively-on-a-compute-node","title":"Running EasyBuild interactively on a compute node","text":"<p>The eb-install-all tool builds the packages directly on a compute node within a SLURM job. If this fails, an investigation step may be running directly on the node, without more control of the setup, e.g. build directories. Therefore, EasyBuild can be started directly in a session on the compute node. First, an interactive session is established on the compute node. For example building Relion in the <code>$HOME</code> on an epyc2 node using a local copy of the EasyConfig file:</p> <pre><code>$ srun --pty --partition epyc2 bash\n$ module load Workspace_Home EasyBuild\n$ eb --tmpdir=$TMPDIR --robot RELION-3.1.3-fosscuda-2020b.eb\n</code></pre> <p>This may also be used when compiling on a specific GPU architecture. </p>"},{"location":"software/installing/easybuild/#further-reading","title":"Further reading","text":"<p>If you want to get more familiar with EasyBuild and develop your own EasyBuild recipes, we suggest the following sources of information:</p> <ul> <li>EasyBuild documentation</li> <li>EasyBuild tutorials</li> <li>The EasyBuild YouTube channel   contains recordings of a four-session tutorial   given for the LUMI User Support Team by Kenneth Hoste (UGent), the lead developer   of EasyBuild and Luca Marsella (CSCS)<ul> <li>Part 1: Introduction</li> <li>Part 2: Using EasyBuild</li> <li>Part 3: Advanced topics</li> <li>Part 4: EasyBuild on Cray systems</li> </ul> </li> <li>EasyBuild recipies<ul> <li>EasyBuilders repository,   the repository of EasyConfig files that also come with EasyBuild.</li> <li>ComputeCanada repository</li> <li>IT4Innovations repository</li> <li>Fred Hutchinson Cancer Research Center repository</li> <li>University of Antwerpen repository</li> <li>University of Leuven repository</li> </ul> </li> </ul>"},{"location":"software/installing/python/","title":"Installing Python packages","text":"<p>Over the past decade, the Python programming language and Scientific Python packages like NumPy, SciPy, JAX, and PyTorch have gained a lot of popularity in the data science and HPC communities.</p> <p>A Python installation usually consists of the Python interpreter, the Python standard library and one or more third party Python packages. Such Python packages may include both compiled code and a lot of so-called Python modules, i.e. a lot of small files containing Python code. A typical Conda environment tends to contain tens to hundreds of thousands of relatively small files filling up your file quota. Additionally, installing such a large number of small files to our storage can put a lot of strain on the filesystem and can lead to suboptimal cluster performance.</p> <p>Expert tip: Use containerized environments</p> <p>In order to circumvent the issue of \u201clots of tiny files\u201d and potentially degraded performance, you can wrap your Python environment in a container and run your simulations from within this isolated environment. Note that  this approach is especially useful if the Python environment is very static and is going to be used over long periods of time!</p>"},{"location":"software/installing/python/#anaconda-conda","title":"Anaconda (<code>conda</code>)","text":"<p>Conda is an open source environment and package management system. With Conda you can create independent environments, where you can install applications such as python and R, together with any packages which will be used by these applications. The environments are independent, with the Conda package manager managing the binaries, resolving dependencies, and ensuring that package used in multiple environments are stored only once. In a typical setting, each user has their own installation of a Conda and a set of personal environments.</p> <p>Generic binaries</p> <p>Conda installs generic binaries that may be suboptimal for the performance on UBELIX clusters. In most situations it is recommended to use the pre-installed libraries over the version that can be installed from <code>conda</code>.</p>"},{"location":"software/installing/python/#channels","title":"Channels","text":"<p>Conda channels are the locations where packages are stored. There are also multiple channels, with some important channels being:</p> <ul> <li><code>defaults</code>, the default channel,</li> <li><code>anaconda</code>, a mirror of the default channel,</li> <li><code>bioconda</code>, a distribution of bioinformatics software, and</li> <li><code>conda-forge</code>, a community-led collection of recipes, build infrastructure, and distributions for the conda package manager.</li> </ul> <p>The most useful channel that comes pre-installed in all distributions, is Conda-Forge. Channels are usually hosted in the official Anaconda page, but in some rare occasions custom channels may be used. For instance the default channel is hosted independently from the official Anaconda page. Many channels also maintain web pages with documentation both for their usage and for packages they distribute:</p> <ul> <li>Default Conda channel</li> <li>Bioconda</li> <li>Conda-Forge</li> </ul>"},{"location":"software/installing/python/#loading-the-anaconda3-module","title":"Loading the <code>Anaconda3</code> module","text":"<p>The <code>Anaconda3</code> distribution is provided as a module on UBELIX. To use any <code>conda</code> commands, load the module using</p> <pre><code>module load Anaconda3\n</code></pre>"},{"location":"software/installing/python/#using-conda","title":"Using <code>conda</code>","text":"<p>When using <code>conda</code> the system may complain about:</p> <pre><code>CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n    $ conda init &lt;SHELL_NAME&gt;\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\nSee 'conda init --help' for more information and options.\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n</code></pre> <p>Please do not run <code>conda init</code>. Instead initialise the conda environment using:</p> <pre><code>module load Anaconda3\neval \"$(conda shell.bash hook)\"\n</code></pre> <p>This should also be used in your batch submission scripts when working with conda environments.</p>"},{"location":"software/installing/python/#managing-environments","title":"Managing environments","text":"<p>As an example, the creation and use of an environment for pandas jobs is presented. The command, <pre><code>conda create --name pandas\n</code></pre> creates an environment named <code>pandas</code>. The environment is activated with the command <pre><code>conda activate pandas\n</code></pre> anywhere in the file system.</p> <p>Next, install the base R environment package that contains the R program, and any R packages required by the project. To install packages, first ensure that the <code>pandas</code> environment is active, and then install any package with the command <pre><code>conda install &lt;package_name&gt;\n</code></pre> all the required packages. Quite often, the channel name must also be specified: <pre><code>conda install --channel &lt;channel_name&gt; &lt;package_name&gt;\n</code></pre> Packages can be found by searching the conda-forge channel.</p> <p><pre><code>conda install --channel conda-forge pandas\n</code></pre> will install all the components required to use the pandas pacakge in Python. After all the required packages have been installed, the environment is ready for use.</p> <p>Packages in the conda-forge channel come with instructions for their installation. Quite often the channel is specified in the installation instructions, <code>-c conda-forge</code> or <code>--channel conda-forge</code>. </p> <p>After work in an environment is complete, deactivate the environment, <pre><code>conda deactivate\n</code></pre> to ensure that it does not interfere with any other operations. In contrast to modules, Conda is designed to operate with a single environment active at a time. Create one environment for each project, and Conda will ensure that any package that is shared between multiple environments is installed once.</p>"},{"location":"software/installing/python/#using-environments-in-submission-scripts","title":"Using environments in submission scripts","text":"<p>Since all computationally heavy operations must be performed in compute nodes, Conda environments are also used in jobs submitted to the queuing system. Returning to the pandas example, a submission script running a single core pandas job can use the <code>pandas</code> environment as follows: <pre><code>#SBATCH --job-name pandas-test-job\n#SBATCH --nodes 1\n#SBATCH --ntasks-per-node 1\n#SBATCH --cpus-per-task 1\n#SBATCH --time=0-02:00:00\n#SBATCH --partition epyc2,bdw\n\necho \"Launched at $(date)\"\necho \"Job ID: ${SLURM_JOBID}\"\necho \"Node list: ${SLURM_NODELIST}\"\necho \"Submit dir.: ${SLURM_SUBMIT_DIR}\"\necho \"Numb. of cores: ${SLURM_CPUS_PER_TASK}\"\n\nmodule load Anaconda3\neval \"$(conda shell.bash hook)\"\n\nconda activate pandas\n\npython pandas_test.py\n\nconda deactivate\n</code></pre></p>"},{"location":"software/installing/python/#cleaning-up-package-data","title":"Cleaning up package data","text":"<p>The Conda environment managers download and store a sizable amount of data to provided packages to the various environments. Even though the package data are shared between the various environments, they still consume space in your or your project\u2019s account.</p> <p>There are two main sources of unused data, the compressed archives of the packages that Conda stores in its cache when downloading a package, and the data of removed packages. All unused data can be removed with the command <pre><code>conda clean --all\n</code></pre> that opens up an interactive dialogue with details about the operations performed. You can follow the default option, unless you have manually edited any files in you package data directory (default location <code>${HOME}/conda</code>).</p> <p>Updating environments to remove old package versions</p> <p>As we create new environments, we often install the latest version of each package. However, if the environments are not updated regularly, we may end up with different versions of the same package across multiple environments. If we have the same version of a package installed in all environments, we can save space by removing unused older versions.</p> <p>To update a package across all environments, use the command <pre><code>for e in $(conda env list | awk 'FNR&gt;2 {print $1}'); do conda update --name $e &lt;package name&gt;; done\n</code></pre></p> <p>WARNING: Ensure this is really what you want! Sometimes you need a specific (older) version of a package in an environment because of compatibility!</p> <p>After updating packages, the <code>clean</code> command can be called to removed the data of unused older package versions.</p> <p>Sources</p> <ul> <li>Oficial Conda <code>clean</code> documentation</li> <li>Understanding Conda <code>clean</code></li> </ul>"},{"location":"software/installing/python/#pip","title":"Pip","text":"<p>In some cases Python packages are not avaible through the conda channels and need to be installed through pip. In this case simply ensure that <code>pip</code> is indeed set to the <code>pip</code> executable within the environment</p> <pre><code>which pip\n~/.conda/envs/&lt;env name&gt;/bin/pip\n</code></pre> <p>For instance, assume that a <code>mkdocs</code> project requires the following packages:</p> <ul> <li><code>mkdocs</code></li> <li><code>mkdocs-minify-plugin</code></li> </ul> <p>The package <code>mkdocs-minify-plugin</code> is less popular and thus is is not available though a Conda channel, but it is available in PyPI. Activate your <code>conda</code> environment and install the required packages with <code>pip</code></p> <pre><code>pip install --upgrade mkdocs mkdocs-minify-plugin\n</code></pre> <p>inside the environment. The packages will be installed inside a directory that <code>conda</code> created for the Conda environment, for instance <pre><code>${HOME}/conda/envs/mkdocs\n</code></pre> along side packages installed by <code>conda</code>. As a results, \u2018system-wide\u2019 installations with <code>pip</code> inside a Conda environment do not interfere with system packages.</p> <p>Do not install packages in Conda environments with pip as a user</p> <p>User installed packages (e.g.<code>pip install --user --upgrade mkdocs-minify-plugin</code>) are installed in the same directory for all environments, typically in <code>~/.local/</code>, and can interfere with other versions of the same package installed from other Conda environments.</p>"},{"location":"software/installing/python/#python-virtual-environments-virtualenv","title":"Python Virtual Environments (virtualenv)","text":"<p>The default Python is the OS Python</p> <p>When you log into UBELIX, running <code>python</code> without loading a module or using a container will result in using the operating system Python installation. This is a Python 3.9 which can\u2019t be upgraded! Make sure this is what you want before continuing with this section!</p> <p>Virtualenv is the Pythons native way of isolating a particular python environment from the default one. Each environment resides in a self-contained directory, so multiple virtualenvs can co-exist side by side with different versions of tools or dependencies installed. The downside of this approach is that the Python version can\u2019t be changed. If you need to be able to run specific versions of Python, please use the Conda installation method.</p>"},{"location":"software/installing/python/#creating-a-virtualenv","title":"Creating a virtualenv","text":"<p>We can create a virtualenv:</p> <pre><code>python -m venv --system-site-packages venvs/venv-for-demo\n</code></pre> <p>or</p> <pre><code>virtualenv --system-site-packages venvs/venv-for-demo\n</code></pre> <p>We recommend that, as in the example above, to use the <code>--system-site-packages</code> option. This ensures that already installed packages are used.</p>"},{"location":"software/installing/python/#using-a-virtualenv","title":"Using a virtualenv","text":"<p>To use a virtualenv it is necessary to activate it:</p> <pre><code>source venvs/venv-for-demo/bin/activate\n(venv-for-demo) [user@cluster:~]$\n</code></pre> <p>Once activated the prompt changes and any Python related commands that follow will refer to the Python installation and packages contained/linked in the virtualenv.</p>"},{"location":"software/installing/python/#installing-new-packages","title":"Installing new packages","text":"<p>To install packages in the virtualenv it needs to be activated, and while it is active any packages installed with pip will be actually installed inside the virtualenv itself:</p> <pre><code>source venvs/venv-for-demo/bin/activate\n(venv-for-demo) [user@cluster:~]$ pip install biopython\n</code></pre>"},{"location":"software/installing/python/#stop-using-a-virtualenv","title":"Stop using a virtualenv","text":"<p>To stop using a virtualenv one needs to deactivate it:</p> <pre><code>(venv-for-demo) [user@cluster:~]$ deactivate\n[user@cluster:~]$\n</code></pre>"},{"location":"software/installing/python/#removing-a-virtualenv","title":"Removing a virtualenv","text":"<p>To permanently remove a virtualenv one simply deletes the directory which contains it.</p> <pre><code>rm -rf venvs/venv-for-demo/\n</code></pre>"},{"location":"software/installing/python/#discouraged-installation-methods","title":"Discouraged installation methods","text":"<p>We strongly discourage installing Python packages directly through the OS Python with <code>pip</code> without using virtual environments as described above.</p>"},{"location":"software/installing/r/","title":"Installing R packages","text":"<p>R is installed as global module on UBELIX. There are already a longer list of pre-installed packages available. If you need additional packages you can install them into a shared HPC Workspace or into your home directory.</p>"},{"location":"software/installing/r/#loading-the-r-module","title":"Loading the R module","text":"<p>R is provided by an environment module and must be loaded explicitly:</p> <pre><code>module load R\nR --version\n  R version 4.2.1 (2022-06-23) -- \"Funny-Looking Kid\"\n  ...\n</code></pre>"},{"location":"software/installing/r/#listing-all-installed-packages","title":"Listing all installed packages","text":"<p>You can list all installed R packages using the following one-liner:</p> <pre><code>R -e 'installed.packages()'\n</code></pre>"},{"location":"software/installing/r/#installing-r-packages-into-a-shared-workspace","title":"Installing R packages into a shared Workspace","text":"<p>With the Workspace module we provide short-cuts to install R packages in the shared Workspace location. Therefore, the environment variable <code>$R_LIBS</code> is set to <code>$WORKSPACE/RPackages</code>. If this directory doesn\u2019t exist it needsto be created using:</p> <p><pre><code>module load Workspace\nmkdir $R_LIBS\n</code></pre> If you get the error <code>mkdir: cannot create directory ...</code> verify that you loaded a Workspace while installing the package.</p> <p>Then R packages can be installed using the <code>install.packages()</code> routine in an interactive R shell, e.g. for doParallel:</p> <pre><code>module load R\nR\n...\n&gt; install.packages(\"doParallel\")\n</code></pre> <p>Please follow the procedure as shown below at installation routine.</p> <p>Then the installed packaged will be available to you and all other Workspace members by simply loading the <code>Workspace</code> module. </p> <p>Tip</p> <p>Please remember to add the Workspace and the R module to your job scripts: <pre><code>module load Workspace\nmodule load R\n</code></pre></p>"},{"location":"software/installing/r/#installing-r-packages-into-your-home-directory","title":"Installing R packages into your home directory","text":"<p>Tip</p> <p>You can also use the procedure above by simply loading <code>Workspace_Home</code> to install into your HOME directory.</p> <p>If you are not using a Workspace module and try to install a package, at the first time R tries to install the package into a global/generic location, which is not writeable by users. You can then select to install in a \u201cpersonal library\u201d into your HOME:</p> <pre><code>module load R\nR\n&gt; install.packages(\"doParallel\")\nInstalling package into \u2018/usr/lib64/R/library\u2019\n(as \u2018lib\u2019 is unspecified)\nWarnung in install.packages(\"doParallel\")\n  'lib = \"/usr/lib64/R/library\" ist nicht schreibbar\nWould you like to use a personal library instead?  (y/n)\n</code></pre> <p>Next, type \u201cy\u201d to create your personal library at the default location within your HOME directory:</p> <pre><code>Would you like to create a personal library\n~/R/x86_64-redhat-linux-gnu-library/4.0\n</code></pre>"},{"location":"software/installing/r/#installation-routine","title":"Installation Routine","text":"<p>You will be asked to select a CRAN mirror to download from. The mirror list will be not the same as below. The mirror list is constantly changing, but will look like it.</p> <p>Pick any country nearby, i.e. Switzerland. If https makes problems, pick \u201c(HTTP mirrors)\u201d and then select something nearby as shown below</p> <pre><code>--- Bitte einen CRAN Spiegel f\u00fcr diese Sitzung ausw\u00e4hlen ---\nError in download.file(url, destfile = f, quiet = TRUE) :\n  nicht unterst\u00fctztes URL Schema\nHTTPS CRAN mirror\n 1: 0-Cloud [https]                2: Austria [https]\n 3: Chile [https]                  4: China (Beijing 4) [https]\n 5: Colombia (Cali) [https]        6: France (Lyon 2) [https]\n 7: France (Paris 2) [https]       8: Germany (M\u00fcnster) [https]\n 9: Iceland [https]               10: Mexico (Mexico City) [https]\n11: Russia (Moscow) [https]       12: Spain (A Coru\u00f1a) [https]\n13: Switzerland [https]           14: UK (Bristol) [https]\n15: UK (Cambridge) [https]        16: USA (CA 1) [https]\n17: USA (KS) [https]              18: USA (MI 1) [https]\n19: USA (TN) [https]              20: USA (TX) [https]\n21: USA (WA) [https]              22: (HTTP mirrors)\n\nSelection: 22\nHTTP CRAN mirror\n 1: 0-Cloud                       2: Algeria\n 3: Argentina (La Plata)          4: Australia (Canberra)\n 5: Australia (Melbourne)         6: Austria\n 7: Belgium (Antwerp)             8: Belgium (Ghent)\n(...)\n65: Slovakia                     66: South Africa (Cape Town)\n67: South Africa (Johannesburg)  68: Spain (A Coru\u00f1a)\n69: Spain (Madrid)               70: Sweden\n71: Switzerland                  72: Taiwan (Chungli)\n73: Taiwan (Taipei)              74: Thailand\n75: Turkey (Denizli)             76: Turkey (Mersin)\n(...)\n93: USA (OH 2)                   94: USA (OR)\n95: USA (PA 2)                   96: USA (TN)\n97: USA (TX)                     98: USA (WA)\n99: Venezuela\nSelection: 71\n</code></pre> <p>Finally, the package gets installed. After installing the package you can close the interactive session by typing q().</p> <p>Do not forget to load the corresponding library (for each R session) before using functions provided by the package:</p> <pre><code>&gt; library(doParallel)\n</code></pre>"},{"location":"software/packages/JupyterLab/","title":"Jupyter Lab","text":"<p>Deprecation notice</p> <p>With the release of UBELIX OnDemand, we provide Jupyter Lab as an interactive app. We recommend to use the new interactive app to run Jupyter Lab on UBELIX. The functionality described on this page may be removed in the future.</p>"},{"location":"software/packages/JupyterLab/#description","title":"Description","text":"<p>Some useful information on using Jupyter Lab on UBELIX compute nodes.  </p> <p>IMPORTANT: in the following we show how to start the server on a compute node.  Please keep in mind that these resources will be dedicated for you, thus and idle session will waste resources.  Please quit your session as soon as you don\u2019t use it anymore, even for a lunch break. Your notebook will maintain all you input/output.</p>"},{"location":"software/packages/JupyterLab/#overview","title":"Overview","text":"<p>On UBELIX we provide Jupyter Lab for working with Jupyter Notebooks.  JupyterLab is a single-user web-based Notebook server, running in the user space.  JupyterLab servers should be started preferably on a compute node, especially for compute intensive or memory intensive workloads.  After starting the Jupyter Lab server your local browser can be connected using port forwarding. Therefore port forwarding needs to be enabled properly.  On this page we describe:</p> <ul> <li>Launch JupyterLab<ul> <li>Connect to UBELIX and establishing SSH port forwarding </li> <li>SSH with port forwarding</li> <li>Launch the JupyterLab server</li> <li>Launch JupyterLab in your local browser</li> </ul> </li> <li>Kernels</li> <li>Packages</li> </ul>"},{"location":"software/packages/JupyterLab/#launch-jupyterlab","title":"Launch JupyterLab","text":"<p>Since JupyterLab is a web based application, a port needs to be forwarded to your local machine, where your browser can connect to.  This port numbers need to be between 2000 and 65000 and need to be unique on the present machine.  The default port for JupyterLab is 8888, but only one user can use this at a time.</p> <p>To avoid the need for modifying the following procedure again and again, we suggest to (once) select a unique number (between 2000 and 65000). And then following commands can be hopefully reused without modification. The port needs to be specified while establishing the connection to UBELIX and while launching JupyterLab. In the following we use the port number 15051 (please select another number).</p>"},{"location":"software/packages/JupyterLab/#setup-ssh-with-port-forwarding","title":"Setup SSH with port forwarding","text":"<p>First, the port forwarding needs to be enabled between your local machine and UBELIX. Therewith a local port will be connected to the remote port on UBELIX. This ports are numbers between 2000 and 65000, which needs to be unique on the both sides. The default port for JupyterLab is 8888, but only one user can use this at a time. For simplicity, we kept both numbers the same (here 15051). This can be specified on the command line in the terminal.</p> <p>The <code>ssh</code> command from your local machine to the ubelix login node  needs to be called with following arguments:</p> <p><pre><code>ssh -L 15051:localhost:15051 &lt;user&gt;@submit03.unibe.ch\n</code></pre> If configured in your <code>.ssh/config</code>, you can also use the alias instead of the full name for UBELIX. Where <code>&lt;user&gt;</code> is you Campus Account username.</p> <p>Note: MobaXterm has an internal terminal which acts like a linux terminal and can be configured as described in the Standard Terminal Setup. Therewith, the SSH command line approach above can be used.</p>"},{"location":"software/packages/JupyterLab/#launch-the-jupyterlab-server","title":"Launch the JupyterLab server","text":"<p>On UBELIX, the required Anaconda3 module needs to be loaded. If you want to use additional kernels (R) you need to load additional modules, e.g. IRkernel (for R kernels):</p> <pre><code>module load Anaconda3\n</code></pre> <p>A script is provided, taking care of enabling the port forwarding to the compute node and launching JupyterLab. </p> <p><pre><code>jupyter-compute 15051 --time=00:45:00  # please change port number\n</code></pre> This tool will lauch the server on a compute node, and establish the port forwarding. After general output, JupyterLab prints a URL with a unique key and the network port number where the web-server is listening, this should look similar to:</p> <pre><code>...\n[C 21:43:35.291 LabApp]\n\n    To access the notebook, open this file in a browser:\n        file:///gpfs/homefs/id/ms20e149/.local/share/jupyter/runtime/nbserver-30194-open.html\n    Or copy and paste one of these URLs:\n        http://anode001:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073\n     or http://127.0.0.1:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073\n</code></pre> <p>The last line needs to be copied in your local browser.</p> <p>Attention</p> <p>do not use <code>Ctrl + C</code> for copying the link, this will abort the server process and kill your job. </p> <p>QOS</p> <p>the <code>jupyter-compute</code> tool uses an special Slurm Quality of Service (QoS), which should reduce queuing times for interactive jobs.  Since interactive jobs are considered to be finished within less than a working day, the walltime limit cannot exceed 8h (default run time is 6h, afterwards you are expected to have a break ;) ).  You can disable that qos using the option <code>--no-qos</code>, but please release the resources as soon as you are not actively working with the resources anymore.</p>"},{"location":"software/packages/JupyterLab/#jupyterlab-in-your-local-browser","title":"JupyterLab in your local browser","text":"<p>The full address on the last line (starting with the 127.0.0.1) of the Jupyter Server statement including the token needs to be copied into your browser on your local machine.  After initializing Jupyter Lab you should see a page similar to:</p> <p></p> <p>Therewith the Notebook and its containing tasks are performed on a compute node, which can double check e.g. using using the following in Python:</p> <pre><code>import socket\nprint(socket.gethostname())\n</code></pre> <p>IMPORTANT: Please remember to stop your Jupyter Lab server and therewith your slurm job, when you do not need it anymore. Thus, the resource get available to other users again. </p> <p>Note: After stopping the JupyterLab server some sessions may get corrupted and do not take input correctly anymore. In this case just quit and re-establish your ssh session.</p>"},{"location":"software/packages/JupyterLab/#jupyterlab-with-multiple-cpu-cores","title":"JupyterLab with multiple CPU cores","text":"<p>More resources can be requested, e.g. by using:</p> <p><pre><code>jupyter-compute 15051 --ntasks 1 --time=01:00:00 --cpus-per-task 5 --mem 512MB\n</code></pre> Where 5 cores are requested for threading and a total memory of 3GB.  Please do not use multiprocessing.cpu_count() since this is returning the total amount of cores on the node.  Furthermore, if you use libraries, which implement threading: align the numbers of threads (often called jobs) to the selected number of cores (otherwise the performance will be affected).</p> <p>OR requesting GPU resources on a node with a NVIDIA graphics card: <pre><code>jupyter-compute 15051 --ntasks 1 --time=01:00:00 --partition=gpu --gres=gpu:rtx3090:1\n</code></pre></p>"},{"location":"software/packages/JupyterLab/#kernels","title":"Kernels","text":""},{"location":"software/packages/JupyterLab/#python","title":"Python","text":"<p>By default the Python3 kernel from the Anaconda3 environment is installed. You can create further kernels by creating new conda environments as well.</p>"},{"location":"software/packages/JupyterLab/#adding-a-conda-environment-to-jupyter","title":"Adding a conda environment to Jupyter","text":"<p>You can add a new (Python) kernel to your Jupyter based on a conda environment. When in Jupyter, you will then be able to select the name from the kernel list, and it will be using the packages you installed. Follow these steps to do this (replacing <code>&lt;env&gt;</code> with the name you want to give your conda environment):</p> <pre><code>conda activate &lt;env&gt;\npython -m ipykernel install --user --name &lt;env&gt;\n</code></pre>"},{"location":"software/packages/JupyterLab/#r","title":"R","text":"<p>To use R with Jupyterlab you can either install use R from the UBELIX software modules or a custom Anaconda3 environment.</p> <p>To use R from the UBELIX software modules ensure that the R module is loaded and the <code>IRkernel</code> package is installed before starting <code>jupyter-compute</code></p> <pre><code>module load R\nR\n&gt; install.packages('IRkernel')\n</code></pre>"},{"location":"software/packages/JupyterLab/#packages","title":"Packages","text":"<p>There are a long list of default packages provided by Anaconda3 (list all using <code>!pip list</code>) and R (list using <code>installed.packages(.Library)</code>, note the list is shortened). </p> <p>Furthermore, you can install additional packages in Python using <code>pip install --user</code> or in R using <code>install.packages(\"sampling\")</code>. </p>"},{"location":"software/packages/ParaView/","title":"ParaView","text":"<p>This article describes the steps to launch a ParaView Server on UBELIX compute nodes and connect a local ParaView client with it.</p>"},{"location":"software/packages/ParaView/#prerequisites","title":"Prerequisites","text":"<p>The local (on your local machine) ParaView version need to match the version loaded on UBELIX.</p>"},{"location":"software/packages/ParaView/#launch-paraview-server","title":"Launch ParaView server","text":"<p>First, as mentioned, establish a SSH session with Port forwarding, another port in the range [2000-65000] should be selected: </p> <pre><code>ssh -Y -L 15051:localhost:15051 submit03.unibe.ch\n</code></pre> <p>Then load the modules:</p> <pre><code>module load ParaView\n</code></pre> <p>The ParaView version can be displayed using:</p> <pre><code>module list ParaView\n\nCurrently Loaded Modules Matching: ParaView\n  1) ParaView/5.8.1-foss-2020b-mpi\n</code></pre> <p>Thus, in this example ParaView 5.8.1 need to be used in the local machine. </p> <p>To start the ParaView server on a compute node you can use:</p> <pre><code>pvserver-parallel 15051   ### use your selected port number\n</code></pre> <p>This submits a job with <code>1 core</code> for <code>1h</code> in the <code>epyc2</code> partition.  The tool prints a reminder to stop the job if not required anymore and shows the queueing information regularly:</p> <pre><code>ParaView remote Server submitted to compute nodes\n  when finished please kill the server using:\n     scancel 2394231\njob 2394231 status:\n   JOBID  PARTITION    STATE               START_TIME\n 2394231      epyc2  PENDING                      N/A\npvserver ready to connect on port 15051.\nWhen finished, please stop ParaView Server using\n scancel 2394231\n</code></pre> <p>Please cancel your job with <code>scancel $JOBID</code> or all your running jobs using <code>scancel -u $USER</code> if not needed anymore. </p> <p>Additional resources can be requested by <code>--slurm-args=\"\"</code> option with the desired slurm options, e.g. <code>3 cores</code> for <code>20 min</code>:</p> <pre><code>pvserver-parallel 15051 --slurm-args=\"--cpus-per-task=3 --time=00:20:00\" \n</code></pre> <p>In addition ParaView arguments can be added without any prefix. </p>"},{"location":"software/packages/ParaView/#connect-local-client","title":"Connect local client","text":"<p>Finally, the client on your local machine can connect using <code>localhost</code> and the selected port, here <code>15051</code>. E.g. using <code>pvpython</code>:</p> <pre><code>pvpython\n\nWARNING: Python 2.7 is not recommended.\nThis version is included in macOS for compatibility with legacy software.\nFuture versions of macOS will not include Python 2.7.\nInstead, it is recommended that you transition to using 'python3' from within Terminal.\n\nPython 2.7.16 (default, May  8 2021, 11:48:02)\n[GCC Apple LLVM 12.0.5 (clang-1205.0.19.59.6) [+internal-os, ptrauth-isa=deploy on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from paraview.simple import *\n&gt;&gt;&gt; Connect(\"localhost\", 15051)\nConnection (cs://localhost:15051) [2]\n</code></pre>"},{"location":"software/packages/VisualStudioCode/","title":"Interactive / Remote Computing with VS Code","text":"<p>Deprecation notice</p> <p>With the release of UBELIX OnDemand, we provide VS Code as an interactive app. We recommend to use the new interactive app to run VS Code on UBELIX. The functionality described on this page may be removed in the future.</p> <p>Important</p> <ul> <li>This tutorial shows how to work interactively on a compute node! If you need to access the submit nodes, most of the steps below can be skipped!</li> <li>Please keep in mind that these resources will be dedicated for you, thus an idle session will waste resources.</li> </ul> <p>This tutorial outlines how to set up VS Code for interactive/remote development/debugging on UBELIX computing nodes.</p>"},{"location":"software/packages/VisualStudioCode/#prerequisites","title":"Prerequisites","text":"<ul> <li>The latest version of VS Code installed on your local machine</li> <li>Latest version of the \u201cRemote Development\u201d extension pack</li> </ul>"},{"location":"software/packages/VisualStudioCode/#setup","title":"Setup","text":"<p>The following steps need to be performed only once.</p>"},{"location":"software/packages/VisualStudioCode/#local-ssh-config","title":"Local ssh config","text":"<p>Add the following lines to the ssh config file on your local machine (~/.ssh/config) and replace  with your UBELIX username: <pre><code>Host ubelix\n  HostName submit02.unibe.ch\n  IdentityFile ~/.ssh/id_rsa_ubelix\n  ServerAliveInterval 60\n  User &lt;name&gt;\n\nHost ubelix-code-tunnel\n  ProxyCommand ssh ubelix \"nc \\$(squeue --me --name=code-tunnel --states=R -h -O NodeList,Comment)\"\n  StrictHostKeyChecking no\n  ServerAliveInterval 240\n  ServerAliveCountMax 2\n  User &lt;name&gt;\n</code></pre> <p>Attention Windows users</p> <p>Due to different symbols escaping sequences between Linux and Windows, the ProxyCommand in the ubelix-code-tunnel block needs to be changed on Windows. Note the extra \\ !</p> <pre><code>Host ubelix-code-tunnel\n  ProxyCommand ssh ubelix \"nc \\\\$(squeue --me --name=code-tunnel --states=R -h -O NodeList,Comment)\"\n  StrictHostKeyChecking no\n  ServerAliveInterval 240\n  ServerAliveCountMax 2\n  User &lt;name&gt;\n</code></pre> <p>You can also make these modifications from within Visual Studio Code using the command palette <code>&gt; Remote-SSH: Open SSH Configuration File</code>.</p>"},{"location":"software/packages/VisualStudioCode/#add-code-tunnelsbatch","title":"Add code-tunnel.sbatch","text":"<p>Connect to the UBELIX cluster and create the following sbatch file in your home directory (<code>~/code-tunnel.sbatch</code>):</p> <pre><code>#!/bin/bash\n# -----------------------------------------------------------------------------\n# USER CODE TUNNEL SETTINGS\n# -----------------------------------------------------------------------------\n# You may add other configs here for your desired development environment,\n# like cpu cores, gpu, memory per core, etc.\n#SBATCH --time=01:00:00\n#SBATCH --partition=epyc2\n#SBATCH --ntasks=4\n#SBATCH --mem-per-cpu=2G\n#\n# -----------------------------------------------------------------------------\n# NO MODIFICATIONS BELOW THIS LINE\n# -----------------------------------------------------------------------------\n#SBATCH --job-name=\"code-tunnel\"\n#SBATCH --output=code-tunnel.log\n#SBATCH --signal=B:TERM@60 # tells the controller\n                           # to send SIGTERM to the job 60 secs\n                           # before its time ends to give it a\n                           # chance for better cleanup.\n\ncleanup() {\n    echo \"Caught signal - removing SLURM env file\"\n    rm -f ~/.code-tunnel-env.bash\n}\n\n# Trap the timeout signal (SIGTERM) and call the cleanup function\ntrap 'cleanup' SIGTERM\n\n# find open port\nPORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()')\nscontrol update JobId=\"$SLURM_JOB_ID\" Comment=\"$PORT\"\n\n# store SLURM variables to file\nenv | awk -F= '$1~/^SLURM_/ || $1==\"TMPDIR\" {print \"export \"$0}' &gt; ~/.code-tunnel-env.bash\n\n# start sshd server on the available port\necho \"Starting sshd on port $PORT\"\n/usr/sbin/sshd -D -p ${PORT} -f /dev/null -h ${HOME}/.ssh/id_ed25519_ubelix_internal &amp;\nwait\n</code></pre>"},{"location":"software/packages/VisualStudioCode/#export-slurm-job-variables","title":"Export SLURM job variables","text":"<p>A running slurm job exports a number of environmental variables to the shell to properly set up the user job. In order to make these automatically available from within your Visual Studio Code tunnel, add the following to the <code>~/.bashrc</code> file on UBELIX:</p> <pre><code># source slurm environment if we're connecting through code-tunnel\n[ -f ~/.code-tunnel-env.bash ] &amp;&amp; source ~/.code-tunnel-env.bash\n</code></pre>"},{"location":"software/packages/VisualStudioCode/#usage","title":"Usage","text":"<p>The following steps are performed every time you want to connect your VS Code to the cluster</p>"},{"location":"software/packages/VisualStudioCode/#start-ubelix-code-tunnel-job","title":"Start UBELIX code-tunnel job","text":"<p>From your local terminal, connect to the cluster using <code>ssh ubelix</code> and once logged in,  use <code>sbatch code-tunnel.sbatch</code> to start the remote server. Make sure that your job does run!</p> <p>Sample output:</p> <pre><code>[user@submit02 ~]$ sbatch code-tunnel.sbatch\nSubmitted batch job 1383495\n[user@login0b ~]$ squeue --me\n\n  JOBID      PARTITION      NAME             USER      ST      TIME      NODES      NODELIST(REASON)\n1383495          epyc2      code-tunnel      user       R      0:17          1           bnode001\n</code></pre>"},{"location":"software/packages/VisualStudioCode/#connect-to-code-tunnel-from-vs-code","title":"Connect to code-tunnel from VS Code","text":"<p>Open VS Code on your local machine and connect to your projects using <code>Remote Explorer</code> with <code>ubelix-code-tunnel</code> as the ssh target.</p>"},{"location":"software/packages/alphafold3/","title":"AlphaFold3","text":"<p>AlphaFold3 is a cutting-edge AI system developed by DeepMind for predicting protein structures with high accuracy. Building on its predecessors, AlphaFold3 integrates additional molecular modeling capabilities, making it a powerful tool for structural biology research.</p> <p>This guide provides step-by-step instructions on setting up and running AlphaFold3 on UBELIX using Slurm.</p>"},{"location":"software/packages/alphafold3/#directory-structure-setup","title":"Directory Structure Setup","text":"<p>Before running AlphaFold3, set up the necessary directory structure.</p> <ol> <li>Choose a suitable location for AlphaFold3. For example:    <pre><code>export AF3_ROOT=~/alphafold3/\n</code></pre></li> <li>Create the required directories:    <pre><code>mkdir -p $AF3_ROOT\nmkdir -p $AF3_ROOT/input             # Store input JSON files\nmkdir -p $AF3_ROOT/output            # Store generated structure outputs\nmkdir -p $AF3_ROOT/model_parameters  # Store downloaded model parameters\nmkdir -p $AF3_ROOT/databases         # Store public databases\n</code></pre></li> </ol>"},{"location":"software/packages/alphafold3/#cloning-the-alphafold3-repository","title":"Cloning the AlphaFold3 Repository","text":"<p>Clone the official AlphaFold3 source code from GitHub:</p> <pre><code>cd $AF3_ROOT\ngit clone https://github.com/google-deepmind/alphafold3.git src\n</code></pre>"},{"location":"software/packages/alphafold3/#creating-the-slurm-submission-script","title":"Creating the Slurm Submission Script","text":"<p>Create a Slurm submission script (e.g., <code>run_alphafold3.sh</code>) for running AlphaFold3 on a GPU node.</p> <ol> <li>Navigate to the project directory:    <pre><code>cd $AF3_ROOT\n</code></pre></li> <li>Create <code>run_alphafold3.sh</code> and add the following content:    <pre><code>#!/bin/bash\n#SBATCH --job-name=\"alphafold3_job\"\n#SBATCH --time=01:00:00\n#SBATCH --partition=gpu\n#SBATCH --gres=gpu:rtx4090:1\n#SBATCH --cpus-per-task=16\n#SBATCH --mem-per-cpu=5760M\n\nmodule load CUDA/12.6\n\nsingularity exec \\\n     --nv \\\n     --bind $PWD/input:/root/af_input \\\n     --bind $PWD/output:/root/af_output \\\n     --bind $PWD/model_parameters:/root/models \\\n     --bind $PWD/databases:/root/public_databases \\\n     /storage/software/singularity/containers/alphafold3.sif \\\n     python src/run_alphafold.py \\\n     --json_path=/root/af_input/fold_input.json \\\n     --model_dir=/root/models \\\n     --db_dir=/root/public_databases \\\n     --output_dir=/root/af_output\n</code></pre></li> <li>Save and close the file</li> </ol>"},{"location":"software/packages/alphafold3/#downloading-public-databases","title":"Downloading Public Databases","text":"<p>AlphaFold3 requires publicly available databases for structure prediction.</p> <p>Danger: Very large databases</p> <p>The AlphaFold3 databases are nearly 650GB in size. To avoid redundant downloads and conserve storage, it is recommended that a single copy be maintained in a shared workspace for the entire lab. Check with colleagues and supervisors if a copy of the databases is already accessible to you!</p> Link existing databasesInstall databases to shared workspaceInstall databases to user home  <p>If the public databases are already available, simply create a symbolic link to their location:</p> <pre><code>cd $AF3_ROOT\nln -s /path/to/workspace/databases databases\n</code></pre> <ol> <li>Navigate to the shared workspace to store the dabases, e.g.:    <pre><code>cd /path/to/workspace/alphafold3/databases\n</code></pre></li> <li>Download the database fetch script:    <pre><code>wget https://raw.githubusercontent.com/google-deepmind/alphafold3/refs/heads/main/fetch_databases.sh\n</code></pre></li> <li>Make the script executable and run it:    <pre><code>chmod u+x fetch_databases.sh\n./fetch_databases.sh databases\n</code></pre></li> <li>Continue with lining existing databases    <pre><code>cd $AF3_ROOT\nln -s /path/to/workspace/databases databases\n</code></pre></li> </ol> <ol> <li>Navigate to the AlphaFold3 project directory:    <pre><code>cd $AF3_ROOT\n</code></pre></li> <li>Download the database fetch script:    <pre><code>wget https://raw.githubusercontent.com/google-deepmind/alphafold3/refs/heads/main/fetch_databases.sh\n</code></pre></li> <li>Make the script executable and run it:    <pre><code>chmod u+x fetch_databases.sh\n./fetch_databases.sh databases\n</code></pre></li> </ol>"},{"location":"software/packages/alphafold3/#downloading-model-parameters","title":"Downloading Model Parameters","text":"<p>AlphaFold3 model parameters need to be downloaded separately. To request access to the model parameters, please complete this form. Access will be granted at Google DeepMind\u2019s sole discretion. You may only use AlphaFold 3 model parameters if received directly from Google. Use is subject to these terms of use.</p> <p>Ensure they are stored in the <code>model_parameters</code> directory:</p> <pre><code>cd $AF3_ROOT/model_parameters\n# Download and extract model parameters following official AlphaFold3 instructions.\n</code></pre>"},{"location":"software/packages/alphafold3/#preparing-input-file","title":"Preparing Input File","text":"<p>AlphaFold3 requires a JSON input file containing sequence and configuration details. Create an input file at <code>input/fold_input.json</code>:</p> <p>Example:</p> <pre><code>{\n  \"name\": \"2PV7\",\n  \"sequences\": [\n    {\n      \"protein\": {\n        \"id\": [\"A\", \"B\"],\n        \"sequence\": \"GMRESYANENQFGFKTINSDIHKIVIVGGYGKLGGLFARYLRASGYPISILDREDWAVAESILANADVVIVSVPINLTLETIERLKPYLTENMLLADLTSVKREPLAKMLEVHTGAVLGLHPMFGADIASMAKQVVVRCDGRFPERYEWLLEQIQIWGAKIYQTNATEHDHNMTYIQALRHFSTFANGLHLSKQPINLANLLALSSPIYRLELAMIGRLFAQDAELYADIIMDKSENLAVIETLKQTYDEALTFFENNDRQGFIDAFHKVRDWFGDYSEQFLKESRQLLQQANDLKQG\"\n      }\n    }\n  ],\n  \"modelSeeds\": [1],\n  \"dialect\": \"alphafold3\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"software/packages/alphafold3/#submitting-the-job","title":"Submitting the Job","text":"<p>Once everything is set up, submit the job to Slurm:</p> <pre><code>sbatch run_alphafold3.sh\n</code></pre>"},{"location":"software/packages/alphafold3/#output-location","title":"Output Location","text":"<p>Once the job is complete, the predicted structures will be available in:</p> <pre><code>$AF3_ROOT/output/\n</code></pre>"},{"location":"software/packages/bzip2/","title":"Parallel BZIP2","text":"<p>Data frequently needs to be packed and compressed for archiving or transfer. There are multiple tools available like tar and gzip, bzip. Pbzip2 is a parallel implementation of bzip2. For general information see bzip2 and pbzip2. The tool is available on all nodes without loading any module. </p>"},{"location":"software/packages/bzip2/#usage","title":"Usage","text":"<p>Parallel packing and compressing can be performed on a compute node using <code>tar</code> with a specified number of threads</p> <p>As an example, a file or directory <code>/data/to/pack</code> can be packed and compressed into a file <code>/packed/file.tar.bz2</code> using the job script:</p> <pre><code>#SBATCH --job-name=\"pbzip2\"\n#SBATCH --time=01:00:00\n#SBATCH --mem-per-cpu=2G\n## For parallel jobs with 8 cores\n#SBATCH --cpus-per-task=8           ## select the amount of cores required\n\nsource=\"data/to/pack\"               ## specify your data to compress\ntarget=\"/packed/file.tar.bz2\"       ## specify directory and filename\n\n# archive dir data_unibe to a tar file and compress it using pbzip2\nsrun tar -cS $source | pbzip2 -p$SLURM_CPUS_PER_TASK &gt; $target\n\n# Generate a sha256 fingerprint, to later check the integrity \nsha256sum $target &gt; ${target}.sha256sum\n</code></pre>"},{"location":"software/packages/matlab/","title":"MATLAB","text":"<p>UBELIX is featuring a recent release of MATLAB.</p>"},{"location":"software/packages/matlab/#facts-about-matlab-on-ubelix","title":"Facts about MATLAB on UBELIX","text":"<ul> <li>It can run in parallel on one node, thanks to the Parallel Computing ToolBox</li> <li>It can take advantage of GPUs</li> <li>It cannot run on more than one node as we do not have the Distributed Computing Toolbox.</li> </ul>"},{"location":"software/packages/matlab/#running-matlab-on-the-compute-nodes","title":"Running MATLAB on the Compute Nodes","text":"<p>Submitting a MATLAB job to the cluster is very similar to submitting any other serial job. Lets try to run a simple MATLAB script which we will put in a file boxfilter.m</p> <p>boxfilter.m <pre><code>% Compute a local mean filter over a neighborhood of 11x11 pixels\n\n% Read image into workspace:\noriginal = imread('girlface.png');\n% Perform the mean filtering:\nfiltered = imboxfilt(original, 11);\n% Save the original and the filtered image side-by-side:\nimwrite([original, filtered],'comparison.png');\n</code></pre></p> <p>Now we need a submission script</p> <p>boxfilter.sh <pre><code>#!/bin/bash\n#SBATCH --mail-user=foo@bar.unibe.ch\n#SBATCH --mail-type=end,fail\n#SBATCH --job-name=boxfilter\n#SBATCH --time=00:10:00\n#SBATCH --mem-per-cpu=2G\n\n# Load MATLAB form the environment modules\nmodule load MATLAB\n# Tell MATLAB to run our box filter.m file and exit\nmatlab -nodisplay -r \"boxfilter, exit\"\n</code></pre></p>"},{"location":"software/packages/matlab/#passing-arguments-to-a-m-file","title":"Passing Arguments to a m-File","text":"<p>There are several ways to provide input arguments in MATLAB.</p>"},{"location":"software/packages/matlab/#define-the-variables-before-running-the-script","title":"Define the Variables Before Running the Script","text":"<p>Lets take the box filter.m example from above. The script is not universal because the name of the input image and the box size is hardcoded in the script. We make the script more generally usable by:</p> <p>boxfilter.m</p> <pre><code>% Compute a local mean filter over a neighborhood of 11x11 pixels\n\n% Read image into workspace:\noriginal = imread(inputImg);\n% Perform the mean filtering:\nfiltered = imboxfilt(original, x);\n% Save the original and the filtered image side-by-side:\nimwrite([original, filtered],'comparison.png');\n</code></pre> <p>and then:</p> <p>boxfilter.qsub</p> <pre><code>#!/bin/bash\n(...)\n# Load MATLAB form the environment modules\nmodule load MATLAB\n# Tell MATLAB to run our box filter.m file and exit\nmatlab -nodisplay -r \"inputImg='girlface.png'; x=11; boxfilter, exit\"\n</code></pre>"},{"location":"software/packages/matlab/#advanced-topics","title":"Advanced Topics","text":""},{"location":"software/packages/matlab/#multithreading","title":"Multithreading","text":"<p>By default, MATLAB makes use of the multithreading capabilities of the node on which it is running. It is crucial that you allocate the same number of slots for your job as your job utilizes cores.</p> <p>Disable Computational Multithreading</p> <p>If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB:</p> <pre><code>matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\"\n</code></pre> <p>Disable Computational Multithreading If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB:</p> <pre><code>matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\"\n</code></pre> <p>Running MATLAB in Multithreaded Mode</p> <p>Most of the time, running MATLAB in single-threaded mode will meet your needs. If you have mathematically intense computations that might benefit from multi-threading capabilities provided by MATLAB\u2019s BLAS implementation, then you should limit MATLAB to a well defined number of threads, so that you can allocate the correct number of slots for your job. Use the maxNumCompThreads(N) function to control the number of computational threads:</p>"},{"location":"software/packages/matlab/#infos-about-featured-matlab","title":"Infos about featured MATLAB","text":"<pre><code>-----------------------------------------------------------------------------------------------------\nMATLAB Version: 9.11.0.1769968 (R2021b)\nMATLAB License Number: 40639324\nOperating System: Linux 3.10.0-1160.76.1.el7.x86_64 #1 SMP Wed Aug 10 16:21:17 UTC 2022 x86_64\nJava Version: Java 1.8.0_202-b08 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n-----------------------------------------------------------------------------------------------------\nMATLAB                                                Version 9.11        (R2021b)\nSimulink                                              Version 10.4        (R2021b)\n5G Toolbox                                            Version 2.3         (R2021b)\nAUTOSAR Blockset                                      Version 2.5         (R2021b)\nAerospace Blockset                                    Version 5.1         (R2021b)\nAerospace Toolbox                                     Version 4.1         (R2021b)\nAntenna Toolbox                                       Version 5.1         (R2021b)\nAudio Toolbox                                         Version 3.1         (R2021b)\nAutomated Driving Toolbox                             Version 3.4         (R2021b)\nBioinformatics Toolbox                                Version 4.15.2      (R2021b)\nCommunications Toolbox                                Version 7.6         (R2021b)\nComputer Vision Toolbox                               Version 10.1        (R2021b)\nControl System Toolbox                                Version 10.11       (R2021b)\nCurve Fitting Toolbox                                 Version 3.6         (R2021b)\nDDS Blockset                                          Version 1.1         (R2021b)\nDSP System Toolbox                                    Version 9.13        (R2021b)\nDatabase Toolbox                                      Version 10.2        (R2021b)\nDatafeed Toolbox                                      Version 6.1         (R2021b)\nDeep Learning HDL Toolbox                             Version 1.2         (R2021b)\nDeep Learning Toolbox                                 Version 14.3        (R2021b)\nEconometrics Toolbox                                  Version 5.7         (R2021b)\nEmbedded Coder                                        Version 7.7         (R2021b)\nFilter Design HDL Coder                               Version 3.1.10      (R2021b)\nFinancial Instruments Toolbox                         Version 3.3         (R2021b)\nFinancial Toolbox                                     Version 6.2         (R2021b)\nFixed-Point Designer                                  Version 7.3         (R2021b)\nFuzzy Logic Toolbox                                   Version 2.8.2       (R2021b)\nGPU Coder                                             Version 2.2         (R2021b)\nGlobal Optimization Toolbox                           Version 4.6         (R2021b)\nHDL Coder                                             Version 3.19        (R2021b)\nHDL Verifier                                          Version 6.4         (R2021b)\nImage Acquisition Toolbox                             Version 6.5         (R2021b)\nImage Processing Toolbox                              Version 11.4        (R2021b)\nInstrument Control Toolbox                            Version 4.5         (R2021b)\nLTE Toolbox                                           Version 3.6         (R2021b)\nLidar Toolbox                                         Version 2.0         (R2021b)\nMATLAB Coder                                          Version 5.3         (R2021b)\nMATLAB Compiler                                       Version 8.3         (R2021b)\nMATLAB Compiler SDK                                   Version 6.11        (R2021b)\nMATLAB Report Generator                               Version 5.11        (R2021b)\nMapping Toolbox                                       Version 5.2         (R2021b)\nMixed-Signal Blockset                                 Version 2.1         (R2021b)\nModel Predictive Control Toolbox                      Version 7.2         (R2021b)\nMotor Control Blockset                                Version 1.3         (R2021b)\nNavigation Toolbox                                    Version 2.1         (R2021b)\nOptimization Toolbox                                  Version 9.2         (R2021b)\nParallel Computing Toolbox                            Version 7.5         (R2021b)\nPartial Differential Equation Toolbox                 Version 3.7         (R2021b)\nPhased Array System Toolbox                           Version 4.6         (R2021b)\nPowertrain Blockset                                   Version 1.10        (R2021b)\nPredictive Maintenance Toolbox                        Version 2.4         (R2021b)\nRF Blockset                                           Version 8.2         (R2021b)\nRF PCB Toolbox                                        Version 1.0         (R2021b)\nRF Toolbox                                            Version 4.2         (R2021b)\nROS Toolbox                                           Version 1.4         (R2021b)\nRadar Toolbox                                         Version 1.1         (R2021b)\nReinforcement Learning Toolbox                        Version 2.1         (R2021b)\nRisk Management Toolbox                               Version 1.10        (R2021b)\nRobotics System Toolbox                               Version 3.4         (R2021b)\nRobust Control Toolbox                                Version 6.11        (R2021b)\nSatellite Communications Toolbox                      Version 1.1         (R2021b)\nSensor Fusion and Tracking Toolbox                    Version 2.2         (R2021b)\nSerDes Toolbox                                        Version 2.2         (R2021b)\nSignal Integrity Toolbox                              Version 1.0         (R2021b)\nSignal Processing Toolbox                             Version 8.7         (R2021b)\nSimBiology                                            Version 6.2         (R2021b)\nSimEvents                                             Version 5.11        (R2021b)\nSimscape                                              Version 5.2         (R2021b)\nSimscape Driveline                                    Version 3.4         (R2021b)\nSimscape Electrical                                   Version 7.6         (R2021b)\nSimscape Fluids                                       Version 3.3         (R2021b)\nSimscape Multibody                                    Version 7.4         (R2021b)\nSimulink 3D Animation                                 Version 9.3         (R2021b)\nSimulink Check                                        Version 5.2         (R2021b)\nSimulink Code Inspector                               Version 4.0         (R2021b)\nSimulink Coder                                        Version 9.6         (R2021b)\nSimulink Compiler                                     Version 1.3         (R2021b)\nSimulink Control Design                               Version 6.0         (R2021b)\nSimulink Coverage                                     Version 5.3         (R2021b)\nSimulink Design Optimization                          Version 3.10        (R2021b)\nSimulink Design Verifier                              Version 4.6         (R2021b)\nSimulink PLC Coder                                    Version 3.5         (R2021b)\nSimulink Report Generator                             Version 5.11        (R2021b)\nSimulink Requirements                                 Version 1.8         (R2021b)\nSimulink Test                                         Version 3.5         (R2021b)\nSoC Blockset                                          Version 1.5         (R2021b)\nStateflow                                             Version 10.5        (R2021b)\nStatistics and Machine Learning Toolbox               Version 12.2        (R2021b)\nSymbolic Math Toolbox                                 Version 9.0         (R2021b)\nSystem Composer                                       Version 2.1         (R2021b)\nSystem Identification Toolbox                         Version 9.15        (R2021b)\nText Analytics Toolbox                                Version 1.8         (R2021b)\nUAV Toolbox                                           Version 1.2         (R2021b)\nVehicle Dynamics Blockset                             Version 1.7         (R2021b)\nVehicle Network Toolbox                               Version 5.1         (R2021b)\nVision HDL Toolbox                                    Version 2.4         (R2021b)\nWLAN Toolbox                                          Version 3.3         (R2021b)\nWavelet Toolbox                                       Version 6.0         (R2021b)\nWireless HDL Toolbox                                  Version 2.3         (R2021b)\n</code></pre>"},{"location":"software/packages/pytorch/","title":"PyTorch","text":"<p>PyTorch is an open source Python package that provides tensor computation, like NumPy, with GPU acceleration and deep neural networks built on a tape-based autograd system.</p> <p>PyTorch can be installed by following the official instructions for installing a CUDA compatible PyTorch via pip or conda. Please consult the Python packages installation guide for an overview of recommended ways to manage Python installations on UBELIX. </p> <p>If you install pre-built binaries (using either pip or conda) then you do not need load any CUDA modules on UBELIX before installing PyTorch. This is because PyTorch, unless compiled from source, is always delivered with a copy of the CUDA library if a CUDA capable version is installed.</p>"},{"location":"software/packages/pytorch/#install-pytorch-using-conda","title":"Install PyTorch using conda","text":"<p>To install any version of PyTorch request an interactive job on a GPU node:</p> <p><pre><code>salloc --time=01:00:00 --partition=gpu --gres=gpu:rtx4090:1 --cpus-per-task=16 --mem-per-cpu=4G\nsrun --pty bash\n</code></pre> This will result in a shell directly on a GPU node.</p>"},{"location":"software/packages/pytorch/#install-the-latest-version","title":"Install the latest version","text":"<pre><code>module load Anaconda3\neval \"$(conda shell.bash hook)\"\n\nconda create -n pytorch python=3.9 -c conda-forge\nconda activate pytorch\nconda install pytorch::pytorch torchvision torchaudio -c pytorch\n</code></pre> <p>You can verify the detection of a GPU in your PyTorch installation the following command:</p> <pre><code>python3 -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre>"},{"location":"software/packages/pytorch/#install-a-previous-version","title":"Install a previous version","text":"<p>To install a previous version of Pytorch, follow the general procedure above but check for the specific commands in the official documentation.</p>"},{"location":"software/packages/pytorch/#install-pytorch-through-pip","title":"Install PyTorch through pip","text":"<p>As an alternative to <code>conda</code> you can install PyTorch directly through <code>pip</code>. Please see our documentation on installing Python packages for general advise on <code>pip</code> and follow the official instruction to install PyTorch.</p>"},{"location":"software/packages/r/","title":"R","text":""},{"location":"software/packages/r/#loading-the-r-module","title":"Loading the R module","text":"<p>R is provided by an environment module and must be loaded explicitly:</p> <pre><code>module load R\nR --version\n  R version 4.2.1 (2022-06-23) -- \"Funny-Looking Kid\"\n  ...\n</code></pre>"},{"location":"software/packages/r/#listing-all-installed-packages","title":"Listing all installed packages","text":"<p>You can list all installed R packages using the following one-liner:</p> <pre><code>R -e 'installed.packages()'\n</code></pre> <p>R is installed as global Module in various versions. There are already a longer list of pre-installed packages available. If you need additional packages you can install them by yourself. See Installing R packages for details.</p>"},{"location":"software/packages/r/#batch-execution-of-r","title":"Batch Execution of R","text":"<p>The syntax for running R non-interactively with input read from infile and output send to outfile is:</p> <pre><code>R CMD BATCH [options] infile [outfile]\n</code></pre> <p>Suppose you placed your R code in a file called foo.R:</p> <p>foo.R</p> <pre><code>set.seed(3000)\nvalx&lt;-seq(-2,2,0.01)\nvaly&lt;-2*valx+rnorm(length(valx),0,4)\n# Save plot to pdf\npdf('histplot.pdf')\nhist(valy,prob=TRUE,breaks=20, main=\"Histogram and PDF\",xlab=\"y\", ylim=c(0,0.15))\ncurve(dnorm(x,mean(valy),sd(valy)),add=T,col=\"red\")\ndev.off()\n</code></pre> <p>To execute foo.R on the cluster, add the R call to your job script\u2026</p> <p>Rbatch.sh</p> <pre><code>#! /bin/bash\n#SBATCH --mail-user=&lt;put your valid email address here!&gt;\n#SBATCH --mail-type=end,fail\n#SBATCH --time=01:00:00\n#SBATCH --mem-per-cpu=2G\n\n# Put your code below this line\nmodule load vital-it\nmodule load R/3.4.2\nR CMD BATCH --no-save --no-restore foo.R\n</code></pre> <p>\u2026and submit your job script to the cluster:</p> <pre><code>sbatch Rbatch.sh\n</code></pre>"},{"location":"software/packages/r/#advanced-topics","title":"Advanced Topics","text":""},{"location":"software/packages/r/#parallel-r","title":"Parallel R","text":"<p>By default, R will not make use of multiple cores available on compute nodes to parallelize computations. Parallel processing functionality is provided by add-on packages. Consider the following contrived example to get you started. To follow the example, you need the following packages installed, and the corresponding libraries loaded:</p> <pre><code>&gt; library(doParallel)\n&gt; library(foreach)\n</code></pre> <p>The foreach package provides a looping construct for executing R statements repeatedly, either sequentially (similar to a for loop) or in parallel. While the binary operator %do% is used for executing the statements sequentially, the %dopar% operator is used to execute code in parallel using the currently registered backend. The getDoParWorkers() function returns the number of execution workers (cores) available in the currently registered doPar backend, by default this corresponds to one worker:</p> <pre><code>&gt; getDoParWorkers()\n[1] 1\n</code></pre> <p>Hence, the following R code will execute on a single core (even with the %dopar% operator):</p> <pre><code>&gt; start.time &lt;- Sys.time()\n&gt; foreach(i=4:1, .combine='c', .inorder=FALSE) %dopar% {\n+ Sys.sleep(3*i)\n+ i\n+ }\nend.time &lt;- Sys.time()\nexec.time &lt;- end.time - start.time\n[1] 4 3 2 1\n</code></pre> <p>Let\u2019s measure the runtime of the sequentiall execution:</p> <pre><code>&gt; start.time &lt;- Sys.time(); foreach(i=4:1, .combine='c', .inorder=TRUE) %dopar% { Sys.sleep(3*i); i }; end.time &lt;- Sys.time(); exec.time &lt;- end.time - start.time; exec.time\n[1] 4 3 2 1\nTime difference of 30.04088 secs\n</code></pre> <p>Now, we will register a parallel backend to allow the %dopar% operator to execute in parallel. The doParallel package provides a parallel backend for the %dopar% operator. Let\u2019s find out the number of cores available on the current node</p> <pre><code>&gt; system('nproc')\n[1] 24\n</code></pre> <p>To register the doPar backend call the function registerDoParallel(). With no arguments provided, the number of cores assigned to the backend matches the value of options(\u201ccores\u201d), or if not set, to half of the cores detected by the parallel package. </p> <pre><code> registerDoParallel()\n&gt; getDoParWorkers()\n[1] 12\n</code></pre> <p>To assign 4 cores to the parallel backend:</p> <pre><code>&gt; registerDoParallel(cores=4)\n&gt; getDoParWorkers()\n[1] 4\n</code></pre> <p>Request the correct number of slots</p> <p>Because it is crucial to request the correct number of slots for a parallel job, we propose to set the number of cores for the doPar backend to the number of slots allocated to your job: <code>registerDoParallel(cores=Sys.getenv(\"SLURM_CPUS_PER_TASK\"))</code></p> <p>Now, run the example again:</p> <pre><code>&gt; foreach(i=4:1, .combine='c', .inorder=FALSE) %dopar% {\n+ Sys.sleep(3*i)\n+ i\n+ }\n[1] 4 3 2 1\n</code></pre> <p>Well, the output is basically the same (the results are combined in the same order!). Let\u2019s again measure the runtime of the parallel execution on 4 cores:</p> <p>The binary operator %do% will always execute a foreach-loop sequentially even if registerDoParallel was called before! To correctly run a foreach in parallel, two conditions must be met:</p> <ul> <li>registerDoParallel() must be called with a certain number of cores</li> <li>The %dopar% operator must be used in the foreach-loop to have it run in parallel!</li> </ul>"},{"location":"software/packages/tensorflow/","title":"TensorFlow","text":"<p>Deep learning framework for Python.</p> <p>There are two options availble to install TensorFlow on UBELIX:</p> <ul> <li>Install Tensorflow using CUDA and cuDNN from UBELIX software stack</li> <li>Install Tensorflow using CUDA from pip-extras</li> </ul>"},{"location":"software/packages/tensorflow/#install-tensorflow-using-cuda-and-cudnn-from-ubelix-software-stack-recommended","title":"Install Tensorflow using CUDA and cuDNN from UBELIX software stack (recommended)","text":"<p>This approach uses for UBELIX optimised installations of CUDA and cuDNN and therefore theoretically provides superior performance.</p> <p>In order to use CUDA and cuDNN modules from the UBELIX software stack with TensorFlow we need to find a matching version of Tensorflow:</p> <ul> <li>List available CUDA and cuDNN version as modules with <code>module spider</code></li> <li>Find matching Tensorflow version here</li> </ul> <p>Currently the following versions are supported:</p> Tensorflow Version CUDA version cuDNN version tensorflow-2.14.0 CUDA/11.8.0 8.7.0.84 tensorflow-2.15.0 CUDA/12.2.0 8.9.2.26 <p>To install either of these version request an interactive job on a GPU node:</p> <p><pre><code>salloc --time=01:00:00 --partition=gpu --gres=gpu/rtx4090:1 --cpus-per-task=16 --mem-per-cpu=4G\nsrun --pty bash\n</code></pre> This will result in a shell directly on a GPU node.</p>"},{"location":"software/packages/tensorflow/#install-tensorflow-2140","title":"Install tensorflow-2.14.0","text":"<pre><code>module load CUDA/11.8.0\nmodule load cuDNN/8.7.0.84-CUDA-11.8.0\n\nmodule load Anaconda3\neval \"$(conda shell.bash hook)\"\n\nconda create -n tf214 python=3.9 -c conda-forge\nconda activate tf214\npip install tensorflow==2.14.0\n</code></pre>"},{"location":"software/packages/tensorflow/#install-tensorflow-2150","title":"Install tensorflow-2.15.0","text":"<pre><code>module load CUDA/12.2.0\nmodule load cuDNN/8.9.2.26-CUDA-12.2.0\n\nmodule load Anaconda3\neval \"$(conda shell.bash hook)\"\n\nconda create -n tf215 python=3.9 -c conda-forge\nconda activate tf215\npip install tensorflow==2.15.0\n</code></pre>"},{"location":"software/packages/tensorflow/#check-the-installation","title":"Check the installation","text":"<p>To check if the installation of TensorFlow was successful we can check if a GPU is detected:</p> <pre><code>python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre> <p>Other versions</p> <p>Other versions might work as well, i.e TensorFlow 2.17 with CUDA 12.2. However we advise against this practice as these unsupported configurations tend to break more easilly and are harder to debug. If you need a different version of TensorFlow, please follow the approach below.</p>"},{"location":"software/packages/tensorflow/#install-tensorflow-using-cuda-from-pip-extras","title":"Install Tensorflow using CUDA from pip-extras","text":"<p>If you need to install a different version of TensorFlow that isn\u2019t available for the CUDA and cuDNN module version on UBELIX you can use a CUDA installation from TensorFlow pip extras that match your required Tensorflow version:</p> <p>To install either of these version request an interactive job on a GPU node:</p> <p><pre><code>salloc --time=01:00:00 --partition=gpu --gres=gpu/rtx4090:1 --cpus-per-task=16 --mem-per-cpu=4G\nsrun --pty bash\n</code></pre> This will result in a shell directly on a GPU node.</p>"},{"location":"software/packages/tensorflow/#install-tensorflow-2170","title":"Install tensorflow-2.17.0","text":"<pre><code>module load Anaconda3\neval \"$(conda shell.bash hook)\"\n\nconda create -n tf217 python=3.9 -c conda-forge\nconda activate tf217\npip install \"tensorflow[and-cuda]==2.17.0\"\n</code></pre> <p>Again, we can verify the installation using the command:</p> <pre><code>python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre> <p>This will throw the following error messages but detects the GPU and works as expected:</p> <pre><code>E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nE external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n</code></pre>"},{"location":"software/packages/tensorflow/#install-tensorflow-2161","title":"Install tensorflow-2.16.1","text":"<pre><code>module load Anaconda3\neval \"$(conda shell.bash hook)\"\n\nconda create -n tf216 python=3.9 -c conda-forge\nconda activate tf216\npip install \"tensorflow[and-cuda]==2.16.1\"\n</code></pre> <p>Due to a bug in this TensorFlow version, the following code needs to executed every time before Tensorflow is used:</p> <pre><code>NVIDIA_DIR=$(dirname $(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\")))\nfor dir in $NVIDIA_DIR/*; do\n    if [ -d \"$dir/lib\" ]; then\n        export LD_LIBRARY_PATH=\"$dir/lib:$LD_LIBRARY_PATH\"\n    fi\ndone\n</code></pre> <p>After this, we can again verify the installation using the command:</p> <pre><code>python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre>"},{"location":"software/packages/tensorflow/#license","title":"License","text":"<p>TensorFlow is licensed under Apache License 2.0.</p>"},{"location":"storage/","title":"Data Storage Overview","text":"<p>This describes the available data storage options on UBELIX. Please look closely at their descriptions to find the ideal storage option for your use case.</p>"},{"location":"storage/#where-to-store-data","title":"Where to store data?","text":"User homeWorkspaces (Research Storage)Capacity StorageNetwork scratchLocal scratch <p>Each user has a home directory (<code>$HOME</code>) that is limited to maximum <code>1TB</code>. It is located under <code>/storage/home/$USER</code>, where <code>$USER</code> is the campus account. <code>$HOME</code> is meant for private and configuration data. Regular Snapshots provide possibility to recover accidentally modified or deleted data.</p> <p>Sharing data with other cluster users is not supported for <code>$HOME</code>. The user home directory is purged once the user account expires.</p> <p>Workspace storage is intended to share data amongst the members of a project. Typically, workspace storage is used to store project data, share applications and libraries compiled for the project. Workspaces are located under <code>/storage/research</code> and <code>/storage/workspaces</code>. The space in each workspace is controlled individually by the quota of the corresponding research storage share. Similar to user home directories, workspaces are protected by regular snapshots to recover accidentally modified or deleted data.</p> <p>Workspace storage is based on the Research Storage service.</p> <p>Capacity storage is intended to store large amounts of reproducible research data amongst the members of a project. Projects are located under <code>/storage/capacicty</code> and only accessible through the <code>submit</code> nodes. The space in each project is controlled individually by the quota of the corresponding capacity storage share. In contrast to workspaces, data is NOT protected by regular snapshots to recover accidentally modified or deleted data.</p> <p>Capacity storage is based on the Capacity Storage service.</p> <p>SCRATCH (<code>/storage/scratch</code>) is a temporary space with less restrictive limitations in size, but more restrictive limitation in time. There is no snapshot or backup service implemented in that space.</p> <p>You are not supposed to use the scratch space as long-term storage. The  scratch file system is a temporary storage space. Files that have not been accessed will be purged after 30 days.</p> <p>Temporary storage for input, output, or checkpoint data of your application. When running jobs on UBELIX, this is the main storage you should use for your disk I/O needs.</p> <p>A high performance variant of network scratch that is local to the node(s) your job is running on. Data is only available as long as the job is running!</p>"},{"location":"storage/#ubelix-file-system-location-of-storage-systems","title":"UBELIX file system location of storage systems","text":"Path Intended use Comment User home <code>/storage/homefs/&lt;username&gt;</code> User home directory for personal and configuration files Workspace (Research Storage) <code>/storage/research/</code><code>/storage/workspaces/</code> Project storage directory for shared project files Capacity Storage <code>/storage/capacity/</code> Project storage directory for large, reproducible data Only available on <code>submit</code> nodes Network scratch <code>/scratch/network/</code> Temporary storage for input, output or checkpoint data Project flash <code>/scratch/local/</code> High performance temporary storage for input and output data Only available during job execution"},{"location":"storage/quota/","title":"File System Quota","text":"<p>This page contains information about quota limits on UBELIX file systems.</p> Quota Max files Expandable Retention Backup User home 1TB 1M No User lifetime Yes Workspace(Research Storage) min 5TB 1M per TB Yes Project lifetime Yes Capacity Storage min 75TB 100K per TB Yes Project lifetime No Network Scratch 30 TB<sup>1</sup> 10M<sup>1</sup> No 30 days No Local Scratch &lt; 1 TB - No Job lifetime No space quota file quota backup expiration HOME 1TB 1M yes - WORKSPACE at least 5TB 1M per TB yes - SCRATCH 30TB<sup>1</sup> 10M<sup>1</sup> no 1 month <p>Note that, except for the user home directory and scratch storage, data storage needs to be requested per project. When a storage space is marked as expandable, it means that you can request more space if needed. StoragePlease contact the UBELIX Support Team to request more storage space.</p> <p>Don\u2019t circumvent the retention policy</p> <p>Deliberately modifying file access times to bypass the retention policy is prohibited. It\u2019s anti-social behavior that may impact other users negatively.</p> <p> Job abortion</p> <p>Jobs will fail if no more disk space can be allocated, or if no more files can be created because the respective quota hard limit is exceeded</p>"},{"location":"storage/quota/#display-quota-information","title":"Display quota information","text":"<p>You can check the memory and file usage quotas of your storage spaces with the following command:</p> <p><pre><code>$ quota\nUniBE Workspace Quota report\n============================\n                    :  used (GB)(   %), quota (GB) |  files used(   %),      quota\n==================================================================================\nHOME                :        420( 41%),       1024 |      773425( 77%),    1000000\nWorkspace1          :        101(  1%),      10240 |           3(  0%),   10000000\n</code></pre> Furthermore, there is a more detailed version using the <code>-l</code> or <code>--long</code> option <pre><code>$ quota -l\nUniBE Workspace Quota report\n============================\n                    : free quota,  used (GB)(   %), quota (GB) |  files used(   %),      quota | start date(1), average quota(2)\n================================================================================================================================\nHOME                :        all,        421( 41%),       1024 |      796058( 79%),    1000000 |              ,\nSCR_usr             :        all,        269(  0%),      30720 |          22(  0%),   10000000 |              ,\nWorkspace1          :          5,        101(  1%),      10240 |           4(  0%),   10000000 |    2021-02-25,           7.5833\n\n(0) space names starting with \"SCR_\" refer to the personal (usr) or Workspace SCRATCH quota.\n(1) accounting period start date, The date from which the average usage is computed.\n(2) file space average quota (not files), calculated by the average of messured values in the actual accounting period.\n</code></pre></p> <p>In the last example the workspace <code>Workspace1</code> has <code>5TB</code> of free quota, and a total of <code>10TB</code> of quota (<code>5TB</code> additional storage requested). The <code>start date</code> defines the start of the accounting period and the average quota is computed as average over all datapoints starting from <code>start date</code>.  Furthermore, the SCRATCH quota is presented starting with <code>SCR_</code>, where <code>SCR_usr</code> is your personal SCRATCH quota and <code>SCR_</code> plus Workspace name the group quota of the Workspace group in the scratch fileset. Workspace SCRATCH quota is only presented if a quota is set and the <code>quota -l</code> option selected.</p> <p>Data updates</p> <ul> <li>Workspaces: Workspace quota information is gathered twice a day. Thus the presented data may not completely represent the current state.</li> <li>HOME and SCRATCH: values presented are actual values directly gathered from the file system</li> </ul>"},{"location":"storage/quota/#about-the-number-of-files-quota","title":"About the number-of-files quota","text":"<p>For reasons related to performance, we are particularly attentive to the number of files present on the parallel file system. A lot of small files negatively impact all users by stressing the file system. Therefore, any requests to increase the number-of-files quota will be evaluated carefully by the UBELIX Team and must be fully justified to be granted.</p> <p>In general, applications that generate a lot of small files per process are not well suited for UBELIX. If you are the developer of such application, you should consider tools like [HDF5][hdf5].</p> <ol> <li> <p>Scratch quota is currently implemented per user\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"storage/scratch/","title":"Scratch - temporary file space","text":"<p>Warning</p> <p>Scratch serves as a temporary repository for compute output and is explicitly designed for short-term usage. Unlike other storage locations, scratch is not backed up. Do not put data here that is crucial for your research! Files are subject to automatic removal if they are not accessed within a timeframe of 30 days.</p> <p>Scratch file space are meant for temporary data storage. Interim computational data should be located there.  We distinguish local and network scratch spaces. </p>"},{"location":"storage/scratch/#network-scratch","title":"Network Scratch","text":"<p>Network scratch spaces are located on the parallel file system and accessible to all nodes. In contrast to <code>HOME</code> or <code>WORKSPACEs</code>, scratch is meant for temporary data, especially with larger quota requirements.  Jobs creating a lot of temporary data, which may need or may not need to be post-processed should run in this space. As an example, a application creating a huge amount of temporary output, which need to be analysed, but only partly need to be stored for a longer term. Block and file quota is less restrictive on scratch compared to <code>HOME</code> or permanent <code>WORKSPACE</code> directories. Every user can use up to 15TB and 10M files. There is no snapshot and no backup feature available. Furthermore, an automatic deletion policy is implemented, deleting files which are older than 30 days. Files are subject to automatic removal only if they are not accessed within a timeframe of 30 days. Any file with an access timestamp within the last 30 days remains exempt from the policy, ensuring that actively utilized files are not inadvertently affected.</p> <p>Scratch file space can be accessed using the Workspace module and the <code>$SCRATCH</code> environment variable. </p> <pre><code>module load Workspace\ncd $SCRATCH\n</code></pre> <p>For personal Scratch see below</p>"},{"location":"storage/scratch/#workspace-scratch","title":"Workspace Scratch","text":"<p>Each Workspace has a <code>$SCRATCH</code> space with the same access permissions like the permanent Workspace directory (using primary and secondary groups). The Workspace can be accessed using <code>$SCRATCH</code> variable (after loading the Workspace module). It will point to <code>/storage/scratch/&lt;researchGroupID&gt;/&lt;WorkspaceID&gt;</code>. Please use <code>$SCRATCH</code> to access it. </p>"},{"location":"storage/scratch/#personal-scratch","title":"Personal Scratch","text":"<p>Users without a Workspace can also use \u201cpersonal\u201d Scratch. This space does need to be created initially: <pre><code>module load Workspace_Home\nmkdir $SCRATCH\ncd $SCRATCH\n</code></pre></p> <p>Private Scratch</p> <p>Please note that this space is per default no private space. If you want to restrict access you can change permissions using:</p> <pre><code>chmod 700 $SCRATCH\n</code></pre>"},{"location":"storage/scratch/#local-scratch","title":"Local Scratch","text":"<p>Cases:</p> <ul> <li>temporary files are produced, which are not relevant after the computation</li> <li>files need to be read or written multiple times within a job</li> </ul> <p>Local storage (<code>$TMPDIR</code>) should be used instead of network storage. </p> <p><code>$TMPDIR</code> is a node local storage which only exists during the job life time and cleaned automatically afterwards. The actual directory is <code>/scratch/local/&lt;jobID&gt;</code>, but it is highly recommended to use <code>$TMPDIR</code>.  If necessary data can be copied there initially at the beginning of the job, processes (multiple times) and necessary results copied back at the end. </p> <p><code>$TMPDIR</code> instead of <code>/tmp</code></p> <p><code>$TMPDIR</code> is much larger than <code>/tmp</code> and cleaned automatically. Especially in case of job errors data in <code>/tmp</code> will persist and clog the nodes memory. </p>"},{"location":"storage/scratch/#example-temporary-files","title":"Example: temporary files","text":"<p>In the following example the <code>example.exe</code> will need a place to store temporary/intermediate files, not necessary after the computation. The location is provided using the <code>--builddir</code> option. And the local scratch (<code>$TMPDDIR</code>) is specified. </p> <pre><code>#!/bin/bash\n#SBATCH --job-name tmpdir\n#SBATCH --nodes 1\n#SBATCH --ntasks 1\n#SBATCH --cpus-per-task 1\n\nsrun example.exe --builddir=$TMPDIR input.dat\n</code></pre> <p>If you want to have the advantage of low latency file system (local) but you need to keep files, you still can use <code>$TMPDIR</code> and copy files to the network storage (e.g. <code>$WORKSPACE</code> or <code>$HOME</code>) at the end of your job. This is only efficient if a) more files are manipulated local (in <code>$TMPDIR</code>) than copied to the network storage or b) files are manipulated multiple times, before copying to the network storage.</p>"},{"location":"storage/scratch/#example-including-data-movement","title":"Example: including data movement","text":"<p>In the following example script, all files from the submitting directory are copied to the head compute node. At the end of the job all files from the compute node local directory is copied back. The compute node local <code>$TMPDIR</code> is used, which points to <code>/scratch/local/&lt;jobid&gt;</code>, a job specific directory in the nodes internal disc.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name tmpdir\n#SBATCH --nodes 1\n#SBATCH --ntasks 1\n#SBATCH --cpus-per-task 1\n###SBATCH --output slurm.out # when specifying output file name, add rm slurm.out in cleanup function\n\n# I. Define directory names [DO NOT CHANGE]\n# =========================================\n# get name of the temporary directory working directory, physically on the compute-node\nworkdir=\"${TMPDIR}\"\n# get submit directory\n# (every file/folder below this directory is copied to the compute node)\nsubmitdir=\"${SLURM_SUBMIT_DIR}\"\n\n# 1. Transfer to node [DO NOT CHANGE]\n# ===================================\n# create/empty the temporary directory on the compute node\nif [ ! -d \"${workdir}\" ]; then\n  mkdir -p \"${workdir}\"\nelse\n  rm -rf \"${workdir}\"/*\nfi\n\n# change current directory to the location of the sbatch command\n# (\"submitdir\" is somewhere in the home directory on the head node)\ncd \"${submitdir}\"\n# copy all files/folders in \"submitdir\" to \"workdir\"\n# (\"workdir\" == temporary directory on the compute node)\ncp -prf * ${workdir}\n# change directory to the temporary directory on the compute-node\ncd ${workdir}\n\n# 3. Function to transfer back to the head node [DO NOT CHANGE]\n# =============================================================\n# define clean-up function\nfunction clean_up {\n  # - remove log-file on the compute-node, to prevent overwiting actual output with empty file\n  rm slurm-${SLURM_JOB_ID}.out\n  # - TODO delete temporary files from the compute-node, before copying. Prevent copying unnecessary files.\n  # rm -r ...\n  # - change directory to the location of the sbatch command (on the head node)\n  cd \"${submitdir}\"\n  # - copy everything from the temporary directory on the compute-node\n  cp -prf \"${workdir}\"/* .\n  # - erase the temporary directory from the compute-node\n  rm -rf \"${workdir}\"/*\n  rm -rf \"${workdir}\"\n  # - exit the script\n  exit\n}\n\n# call \"clean_up\" function when this script exits, it is run even if SLURM cancels the job\ntrap 'clean_up' EXIT\n\n# 2. Execute [MODIFY COMPLETELY TO YOUR NEEDS]\n# ============================================\n# TODO add your computation here\n# simple example, hello world\nsrun echo \"hello world from $HOSTNAME\"\n</code></pre> <p>Further aspects to consider:</p> <ul> <li>copy only necessary files<ul> <li>have only necessary files in the submit directory</li> <li>remove all unnecessary files before copying the data back, e.g. remove large input files</li> </ul> </li> <li>In case of a parallel job, you need to verify that all process run on one single node (<code>--nodes=1</code>) OR copy the data to all related nodes (e.g. <code>srun -n1 cp ...</code>).</li> </ul>"},{"location":"support/","title":"Support","text":""},{"location":"support/#support","title":"Support","text":"<p>In case of questions, comments or issues get in touch with us. Please first collect all necessary details. In case of issues, this should include:</p> <ul> <li>JobID</li> <li>location of job scripts</li> <li>location of output/error files</li> <li>the full error message(s)</li> <li>your user name</li> <li>detailed description of expectation and observation</li> </ul>    Open a Support Ticket"},{"location":"support/dsl/","title":"Data Science Lab","text":"<p>The Data Science Lab (DSL) provides university-wide support and training for researchers and research groups in data science, machine learning, artificial intelligence and research IT related matters.</p> <p>Services range from rapid advice by e\u2011mail and code or algorithm reviews to long-term collaborations on infrastructure and research projects with co\u2011analysis and co\u2011authoring.</p> <p>DSL is part of the Swiss EnhanceR research IT community and is an interfaculty core facility of the University of Bern.</p> <p>Check here for more DSL related information.</p>"},{"location":"support/faq/","title":"FAQ","text":"<p>This page provides a collection of frequently asked questions.</p>"},{"location":"support/faq/#file-system","title":"File system","text":""},{"location":"support/faq/#what-if-my-home-is-full","title":"What if my HOME is full?","text":"<p>If you reached your quota, you will get strange warning about not being able to write temporary files etc. You can check your quota using the <code>quota</code> command. To resolve the situation you can follow these strategies:</p> <ol> <li>Decluttering: Check for unnecessary data. This could be:</li> </ol> <ul> <li>temporary computational data, like already post processed output files</li> <li>duplicated data</li> <li>unused application packages, e.g. Python packages in <code>$HOME/.local/lib/python*/site-packages/*</code></li> </ul> <ol> <li>Pack and archive: The HPC storage is a high performance parallel storage and not meant to be an archive. Data not used in the short to midterm should be packed and moved to an archive storage. </li> </ol> <p>In general, we consider data on our HPC systems as research data. Further we consider research data to be shared sooner or later. And we aim to support and enhance collaborations. Therefore, we introduce group shared spaces, called HPC Workspaces. Ask your research group manager to add you to an existing Workspace or create a new one.  There will be no quota increase for HOME directories. </p>"},{"location":"support/faq/#workspaces","title":"Workspaces","text":""},{"location":"support/faq/#i-need-access-to-a-hpc-workspace-who-do-i-need-to-ask","title":"I need access to a HPC Workspace. Who do I need to ask?","text":"<p>HPC Workspaces are managed by the group manager/leader. Therewith you need to ask them to add you to the primary or secondary group.</p>"},{"location":"support/faq/#i-need-to-share-data-with-my-colleges-what-can-i-do","title":"I need to share data with my colleges. What can I do?","text":"<p>HPC Workspaces are meant to host shared data. See HPC Workspaces.</p>"},{"location":"support/faq/#software-issues","title":"Software issues","text":""},{"location":"support/faq/#why-the-system-is-complaining-about-not-finding-an-existing-module","title":"Why the system is complaining about not finding an existing module?","text":"<p>There are cases modules could not be found. This could be that the modules is not exiting in the target software stack, it could be hidden, or a version inconsistency.</p>"},{"location":"support/faq/#hidden-modules","title":"hidden modules","text":"<p>Some modules are provided as hidden modules to keep the presented software stack nice and clean. Hidden modules can be listed using <code>module --show-hidden avail</code>.</p>"},{"location":"support/faq/#software-stack-inconstency","title":"software stack inconstency","text":"<p>It is strongly suggested to not mix different toolchains like foss or intel. Additionally, it is advised to stay with one version of a toolchain, e.g. foss/2023a and its dependency versions, e.g. GCC/12.3.0.</p>"},{"location":"support/faq/#environment-issues","title":"Environment issues","text":""},{"location":"support/faq/#i-modified-my-bashrc-but-its-not-doing-what-i-expect-how-can-i-debug-that-bash-script","title":"I modified my bashrc, but its not doing what I expect, how can I debug that bash script?","text":"<p>The bashrc can be debugged as all other bash scripts, using </p> <ul> <li><code>set -x</code> at the beginning of the script. This will print all commands executed on screen, including all subcommand also included in called scripts and tools</li> <li>print statements, e.g. <code>echo \"DEBUG: variable PATH=$PATH\"</code></li> </ul> <p>These should provide a good indication where the script diverge from your expectation. </p>"},{"location":"support/faq/#job-issues","title":"Job issues","text":""},{"location":"support/faq/#why-is-my-job-still-pending","title":"Why is my job still pending?","text":"<p>Tip</p> <p>The REASON column of the <code>squeue</code> output gives you a hint why your job is not running.</p> <p>(Resources) The job is waiting for resources to become available so that the jobs resource request can be fulfilled.</p> <p>(Priority) The job is not allowed to run because at least one higher prioritized job is waiting for resources.</p> <p>(Dependency) The job is waiting for another job to finish first (<code>--dependency=... option</code>).</p> <p>(DependencyNeverSatisfied) The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs.</p> <p>(QOSMaxCpuPerUserLimit) The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish.</p> <p>(AssocGrpCpuLimit) dito.</p> <p>(AssocGrpJobsLimit) The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish.</p> <p>(ReqNodeNotAvail, UnavailableNodes:\u2026) Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, <code>DOWN</code>, <code>DRAINED</code>, or not responding.Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime (see output of <code>scontrol show reservation</code>) and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using <code>scontrol show reservation</code>.</p>"},{"location":"support/faq/#why-cant-i-submit-further-jobs","title":"Why can\u2019t I submit further jobs?","text":"<p><code>sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)</code></p> <p>\u2026 indicates that you have reached the maximum of allowed jobs to be submitted to a specific partition.</p>"},{"location":"support/faq/#job-in-state-failed-although-job-completed-successfully","title":"Job in state FAILED although job completed successfully","text":"<p>Slurm captures the return value of the batch script/last command and reports this value as the completion status of the job/job step. Slurm indicates status FAILED if the value captured is non-zero.</p> <p>The following simplified example illustrates the issue:</p> <p>simple.c</p> <pre><code>#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\nint main (int argc, char *argv[]) {\n  char hostname[128];\n  gethostname(hostname, sizeof(hostname));\n  printf(\"%s says: Hello World.\\n\", hostname);\n}\n</code></pre> <p>job.sh</p> <pre><code>#!/bin/bash\n# Slurm options\n#SBATCH --mail-user=foo@bar.unibe.ch\n#SBATCH --mail-type=END\n#SBATCH --job-name=\"Simple Hello World\"\n#SBATCH --time=00:05:00\n#SBATCH --nodes=1\n# Put your code below this line\n./simple\n</code></pre> <pre><code>bash$ sbatch job.sh\nSubmitted batch job 104\n</code></pre> <p>Although the job finished successfully\u2026</p> <p>slurm-104.out</p> <pre><code>knlnode02.ubelix.unibe.ch says: Hello World.\n</code></pre> <p>\u2026Slurm reports job FAILED:</p> <pre><code>bash$ sacct -j 104\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n104          Simple He+        all                     1     FAILED     45:0\n104.batch         batch                                1     FAILED     45:0\n</code></pre> <p>Problem: The exit code of the job is the exit status of batch script (job.sh) which in turn returns the exit status of the last command executed (simple) which in turn returns the return value of the last statement (printf()). Since printf() returns the number of characters printed (45), the exit code of the batch script is non-zero and consequently Slurm reports job FAILED although the job produces the desired output.</p> <p>Solution: Explicitly return a value:</p> <pre><code>#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\nint main (int argc, char *argv[]) {\n  char hostname[128];\n  int n;\n  gethostname(hostname, sizeof(hostname));\n  // If successful, the total number of characters written is returned. On failure, a negative number is returned.\n  n = printf(\"%s says: Hello World.\\n\", hostname);\n  if (n &lt; 0)\n    return 1;\n  return 0;\n}\n</code></pre> <pre><code>bash$ sacct -j 105\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n105          Simple He+        all                     1  COMPLETED      0:0\n105.batch         batch                                1  COMPLETED      0:0\n</code></pre>"},{"location":"support/issues/","title":"Known issues","text":""},{"location":"support/issues/#known-issues","title":"Known Issues","text":"<p>This page lists known issues on UBELIX and any known workarounds.</p>"}]}